# TP1 - Premier dÃ©ploiement Kubernetes sur AlmaLinux

> **ğŸ’» Utilisateurs Windows :** Consultez le [guide spÃ©cifique Windows (WINDOWS.md)](WINDOWS.md) qui adapte ce TP pour Windows avec Minikube ou WSL2. Voir aussi le [guide d'installation Windows complet](../docs/WINDOWS_SETUP.md).

## Objectifs du TP

Ã€ la fin de ce TP, vous serez capable de :
- Installer et configurer un cluster Kubernetes (minikube ou kubeadm)
- DÃ©marrer un cluster Kubernetes
- DÃ©ployer votre premiÃ¨re application
- Exposer l'application via un service
- Interagir avec les pods et services

## PrÃ©requis

### Pour minikube (dÃ©veloppement local)
- Une machine AlmaLinux (physique ou virtuelle) **ou Windows** ([voir guide Windows](WINDOWS.md))
- 2 CPU minimum (4 CPU recommandÃ© pour Windows)
- 2 Go de RAM minimum (4 Go recommandÃ© pour Windows)
- 20 Go d'espace disque
- AccÃ¨s root ou sudo (ou droits administrateur sur Windows)

### Pour kubeadm (environnement multi-nÅ“uds)
- 2-3 machines AlmaLinux (1 master + 1-2 workers) **ou WSL2 sur Windows**
- **Master :** 2 CPU, 2 Go RAM, 20 Go disque
- **Workers :** 1 CPU, 1 Go RAM, 20 Go disque
- RÃ©seau entre les machines
- AccÃ¨s root ou sudo

## Choix de votre environnement

Ce TP peut Ãªtre rÃ©alisÃ© avec **deux approches diffÃ©rentes** :

### Option A : minikube (recommandÃ© pour dÃ©buter)
- âœ… Installation rapide et simple
- âœ… IdÃ©al pour le dÃ©veloppement local
- âœ… NÃ©cessite une seule machine
- âœ… Gestion automatique du rÃ©seau
- âŒ Ne reflÃ¨te pas un environnement de production
- âŒ Limitations pour le multi-nÅ“ud

### Option B : kubeadm (recommandÃ© pour la production)
- âœ… Architecture rÃ©aliste multi-nÅ“uds
- âœ… Proche d'un environnement de production
- âœ… ContrÃ´le total sur la configuration
- âœ… ScalabilitÃ© native
- âŒ Installation plus complexe
- âŒ NÃ©cessite plusieurs machines

**ğŸ’¡ Conseil :** Commencez avec minikube pour apprendre les concepts, puis passez Ã  kubeadm pour comprendre la production.

---

## Partie 1 : Installation de l'environnement

### 1.1 Mise Ã  jour du systÃ¨me

```bash
sudo dnf update -y
```

### 1.2 Installation de Docker

```bash
# Installer les dÃ©pendances
sudo dnf install -y yum-utils device-mapper-persistent-data lvm2

# Ajouter le repository Docker
sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

# Installer Docker
sudo dnf install -y docker-ce docker-ce-cli containerd.io

# DÃ©marrer et activer Docker
sudo systemctl start docker
sudo systemctl enable docker

# Ajouter votre utilisateur au groupe docker
sudo usermod -aG docker $USER

# Appliquer les changements (ou se reconnecter)
newgrp docker
```

### 1.3 Installation de kubectl

```bash
# TÃ©lÃ©charger kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"

# Rendre le binaire exÃ©cutable
chmod +x kubectl

# DÃ©placer vers /usr/local/bin
sudo mv kubectl /usr/local/bin/

# VÃ©rifier l'installation
kubectl version --client
```

### 1.4 Installation de minikube (Option A)

**Si vous choisissez minikube :**

```bash
# TÃ©lÃ©charger minikube
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64

# Installer minikube
sudo install minikube-linux-amd64 /usr/local/bin/minikube

# VÃ©rifier l'installation
minikube version
```

### 1.5 Installation de kubeadm (Option B)

**Si vous choisissez kubeadm :**

Pour une installation complÃ¨te avec kubeadm, consultez le **[Guide d'installation kubeadm](../docs/KUBEADM_SETUP.md)** qui couvre :
- La prÃ©paration des nÅ“uds (dÃ©sactivation swap, modules kernel, etc.)
- L'installation de containerd
- L'installation de kubeadm, kubelet et kubectl
- L'initialisation du cluster
- L'ajout de workers
- La configuration du rÃ©seau (CNI)

**Installation rapide (rÃ©sumÃ©) :**

```bash
# Sur TOUS les nÅ“uds (master et workers)

# 1. DÃ©sactiver swap et SELinux
sudo swapoff -a
sudo sed -i '/ swap / s/^/#/' /etc/fstab
sudo setenforce 0
sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

# 2. Configurer les modules kernel
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sudo sysctl --system

# 3. Installer containerd
sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
sudo dnf install -y containerd.io
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml
sudo systemctl restart containerd
sudo systemctl enable containerd

# 4. Installer kubeadm, kubelet et kubectl
cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/repodata/repomd.xml.key
exclude=kubelet kubeadm kubectl cri-tools kubernetes-cni
EOF

sudo dnf install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
sudo systemctl enable --now kubelet
```

**Sur le nÅ“ud MASTER uniquement :**

```bash
# Initialiser le cluster
sudo kubeadm init --pod-network-cidr=10.244.0.0/16

# Configurer kubectl
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

# Installer Flannel (CNI)
kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
```

**Sur chaque nÅ“ud WORKER :**

```bash
# Utiliser la commande 'kubeadm join' affichÃ©e aprÃ¨s l'init sur le master
# Exemple :
# sudo kubeadm join <master-ip>:6443 --token <token> \
#   --discovery-token-ca-cert-hash sha256:<hash>
```

**ğŸ’¡ Note :** Consultez le [guide complet kubeadm](../docs/KUBEADM_SETUP.md) pour plus de dÃ©tails et le dÃ©pannage.

---

## Partie 2 : DÃ©marrage du cluster Kubernetes

### 2.1 Option A : DÃ©marrer minikube

**Si vous utilisez minikube :**

```bash
# DÃ©marrer minikube avec Docker comme driver
minikube start --driver=docker

# VÃ©rifier le statut
minikube status
```

**RÃ©sultat attendu :**
```
minikube
type: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured
```

### 2.1 Option B : VÃ©rifier le cluster kubeadm

**Si vous utilisez kubeadm :**

AprÃ¨s avoir suivi les Ã©tapes d'installation de la section 1.5, vÃ©rifiez que votre cluster est opÃ©rationnel :

```bash
# VÃ©rifier que tous les pods systÃ¨me sont prÃªts
kubectl get pods -n kube-system

# Attendre que tous les pods soient Running
kubectl wait --for=condition=ready pod --all -n kube-system --timeout=300s
```

**RÃ©sultat attendu :** Tous les pods (coredns, flannel, kube-proxy, etc.) doivent Ãªtre en Ã©tat `Running`.

### 2.2 VÃ©rifier le cluster

**Ces commandes fonctionnent pour minikube ET kubeadm :**

```bash
# Afficher les informations du cluster
kubectl cluster-info

# Lister les nÅ“uds
kubectl get nodes

# Afficher plus de dÃ©tails sur les nÅ“uds
kubectl describe nodes
```

**Avec minikube, vous verrez :**
```
NAME       STATUS   ROLES           AGE   VERSION
minikube   Ready    control-plane   5m    v1.29.0
```

**Avec kubeadm (exemple 1 master + 2 workers), vous verrez :**
```
NAME              STATUS   ROLES           AGE   VERSION
master-node       Ready    control-plane   10m   v1.29.0
worker-node-1     Ready    <none>          5m    v1.29.0
worker-node-2     Ready    <none>          4m    v1.29.0
```

## Partie 3 : Premier dÃ©ploiement

### 3.1 DÃ©ployer une application Nginx

```bash
# CrÃ©er un dÃ©ploiement nginx
kubectl create deployment nginx-demo --image=nginx:latest

# VÃ©rifier le dÃ©ploiement
kubectl get deployments

# VÃ©rifier les pods
kubectl get pods
```

### 3.2 Comprendre ce qui se passe en coulisse

Lorsque vous exÃ©cutez `kubectl create deployment nginx-demo --image=nginx:latest`, voici le processus complet qui se dÃ©roule dans votre cluster Kubernetes :

#### Le flux de crÃ©ation du dÃ©ploiement

**1. kubectl â†’ API Server**
- `kubectl` envoie une requÃªte HTTP REST Ã  l'**API Server** de Kubernetes
- L'API Server authentifie et autorise la requÃªte
- La dÃ©finition du Deployment est stockÃ©e dans **etcd** (la base de donnÃ©es du cluster)

**2. Deployment Controller**
- Le **Deployment Controller** (dans `kube-controller-manager`) dÃ©tecte le nouveau Deployment
- Il crÃ©e automatiquement un **ReplicaSet** pour gÃ©rer les pods
- Le ReplicaSet spÃ©cifie 1 rÃ©plica par dÃ©faut

**3. ReplicaSet Controller**
- Le **ReplicaSet Controller** crÃ©e la dÃ©finition d'un **Pod** avec le conteneur nginx

**4. Scheduler**
- Le **kube-scheduler** cherche le meilleur nÅ“ud disponible
- Il assigne le pod Ã  un nÅ“ud en fonction des ressources disponibles

**5. Kubelet (sur le nÅ“ud sÃ©lectionnÃ©)**
- Le **kubelet** reÃ§oit l'instruction de crÃ©er le pod
- Il communique avec le **container runtime** (containerd, Docker, CRI-O)
- C'est ici que l'image va Ãªtre tÃ©lÃ©chargÃ©e !

#### OÃ¹ Kubernetes va-t-il chercher l'image ?

Quand vous spÃ©cifiez `--image=nginx:latest`, le nom complet implicite est :

```
docker.io/library/nginx:latest
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”¬â”€â”˜ â””â”€â”€â”¬â”€â”˜
  Registry  Namespace Nom  Tag
```

- **Registry** : `docker.io` (Docker Hub) - **DÃ‰FAUT** si non spÃ©cifiÃ©
- **Namespace** : `library` (images officielles Docker) - **DÃ‰FAUT**
- **Image** : `nginx`
- **Tag** : `latest`

**Processus de tÃ©lÃ©chargement :**

1. Le kubelet demande au container runtime l'image `nginx:latest`
2. Le runtime vÃ©rifie si l'image existe localement
3. Si elle n'existe pas, il contacte `https://registry-1.docker.io`
4. Il tÃ©lÃ©charge les diffÃ©rentes couches (layers) de l'image
5. Il extrait et assemble l'image
6. Il crÃ©e et dÃ©marre le conteneur

**SchÃ©ma du flux complet :**

```
Vous (kubectl)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Cluster Kubernetes                   â”‚
â”‚                                          â”‚
â”‚  API Server â†’ etcd                       â”‚
â”‚       â†“                                  â”‚
â”‚  Deployment Controller                   â”‚
â”‚       â†“                                  â”‚
â”‚  ReplicaSet Controller                   â”‚
â”‚       â†“                                  â”‚
â”‚  Scheduler                               â”‚
â”‚       â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  NÅ“ud (minikube)       â”‚             â”‚
â”‚  â”‚                        â”‚             â”‚
â”‚  â”‚  Kubelet               â”‚             â”‚
â”‚  â”‚    â†“                   â”‚             â”‚
â”‚  â”‚  Container Runtime     â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚           â”‚                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ Pull image
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Docker Hub    â”‚  registry-1.docker.io
    â”‚  nginx:latest   â”‚  (image officielle)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### VÃ©rifier le processus en temps rÃ©el

Vous pouvez observer ce qui se passe avec les commandes suivantes :

```bash
# Voir les Ã©vÃ©nements du cluster en temps rÃ©el
kubectl get events --watch

# Voir les dÃ©tails du dÃ©ploiement d'un pod
kubectl describe pod <pod-name>
```

Dans les Ã©vÃ©nements, vous verrez :
```
Type    Reason     Message
----    ------     -------
Normal  Scheduled  Successfully assigned default/nginx-demo-xxx to minikube
Normal  Pulling    Pulling image "nginx:latest"
Normal  Pulled     Successfully pulled image "nginx:latest"
Normal  Created    Created container nginx
Normal  Started    Started container nginx
```

#### Autres registries disponibles

Kubernetes peut tÃ©lÃ©charger des images depuis n'importe quel registry :

```bash
# Docker Hub (dÃ©faut)
kubectl create deployment nginx --image=nginx:latest

# Google Container Registry
kubectl create deployment nginx --image=gcr.io/project/nginx:v1

# Amazon ECR
kubectl create deployment nginx --image=123456789.dkr.ecr.us-east-1.amazonaws.com/nginx:v1

# Registry privÃ©
kubectl create deployment nginx --image=registry.example.com:5000/nginx:v1
```

#### Politique de pull d'image

Le comportement de tÃ©lÃ©chargement dÃ©pend du tag et de la politique `imagePullPolicy` :

- **Tag `latest`** : Kubernetes tÃ©lÃ©charge toujours l'image (`imagePullPolicy: Always`)
- **Tag spÃ©cifique** (ex: `nginx:1.24`) : Kubernetes utilise l'image locale si disponible (`imagePullPolicy: IfNotPresent`)

**ğŸ’¡ Astuce :** En production, Ã©vitez d'utiliser le tag `latest`. PrÃ©fÃ©rez des versions spÃ©cifiques (ex: `nginx:1.24-alpine`) pour garantir la reproductibilitÃ©.

### 3.3 Examiner le pod

```bash
# Obtenir plus d'informations sur le pod
kubectl get pods -o wide

# DÃ©crire le pod (remplacer <pod-name> par le nom rÃ©el)
kubectl describe pod <pod-name>

# Voir les logs du pod
kubectl logs <pod-name>
```

## Partie 4 : Comprendre les types de Service Kubernetes

Avant d'exposer notre application, il est important de comprendre les diffÃ©rents types de services disponibles dans Kubernetes. Un **Service** est une abstraction qui dÃ©finit un ensemble logique de pods et une politique d'accÃ¨s Ã  ces pods.

### 4.1 Les trois types de Service principaux

#### ClusterIP (par dÃ©faut)

**Description :** Expose le service sur une IP interne au cluster. Ce type rend le service accessible uniquement depuis l'intÃ©rieur du cluster Kubernetes.

**Cas d'usage :**
- Communication entre services internes (ex: backend vers base de donnÃ©es)
- Services qui ne doivent pas Ãªtre accessibles depuis l'extÃ©rieur
- Micro-services communiquant entre eux

**Exemple :**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-internal-service
spec:
  type: ClusterIP  # Peut Ãªtre omis car c'est la valeur par dÃ©faut
  selector:
    app: my-app
  ports:
  - protocol: TCP
    port: 80          # Port du service
    targetPort: 8080  # Port du conteneur
```

**SchÃ©ma conceptuel :**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Cluster Kubernetes          â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Pod A   â”‚â”€â”€â”€â”€â”€â–¶â”‚ Service  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ClusterIP â”‚   â”‚
â”‚                    â”‚ 10.0.0.5 â”‚   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”‚  Pod B   â”‚â”€â”€â”€â”€â”€â–¶      â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â–¼         â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚                    â”‚  Pods    â”‚    â”‚
â”‚                    â”‚  Backend â”‚    â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### NodePort

**Description :** Expose le service sur un port statique de chaque nÅ“ud du cluster. Kubernetes alloue automatiquement un port dans la plage 30000-32767 (configurable). Le service devient accessible depuis l'extÃ©rieur via `<NodeIP>:<NodePort>`.

**Cas d'usage :**
- Environnements de dÃ©veloppement/test (comme minikube)
- Applications qui doivent Ãªtre accessibles depuis l'extÃ©rieur sans load balancer
- AccÃ¨s direct pour le dÃ©bogage

**Exemple :**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-nodeport-service
spec:
  type: NodePort
  selector:
    app: my-app
  ports:
  - protocol: TCP
    port: 80           # Port du service
    targetPort: 8080   # Port du conteneur
    nodePort: 30080    # Port sur chaque nÅ“ud (optionnel, sinon auto-assignÃ©)
```

**SchÃ©ma conceptuel :**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Cluster Kubernetes            â”‚
â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   Node     â”‚    â”‚ Service  â”‚      â”‚
â”‚  â”‚192.168.1.10â”‚    â”‚ NodePort â”‚      â”‚
â”‚  â”‚Port: 30080 â”‚â—€â”€â”€â”€â”‚          â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                 â”‚           â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                       â”‚  Pods    â”‚   â”‚
â”‚                       â”‚  Backend â”‚   â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²
         â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Client â”‚ accÃ¨de via http://192.168.1.10:30080
    â”‚Externe â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### LoadBalancer

**Description :** Expose le service via un load balancer externe fourni par le cloud provider (AWS ELB, GCP Load Balancer, Azure Load Balancer, etc.). C'est une extension du type NodePort : un service LoadBalancer crÃ©e automatiquement un NodePort et demande au cloud provider de crÃ©er un load balancer pointant vers ce NodePort.

**Cas d'usage :**
- Applications en production sur des plateformes cloud
- Services qui nÃ©cessitent une IP publique stable
- Distribution automatique du trafic avec haute disponibilitÃ©

**Exemple :**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-loadbalancer-service
spec:
  type: LoadBalancer
  selector:
    app: my-app
  ports:
  - protocol: TCP
    port: 80           # Port du load balancer
    targetPort: 8080   # Port du conteneur
```

**SchÃ©ma conceptuel :**
```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Client â”‚
    â”‚Internetâ”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Load Balancer   â”‚  â—€â”€â”€â”€ IP Publique: 203.0.113.10
â”‚  (Cloud Provider)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Cluster Kubernetes             â”‚
â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   Nodes    â”‚    â”‚ Service  â”‚      â”‚
â”‚  â”‚:30080-32767â”‚â—€â”€â”€â”€â”‚LoadBal.  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                 â”‚           â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                       â”‚  Pods    â”‚   â”‚
â”‚                       â”‚  Backend â”‚   â”‚
â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Note sur minikube :** Dans un environnement minikube (cluster local), le type LoadBalancer sera automatiquement converti en NodePort car il n'y a pas de cloud provider pour crÃ©er un vrai load balancer. Minikube fournit la commande `minikube tunnel` pour simuler un load balancer en environnement local.

### 4.2 Tableau comparatif

| Type | Accessible depuis | IP externe | Cas d'usage principal | Port Range |
|------|-------------------|------------|----------------------|------------|
| **ClusterIP** | Cluster uniquement | Non | Services internes | Port du service (ex: 80, 3306) |
| **NodePort** | Externe (NodeIP:Port) | Non | Dev/Test, accÃ¨s direct | 30000-32767 |
| **LoadBalancer** | Externe (via LB) | Oui | Production cloud | Standard (80, 443, etc.) |

### 4.3 Comment choisir le bon type ?

```
Besoin d'accÃ¨s externe ?
â”‚
â”œâ”€ NON  â”€â”€â–¶ ClusterIP
â”‚           (communication interne)
â”‚
â””â”€ OUI
    â”‚
    â”œâ”€ Environnement local/dev ?
    â”‚  OUI â”€â”€â–¶ NodePort
    â”‚          (accÃ¨s via IP:Port du nÅ“ud)
    â”‚
    â””â”€ NON (Production cloud)
       â””â”€â”€â–¶ LoadBalancer
            (IP publique + distribution)
```

## Partie 5 : Exposition de l'application

### 5.1 Exemples concrets pour chaque type de service

Voici des exemples pratiques et dÃ©ployables pour chaque type de service. Vous pouvez les tester directement dans votre cluster.

#### Exemple 1 : Service ClusterIP - Base de donnÃ©es Redis

**ScÃ©nario :** DÃ©ployer une base de donnÃ©es Redis qui sera utilisÃ©e uniquement par d'autres applications dans le cluster.

```yaml
# redis-clusterip.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-backend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        tier: backend
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
---
apiVersion: v1
kind: Service
metadata:
  name: redis-service
spec:
  type: ClusterIP  # Accessible uniquement depuis l'intÃ©rieur du cluster
  selector:
    app: redis
    tier: backend
  ports:
  - protocol: TCP
    port: 6379
    targetPort: 6379
```

**DÃ©ploiement et test :**
```bash
# DÃ©ployer Redis avec ClusterIP
kubectl apply -f redis-clusterip.yaml

# VÃ©rifier le service (notez l'IP interne)
kubectl get service redis-service

# Tester l'accÃ¨s depuis un pod temporaire dans le cluster
kubectl run redis-client --rm -it --image=redis:7-alpine -- redis-cli -h redis-service ping
# RÃ©sultat attendu : PONG

# Essayer d'accÃ©der depuis l'extÃ©rieur (cela Ã©chouera car ClusterIP est interne)
# curl http://<CLUSTER-IP>:6379  # Ne fonctionnera pas depuis votre machine
```

**Cas d'usage rÃ©el :** Base de donnÃ©es pour une API, cache interne, message queue (RabbitMQ, Kafka), services de stockage interne.

---

#### Exemple 2 : Service NodePort - Application de dÃ©veloppement

**ScÃ©nario :** DÃ©ployer une application web simple accessible depuis l'extÃ©rieur pour les tests et le dÃ©veloppement.

```yaml
# webapp-nodeport.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-dev
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp
      env: dev
  template:
    metadata:
      labels:
        app: webapp
        env: dev
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: html
          mountPath: /usr/share/nginx/html
      volumes:
      - name: html
        configMap:
          name: webapp-html
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: webapp-html
data:
  index.html: |
    <!DOCTYPE html>
    <html>
    <head><title>Application de DÃ©veloppement</title></head>
    <body>
      <h1>ğŸš€ Application NodePort</h1>
      <p>Cette application est exposÃ©e via NodePort et accessible depuis l'extÃ©rieur du cluster.</p>
      <p>Hostname: <span id="hostname"></span></p>
      <script>
        fetch('/hostname.txt').then(r => r.text()).then(h => {
          document.getElementById('hostname').textContent = h || window.location.hostname;
        }).catch(() => {
          document.getElementById('hostname').textContent = window.location.hostname;
        });
      </script>
    </body>
    </html>
---
apiVersion: v1
kind: Service
metadata:
  name: webapp-nodeport
spec:
  type: NodePort
  selector:
    app: webapp
    env: dev
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
    nodePort: 30100  # Port fixe pour un accÃ¨s prÃ©visible
```

**DÃ©ploiement et test :**
```bash
# DÃ©ployer l'application avec NodePort
kubectl apply -f webapp-nodeport.yaml

# VÃ©rifier le service et noter le NodePort
kubectl get service webapp-nodeport

# Option A : Avec minikube
minikube service webapp-nodeport --url
curl $(minikube service webapp-nodeport --url)

# Option B : Avec kubeadm
export NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')
curl http://$NODE_IP:30100

# Ouvrir dans le navigateur
# http://<NODE-IP>:30100
```

**Cas d'usage rÃ©el :** Applications de dÃ©veloppement/test, API de dÃ©bogage, dashboards internes, dÃ©mos temporaires.

---

#### Exemple 3 : Service LoadBalancer - Frontend web en production

**ScÃ©nario :** DÃ©ployer une application web frontend qui doit Ãªtre accessible publiquement avec une IP stable.

```yaml
# frontend-loadbalancer.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend-prod
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend
      tier: web
  template:
    metadata:
      labels:
        app: frontend
        tier: web
    spec:
      containers:
      - name: nginx
        image: nginx:1.24-alpine
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "200m"
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 5
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 3
---
apiVersion: v1
kind: Service
metadata:
  name: frontend-loadbalancer
  annotations:
    # Annotations spÃ©cifiques aux cloud providers (exemples)
    # AWS ELB
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    # GCP
    cloud.google.com/load-balancer-type: "External"
    # Azure
    service.beta.kubernetes.io/azure-load-balancer-internal: "false"
spec:
  type: LoadBalancer
  selector:
    app: frontend
    tier: web
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
  sessionAffinity: ClientIP  # Optionnel : maintenir les sessions utilisateur
```

**DÃ©ploiement et test :**
```bash
# DÃ©ployer l'application avec LoadBalancer
kubectl apply -f frontend-loadbalancer.yaml

# VÃ©rifier le service
kubectl get service frontend-loadbalancer -w
# Attendez que EXTERNAL-IP passe de <pending> Ã  une IP rÃ©elle

# Sur un cloud provider (AWS, GCP, Azure)
export LB_IP=$(kubectl get service frontend-loadbalancer -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
curl http://$LB_IP

# Avec minikube (simulation de LoadBalancer)
# Terminal 1 : CrÃ©er un tunnel
minikube tunnel

# Terminal 2 : Tester l'accÃ¨s
kubectl get service frontend-loadbalancer
# L'EXTERNAL-IP sera maintenant 127.0.0.1
curl http://127.0.0.1

# Tester la haute disponibilitÃ©
# Supprimer un pod et vÃ©rifier que le service reste accessible
kubectl delete pod $(kubectl get pods -l app=frontend -o jsonpath='{.items[0].metadata.name}')
curl http://$LB_IP  # Fonctionne toujours grÃ¢ce aux autres rÃ©plicas
```

**Cas d'usage rÃ©el :** Site web public, API REST publique, application SaaS, microservices exposÃ©s Ã  des clients externes.

---

#### Exemple 4 : Architecture complÃ¨te (3 tiers)

**ScÃ©nario :** Application complÃ¨te avec frontend (LoadBalancer), backend (ClusterIP), et base de donnÃ©es (ClusterIP).

```yaml
# architecture-complete.yaml
# Base de donnÃ©es (ClusterIP - interne uniquement)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: database
spec:
  replicas: 1
  selector:
    matchLabels:
      app: db
  template:
    metadata:
      labels:
        app: db
    spec:
      containers:
      - name: postgres
        image: postgres:15-alpine
        env:
        - name: POSTGRES_PASSWORD
          value: "secretpassword"
        - name: POSTGRES_DB
          value: "appdb"
        ports:
        - containerPort: 5432
---
apiVersion: v1
kind: Service
metadata:
  name: database-service
spec:
  type: ClusterIP  # Accessible uniquement depuis le cluster
  selector:
    app: db
  ports:
  - port: 5432
    targetPort: 5432
---
# Backend API (ClusterIP - appelÃ© par le frontend)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-api
spec:
  replicas: 2
  selector:
    matchLabels:
      app: api
  template:
    metadata:
      labels:
        app: api
    spec:
      containers:
      - name: api
        image: httpd:2.4-alpine  # Remplacer par votre API rÃ©elle
        ports:
        - containerPort: 80
        env:
        - name: DATABASE_HOST
          value: "database-service"  # Utilise le nom du service
        - name: DATABASE_PORT
          value: "5432"
---
apiVersion: v1
kind: Service
metadata:
  name: backend-api-service
spec:
  type: ClusterIP  # Interne, appelÃ© uniquement par le frontend
  selector:
    app: api
  ports:
  - port: 8080
    targetPort: 80
---
# Frontend (LoadBalancer - accessible publiquement)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
        env:
        - name: API_URL
          value: "http://backend-api-service:8080"  # Utilise le nom du service
---
apiVersion: v1
kind: Service
metadata:
  name: frontend-service
spec:
  type: LoadBalancer  # Accessible depuis Internet
  selector:
    app: frontend
  ports:
  - port: 80
    targetPort: 80
```

**DÃ©ploiement et test :**
```bash
# DÃ©ployer toute l'architecture
kubectl apply -f architecture-complete.yaml

# VÃ©rifier tous les services
kubectl get services
# Vous devriez voir :
# - database-service (ClusterIP)
# - backend-api-service (ClusterIP)
# - frontend-service (LoadBalancer avec EXTERNAL-IP)

# Tester la connectivitÃ© interne
kubectl run test-pod --rm -it --image=busybox -- sh
# Dans le pod :
# wget -qO- http://backend-api-service:8080
# wget -qO- http://database-service:5432
# exit

# AccÃ©der au frontend depuis l'extÃ©rieur
# Avec minikube tunnel
minikube tunnel  # Dans un terminal sÃ©parÃ©
curl http://127.0.0.1

# Sur un cloud provider
export LB_IP=$(kubectl get service frontend-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
curl http://$LB_IP
```

**Visualisation de l'architecture :**
```
Internet
   â”‚
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LoadBalancer    â”‚ â—€â”€â”€ IP Publique (ex: 203.0.113.10)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Cluster Kubernetes              â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ Frontend Serviceâ”‚ (LoadBalancer)    â”‚
â”‚  â”‚  Port 80        â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚           â”‚                             â”‚
â”‚           â–¼                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ Frontend Pods   â”‚                   â”‚
â”‚  â”‚ (3 rÃ©plicas)    â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚           â”‚                             â”‚
â”‚           â”‚ Appel HTTP interne          â”‚
â”‚           â–¼                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ Backend API Svc â”‚ (ClusterIP)       â”‚
â”‚  â”‚  Port 8080      â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚           â”‚                             â”‚
â”‚           â–¼                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ Backend Pods    â”‚                   â”‚
â”‚  â”‚ (2 rÃ©plicas)    â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚           â”‚                             â”‚
â”‚           â”‚ RequÃªte SQL                 â”‚
â”‚           â–¼                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ Database Svc    â”‚ (ClusterIP)       â”‚
â”‚  â”‚  Port 5432      â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚           â”‚                             â”‚
â”‚           â–¼                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ PostgreSQL Pod  â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 5.2 CrÃ©er un service (exemple simple)

```bash
# Exposer le dÃ©ploiement nginx-demo via un service de type NodePort
kubectl expose deployment nginx-demo --type=NodePort --port=80

# VÃ©rifier le service
kubectl get services
```

**Note :** Nous utilisons NodePort ici car minikube est un environnement local. Pour comprendre quand utiliser NodePort vs ClusterIP vs LoadBalancer, rÃ©fÃ©rez-vous Ã  la section 4 ci-dessus.

### 5.3 AccÃ©der Ã  l'application

#### Option A : Avec minikube

```bash
# Obtenir l'URL du service
minikube service nginx-demo --url

# Ou ouvrir directement dans le navigateur
minikube service nginx-demo
```

**Alternative avec curl :**
```bash
# RÃ©cupÃ©rer l'IP et le port
export NODE_PORT=$(kubectl get services nginx-demo -o jsonpath='{.spec.ports[0].nodePort}')
export NODE_IP=$(minikube ip)

# Tester l'accÃ¨s
curl http://$NODE_IP:$NODE_PORT
```

#### Option B : Avec kubeadm

Avec kubeadm, vous accÃ©dez au service via l'IP de n'importe quel nÅ“ud et le NodePort :

```bash
# RÃ©cupÃ©rer le NodePort assignÃ©
export NODE_PORT=$(kubectl get services nginx-demo -o jsonpath='{.spec.ports[0].nodePort}')

# RÃ©cupÃ©rer l'IP d'un worker (ou du master si scheduling autorisÃ©)
export NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')

# Afficher l'URL
echo "Service accessible Ã  : http://$NODE_IP:$NODE_PORT"

# Tester l'accÃ¨s
curl http://$NODE_IP:$NODE_PORT
```

**Note :** Avec kubeadm en multi-nÅ“uds, le service est accessible via **n'importe quel nÅ“ud** du cluster grÃ¢ce Ã  kube-proxy, mÃªme si le pod n'est pas sur ce nÅ“ud.

**Astuce :** Pour un accÃ¨s plus simple en production, considÃ©rez :
- **Ingress Controller** : Pour le routage HTTP/HTTPS (voir TPs suivants)
- **MetalLB** : Pour des LoadBalancers avec IP externe (voir [guide kubeadm](../docs/KUBEADM_SETUP.md#partie-6--configuration-du-loadbalancer-metallb))
- **HAProxy/Nginx externe** : Pour load balancer devant les NodePorts

## Partie 6 : Manipulation avancÃ©e

### 6.1 Scaler l'application

```bash
# Augmenter le nombre de rÃ©plicas Ã  3
kubectl scale deployment nginx-demo --replicas=3

# VÃ©rifier les pods
kubectl get pods

# Observer la distribution
kubectl get pods -o wide
```

### 6.2 Mettre Ã  jour l'application

```bash
# Mettre Ã  jour l'image vers une version spÃ©cifique
kubectl set image deployment/nginx-demo nginx=nginx:1.24

# Suivre le rollout
kubectl rollout status deployment/nginx-demo

# Voir l'historique des dÃ©ploiements
kubectl rollout history deployment/nginx-demo
```

### 6.3 Revenir Ã  la version prÃ©cÃ©dente

```bash
# Annuler le dernier dÃ©ploiement
kubectl rollout undo deployment/nginx-demo

# VÃ©rifier le statut
kubectl rollout status deployment/nginx-demo
```

## Partie 7 : Utilisation de fichiers YAML

### 7.1 CrÃ©er un fichier de dÃ©ploiement

CrÃ©er un fichier `webapp-deployment.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
  labels:
    app: webapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
      - name: webapp
        image: httpd:2.4
        ports:
        - containerPort: 80
```

### 7.2 CrÃ©er un fichier de service

CrÃ©er un fichier `webapp-service.yaml` :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  type: NodePort
  selector:
    app: webapp
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
    nodePort: 30080
```

**Note :** Ce service utilise un NodePort fixe (30080) ce qui est pratique pour le dÃ©veloppement. Consultez la Partie 4 pour comprendre quand utiliser ce type de service.

### 7.3 Appliquer les configurations

```bash
# Appliquer le dÃ©ploiement
kubectl apply -f webapp-deployment.yaml

# Appliquer le service
kubectl apply -f webapp-service.yaml

# VÃ©rifier les ressources crÃ©Ã©es
kubectl get deployments,services,pods
```

### 7.4 Tester l'application

#### Option A : Avec minikube

```bash
# AccÃ©der au service
curl http://$(minikube ip):30080
```

#### Option B : Avec kubeadm

```bash
# RÃ©cupÃ©rer l'IP d'un nÅ“ud
export NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')

# Tester l'accÃ¨s
curl http://$NODE_IP:30080

# Ou depuis un autre serveur sur le rÃ©seau
# curl http://<IP-du-noeud>:30080
```

**Note :** Avec kubeadm, le service est accessible sur le port 30080 depuis n'importe quel nÅ“ud du cluster grÃ¢ce Ã  kube-proxy.

## Partie 8 : Nettoyage et commandes utiles

### 8.1 Nettoyer les ressources

```bash
# Supprimer le dÃ©ploiement nginx-demo
kubectl delete deployment nginx-demo
kubectl delete service nginx-demo

# Supprimer les ressources webapp
kubectl delete -f webapp-deployment.yaml
kubectl delete -f webapp-service.yaml

# Ou supprimer par nom
kubectl delete deployment webapp
kubectl delete service webapp-service
```

### 8.2 Commandes utiles

#### Communes (minikube et kubeadm)

```bash
# Voir toutes les ressources dans le namespace par dÃ©faut
kubectl get all

# Voir les pods de tous les namespaces
kubectl get pods --all-namespaces

# Afficher les Ã©vÃ©nements rÃ©cents
kubectl get events --sort-by='.lastTimestamp'
```

#### SpÃ©cifiques minikube

```bash
# AccÃ©der au dashboard Kubernetes
minikube dashboard

# Voir les addons disponibles
minikube addons list

# Activer un addon (exemple: metrics-server)
minikube addons enable metrics-server

# Voir les logs de minikube
minikube logs

# SSH dans le nÅ“ud minikube
minikube ssh
```

#### SpÃ©cifiques kubeadm

```bash
# Installer le dashboard manuellement
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml

# CrÃ©er un token pour accÃ©der au dashboard
kubectl -n kubernetes-dashboard create token admin-user

# AccÃ©der au dashboard via port-forward
kubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard 8443:443

# Installer metrics-server manuellement
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# SSH dans un nÅ“ud spÃ©cifique (adapter l'IP)
ssh user@<node-ip>

# Voir les logs des composants systÃ¨me
kubectl logs -n kube-system -l component=kube-apiserver
kubectl logs -n kube-system -l k8s-app=kube-proxy
```

### 8.3 ArrÃªter et supprimer le cluster

#### Avec minikube

```bash
# ArrÃªter minikube
minikube stop

# Supprimer le cluster
minikube delete

# DÃ©marrer Ã  nouveau
minikube start
```

#### Avec kubeadm

```bash
# Pour arrÃªter le cluster, arrÃªter les VMs/serveurs ou :
# Sur chaque nÅ“ud
sudo systemctl stop kubelet

# Pour redÃ©marrer
sudo systemctl start kubelet

# Pour supprimer complÃ¨tement le cluster
# Sur tous les nÅ“uds (master et workers)
sudo kubeadm reset -f
sudo rm -rf /etc/cni/net.d
sudo rm -rf $HOME/.kube/config
sudo iptables -F && sudo iptables -t nat -F && sudo iptables -t mangle -F && sudo iptables -X

# Puis rÃ©initialiser depuis le dÃ©but si nÃ©cessaire (voir section 1.5)
```

## Exercices pratiques

### Exercice 1 : DÃ©ploiement Redis
1. DÃ©ployer une instance Redis avec l'image `redis:7-alpine`
2. L'exposer via un service de type **ClusterIP** sur le port 6379
3. VÃ©rifier que le pod est en cours d'exÃ©cution

**Pourquoi ClusterIP ?** Redis est typiquement une base de donnÃ©es backend qui doit Ãªtre accessible uniquement depuis l'intÃ©rieur du cluster par d'autres applications. Il n'a pas besoin d'Ãªtre exposÃ© Ã  l'extÃ©rieur. Voir Partie 4.1 pour plus de dÃ©tails sur ClusterIP.

### Exercice 2 : Application multi-conteneurs
1. CrÃ©er un dÃ©ploiement avec 3 rÃ©plicas d'nginx
2. CrÃ©er un service **LoadBalancer**
3. Tester l'accÃ¨s Ã  l'application
4. Scaler Ã  5 rÃ©plicas
5. Observer la distribution des pods

**Ã€ propos de LoadBalancer :**
- **Avec minikube :** Le type LoadBalancer est automatiquement converti en NodePort. Pour simuler un vrai LoadBalancer localement, vous pouvez utiliser `minikube tunnel` dans un terminal sÃ©parÃ©.
- **Avec kubeadm :** Installez MetalLB pour obtenir des IPs externes pour vos LoadBalancers (voir [guide kubeadm](../docs/KUBEADM_SETUP.md#partie-6--configuration-du-loadbalancer-metallb))

### Exercice 3 : Manipulation YAML
1. CrÃ©er un fichier YAML pour dÃ©ployer MySQL
   - Image: `mysql:8.0`
   - Variables d'environnement: `MYSQL_ROOT_PASSWORD=secret`
   - Port: 3306
2. Appliquer le dÃ©ploiement
3. VÃ©rifier les logs du pod MySQL

## Solutions des exercices

<details>
<summary>Solution Exercice 1</summary>

```bash
# CrÃ©er le dÃ©ploiement
kubectl create deployment redis-demo --image=redis:7-alpine

# CrÃ©er le service
kubectl expose deployment redis-demo --type=ClusterIP --port=6379

# VÃ©rifier
kubectl get pods,services
```
</details>

<details>
<summary>Solution Exercice 2</summary>

```bash
# CrÃ©er le dÃ©ploiement
kubectl create deployment nginx-multi --image=nginx --replicas=3

# Exposer le service
kubectl expose deployment nginx-multi --type=LoadBalancer --port=80

# Obtenir l'URL
minikube service nginx-multi --url

# Scaler
kubectl scale deployment nginx-multi --replicas=5

# Observer
kubectl get pods -o wide
```
</details>

<details>
<summary>Solution Exercice 3</summary>

Fichier `mysql-deployment.yaml` :
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: secret
        ports:
        - containerPort: 3306
```

Commandes :
```bash
kubectl apply -f mysql-deployment.yaml
kubectl get pods
kubectl logs <mysql-pod-name>
```
</details>

## DÃ©pannage

### ProblÃ¨me : minikube ne dÃ©marre pas
```bash
# VÃ©rifier Docker
sudo systemctl status docker

# VÃ©rifier les logs
minikube logs

# Supprimer et recrÃ©er
minikube delete
minikube start --driver=docker --force
```

### ProblÃ¨me : Impossible de se connecter au service
```bash
# VÃ©rifier que le service existe
kubectl get services

# VÃ©rifier les endpoints
kubectl get endpoints

# VÃ©rifier les pods
kubectl get pods

# Utiliser port-forward comme alternative
kubectl port-forward service/nginx-demo 8080:80
```

### ProblÃ¨me : Permission denied avec Docker
```bash
# S'assurer d'Ãªtre dans le groupe docker
sudo usermod -aG docker $USER

# Se reconnecter ou utiliser
newgrp docker
```

## Ressources complÃ©mentaires

- Documentation officielle Kubernetes : https://kubernetes.io/docs/
- Documentation minikube : https://minikube.sigs.k8s.io/docs/
- Tutoriels interactifs : https://kubernetes.io/docs/tutorials/
- Cheat sheet kubectl : https://kubernetes.io/docs/reference/kubectl/cheatsheet/

## Points clÃ©s Ã  retenir

1. **minikube** est un outil pour exÃ©cuter Kubernetes localement
2. **kubectl** est l'outil en ligne de commande pour interagir avec Kubernetes
3. Un **Deployment** gÃ¨re les rÃ©plicas de vos pods
4. Un **Service** expose vos pods au rÃ©seau
5. Les fichiers **YAML** permettent de dÃ©finir l'infrastructure as code
6. Le scaling est simple avec la commande `kubectl scale`
7. Les rollouts permettent des mises Ã  jour sans interruption
