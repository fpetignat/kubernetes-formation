# TP4 - Monitoring et Gestion des Logs dans Kubernetes

## Objectifs du TP

Ã€ la fin de ce TP, vous serez capable de :
- Comprendre l'observabilitÃ© dans Kubernetes
- Installer et utiliser Metrics Server pour la collecte de mÃ©triques
- Utiliser le Dashboard Kubernetes pour la visualisation
- Collecter et analyser les logs des pods
- DÃ©ployer Prometheus et Grafana pour le monitoring avancÃ©
- CrÃ©er des dashboards personnalisÃ©s
- Mettre en place des alertes
- Appliquer les bonnes pratiques de monitoring

## PrÃ©requis

- Avoir complÃ©tÃ© les TP1, TP2 et TP3
- Un cluster minikube fonctionnel
- 4 Go de RAM minimum (pour Prometheus et Grafana)
- Connaissance des Deployments et Services

## Partie 1 : Introduction Ã  l'observabilitÃ©

### 1.1 Les trois piliers de l'observabilitÃ©

L'observabilitÃ© repose sur trois piliers :

1. **MÃ©triques (Metrics)** : DonnÃ©es numÃ©riques agrÃ©gÃ©es
   - CPU, mÃ©moire, rÃ©seau
   - RequÃªtes par seconde
   - Temps de rÃ©ponse

2. **Logs** : Enregistrements d'Ã©vÃ©nements
   - Logs applicatifs
   - Logs systÃ¨me
   - Logs d'audit

3. **Traces** : Suivi des requÃªtes distribuÃ©es
   - Temps de traitement
   - Propagation entre services
   - Identification des goulots d'Ã©tranglement

### 1.2 Architecture de monitoring Kubernetes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Grafana (Visualisation)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Prometheus (Collecte)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚             â”‚             â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚ Node    â”‚   â”‚ Pods    â”‚   â”‚ cAdvisorâ”‚
    â”‚ Exporterâ”‚   â”‚ Metrics â”‚   â”‚         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Partie 2 : Metrics Server

### 2.1 Qu'est-ce que Metrics Server ?

Metrics Server collecte les mÃ©triques de ressources (CPU, mÃ©moire) des nÅ“uds et pods via l'API kubelet.

### 2.2 Installation de Metrics Server

```bash
# Activer l'addon metrics-server dans minikube
minikube addons enable metrics-server

# VÃ©rifier le dÃ©ploiement
kubectl get deployment metrics-server -n kube-system

# Attendre que le pod soit prÃªt
kubectl wait --for=condition=ready pod -l k8s-app=metrics-server -n kube-system --timeout=60s
```

### 2.3 Utiliser les mÃ©triques

**Exercice 1 : MÃ©triques de base**

```bash
# Voir l'utilisation des nÅ“uds
kubectl top nodes

# CrÃ©er un dÃ©ploiement de test
kubectl create deployment nginx-test --image=nginx:alpine --replicas=3

# Attendre que les pods soient prÃªts
kubectl wait --for=condition=ready pod -l app=nginx-test --timeout=60s

# Voir l'utilisation des pods
kubectl top pods

# MÃ©triques d'un namespace spÃ©cifique
kubectl top pods -n kube-system

# Trier par consommation CPU
kubectl top pods --sort-by=cpu

# Trier par consommation mÃ©moire
kubectl top pods --sort-by=memory
```

**Note** : Les mÃ©triques peuvent prendre 1-2 minutes pour apparaÃ®tre aprÃ¨s l'installation.

### 2.4 Horizontal Pod Autoscaler (HPA)

Le HPA ajuste automatiquement le nombre de replicas basÃ© sur les mÃ©triques.

CrÃ©er `01-hpa-demo.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-apache
spec:
  replicas: 1
  selector:
    matchLabels:
      app: php-apache
  template:
    metadata:
      labels:
        app: php-apache
    spec:
      containers:
      - name: php-apache
        image: registry.k8s.io/hpa-example
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 200m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 256Mi
---
apiVersion: v1
kind: Service
metadata:
  name: php-apache
spec:
  selector:
    app: php-apache
  ports:
  - port: 80
    targetPort: 80
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
```

**Exercice 2 : Tester l'autoscaling**

```bash
# Appliquer le manifest
kubectl apply -f 01-hpa-demo.yaml

# VÃ©rifier le HPA
kubectl get hpa

# Observer l'Ã©tat initial
kubectl get hpa php-apache-hpa -w &

# Dans un autre terminal, gÃ©nÃ©rer de la charge
kubectl run load-generator --image=busybox --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://php-apache; done"

# Observer le scaling (attendre 2-3 minutes)
kubectl get pods -l app=php-apache -w

# AprÃ¨s quelques minutes, arrÃªter le gÃ©nÃ©rateur de charge
kubectl delete pod load-generator

# Observer le scale down (prend environ 5 minutes)
kubectl get hpa php-apache-hpa -w
```

## Partie 3 : Dashboard Kubernetes

### 3.1 Activer le Dashboard

```bash
# Activer le dashboard dans minikube
minikube addons enable dashboard

# VÃ©rifier le dÃ©ploiement
kubectl get pods -n kubernetes-dashboard

# Lancer le dashboard
minikube dashboard
```

Le dashboard s'ouvre automatiquement dans votre navigateur.

### 3.2 Explorer le Dashboard

**FonctionnalitÃ©s principales** :
- Vue d'ensemble du cluster
- Ã‰tat des dÃ©ploiements, pods, services
- Utilisation des ressources (CPU, mÃ©moire)
- Logs des pods
- ExÃ©cution de commandes dans les pods
- Ã‰dition des ressources

**Exercice 3 : Navigation dans le Dashboard**

1. Naviguez vers "Workloads" â†’ "Deployments"
2. SÃ©lectionnez le namespace "default"
3. Cliquez sur un dÃ©ploiement
4. Explorez les informations affichÃ©es
5. Cliquez sur "Pods" et sÃ©lectionnez un pod
6. Consultez les logs du pod
7. Essayez la fonction "Exec" pour ouvrir un shell

## Partie 4 : Gestion des Logs

### 4.1 Logs avec kubectl

```bash
# Voir les logs d'un pod
kubectl logs <pod-name>

# Logs en temps rÃ©el (-f = follow)
kubectl logs -f <pod-name>

# Logs d'un conteneur spÃ©cifique dans un pod multi-conteneurs
kubectl logs <pod-name> -c <container-name>

# Logs du conteneur prÃ©cÃ©dent (en cas de crash)
kubectl logs <pod-name> --previous

# DerniÃ¨res N lignes
kubectl logs <pod-name> --tail=50

# Logs depuis X minutes
kubectl logs <pod-name> --since=5m

# Logs avec timestamps
kubectl logs <pod-name> --timestamps
```

### 4.2 Application de dÃ©monstration avec logs

CrÃ©er `02-logging-demo.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: log-generator
spec:
  replicas: 2
  selector:
    matchLabels:
      app: log-generator
  template:
    metadata:
      labels:
        app: log-generator
    spec:
      containers:
      - name: logger
        image: busybox
        command: ["/bin/sh"]
        args:
          - -c
          - >
            while true; do
              echo "$(date '+%Y-%m-%d %H:%M:%S') [INFO] Application running - Request ID: $RANDOM";
              sleep 2;
              if [ $((RANDOM % 10)) -eq 0 ]; then
                echo "$(date '+%Y-%m-%d %H:%M:%S') [ERROR] Something went wrong!";
              fi
            done
```

**Exercice 4 : Analyser les logs**

```bash
# DÃ©ployer l'application
kubectl apply -f 02-logging-demo.yaml

# Lister les pods
kubectl get pods -l app=log-generator

# Voir les logs d'un pod
POD_NAME=$(kubectl get pods -l app=log-generator -o jsonpath='{.items[0].metadata.name}')
kubectl logs $POD_NAME

# Suivre les logs en temps rÃ©el
kubectl logs -f $POD_NAME

# Rechercher les erreurs
kubectl logs $POD_NAME | grep ERROR

# Logs de tous les pods avec le label app=log-generator
kubectl logs -l app=log-generator --all-containers=true

# Logs depuis les 5 derniÃ¨res minutes
kubectl logs $POD_NAME --since=5m
```

### 4.3 Logging multi-conteneurs

CrÃ©er `03-multi-container-logging.yaml` :

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: multi-log-pod
spec:
  containers:
  - name: frontend
    image: busybox
    command: ["/bin/sh"]
    args:
      - -c
      - >
        while true; do
          echo "[FRONTEND] Handling request at $(date)";
          sleep 3;
        done

  - name: backend
    image: busybox
    command: ["/bin/sh"]
    args:
      - -c
      - >
        while true; do
          echo "[BACKEND] Processing data at $(date)";
          sleep 5;
        done

  - name: cache
    image: busybox
    command: ["/bin/sh"]
    args:
      - -c
      - >
        while true; do
          echo "[CACHE] Cache hit/miss at $(date)";
          sleep 4;
        done
```

**Exercice 5 : Logs multi-conteneurs**

```bash
# CrÃ©er le pod
kubectl apply -f 03-multi-container-logging.yaml

# Voir les logs de chaque conteneur
kubectl logs multi-log-pod -c frontend
kubectl logs multi-log-pod -c backend
kubectl logs multi-log-pod -c cache

# Suivre les logs de plusieurs conteneurs en parallÃ¨le (dans des terminaux sÃ©parÃ©s)
kubectl logs -f multi-log-pod -c frontend
kubectl logs -f multi-log-pod -c backend
kubectl logs -f multi-log-pod -c cache

# Astuce : Utiliser stern pour voir tous les logs
# Installation de stern (optionnel)
# wget https://github.com/stern/stern/releases/download/v1.27.0/stern_1.27.0_linux_amd64.tar.gz
# tar xzf stern_1.27.0_linux_amd64.tar.gz
# sudo mv stern /usr/local/bin/

# Utilisation de stern (si installÃ©)
# stern multi-log-pod
```

## Partie 5 : Prometheus et Grafana

### 5.1 Architecture Prometheus

Prometheus est un systÃ¨me de monitoring et d'alerte open-source :
- **Collecte** : Scrape les mÃ©triques des cibles
- **Stockage** : Base de donnÃ©es time-series
- **RequÃªtes** : Langage PromQL
- **Alertes** : RÃ¨gles d'alerte configurables

### 5.2 Installation de Prometheus

CrÃ©er `04-prometheus-deployment.yaml` :

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s

    scrape_configs:
      - job_name: 'kubernetes-nodes'
        kubernetes_sd_configs:
          - role: node
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)

      - job_name: 'kubernetes-cadvisor'
        kubernetes_sd_configs:
          - role: node
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __metrics_path__
            replacement: /metrics/cadvisor

      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__

      - job_name: 'kubernetes-service-endpoints'
        kubernetes_sd_configs:
          - role: endpoints
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      serviceAccountName: prometheus
      containers:
      - name: prometheus
        image: prom/prometheus:v2.45.0
        args:
          - '--config.file=/etc/prometheus/prometheus.yml'
          - '--storage.tsdb.path=/prometheus'
          - '--web.console.libraries=/usr/share/prometheus/console_libraries'
          - '--web.console.templates=/usr/share/prometheus/consoles'
        ports:
        - containerPort: 9090
          name: web
        volumeMounts:
        - name: prometheus-config
          mountPath: /etc/prometheus
        - name: prometheus-storage
          mountPath: /prometheus
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
      volumes:
      - name: prometheus-config
        configMap:
          name: prometheus-config
      - name: prometheus-storage
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: monitoring
spec:
  type: NodePort
  selector:
    app: prometheus
  ports:
  - port: 9090
    targetPort: 9090
    nodePort: 30090
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: monitoring
```

**Exercice 6 : DÃ©ployer Prometheus**

```bash
# Appliquer la configuration
kubectl apply -f 04-prometheus-deployment.yaml

# VÃ©rifier le dÃ©ploiement
kubectl get all -n monitoring

# Attendre que Prometheus soit prÃªt
kubectl wait --for=condition=ready pod -l app=prometheus -n monitoring --timeout=120s

# Obtenir l'URL de Prometheus
minikube service prometheus -n monitoring --url

# Ou avec port-forward
kubectl port-forward -n monitoring svc/prometheus 9090:9090
```

AccÃ©dez Ã  Prometheus via `http://localhost:9090` (si vous utilisez port-forward).

### 5.3 Explorer Prometheus

**Note importante sur les mÃ©triques cAdvisor** :

Les mÃ©triques comme `container_cpu_usage_seconds_total` et `container_memory_usage_bytes` sont collectÃ©es par cAdvisor (Container Advisor) qui est intÃ©grÃ© dans kubelet.

Points importants :
- Ces mÃ©triques utilisent le label `container` (et non `pod`)
- Le container "POD" reprÃ©sente le conteneur infrastructure et doit Ãªtre filtrÃ©
- Les conteneurs vides doivent Ãªtre exclus avec `container!=""`
- Les requÃªtes doivent gÃ©nÃ©ralement agrÃ©ger par `pod` et `namespace`

#### 5.3.1 Guide complet : Utiliser `container_cpu_usage_seconds_total`

La mÃ©trique `container_cpu_usage_seconds_total` est **la mÃ©trique principale pour surveiller l'utilisation CPU** dans Kubernetes. C'est un **compteur cumulatif** (counter) qui reprÃ©sente le temps CPU total consommÃ© par un conteneur en secondes.

**ğŸ”‘ RÃ¨gles essentielles Ã  respecter** :

1. **Toujours utiliser `rate()`** : La mÃ©trique est cumulative, il faut calculer le taux de variation
   ```promql
   rate(container_cpu_usage_seconds_total[5m])
   ```

2. **Filtrer le conteneur infrastructure** : `container!="POD"`
   ```promql
   container_cpu_usage_seconds_total{container!="POD"}
   ```

3. **Exclure les conteneurs vides** : `container!=""`
   ```promql
   container_cpu_usage_seconds_total{container!="",container!="POD"}
   ```

4. **AgrÃ©ger par pod et namespace** : Utiliser `sum()` et `by()`
   ```promql
   sum(rate(container_cpu_usage_seconds_total{container!="",container!="POD"}[5m])) by (pod, namespace)
   ```

**ğŸ“Š Exemples pratiques** :

```promql
# 1. Utilisation CPU par pod (en cores) - REQUÃŠTE STANDARD
sum(rate(container_cpu_usage_seconds_total{container!="",container!="POD"}[5m])) by (pod, namespace)

# 2. Top 5 des pods les plus gourmands en CPU
topk(5, sum(rate(container_cpu_usage_seconds_total{container!="",container!="POD"}[5m])) by (pod))

# 3. CPU total utilisÃ© dans un namespace spÃ©cifique
sum(rate(container_cpu_usage_seconds_total{namespace="monitoring",container!="",container!="POD"}[5m]))

# 4. CPU total du cluster (tous les pods)
sum(rate(container_cpu_usage_seconds_total{container!="",container!="POD"}[5m]))

# 5. CPU par conteneur (dÃ©tail fin)
rate(container_cpu_usage_seconds_total{container!="",container!="POD"}[5m])

# 6. Moyenne CPU des replicas d'un dÃ©ploiement
avg(rate(container_cpu_usage_seconds_total{pod=~"nginx-.*",container!="",container!="POD"}[5m]))
```

**ğŸ’¡ Comprendre les rÃ©sultats** :

- RÃ©sultat = **0.5** â†’ Le pod utilise **0.5 core** (50% d'un CPU)
- RÃ©sultat = **1.2** â†’ Le pod utilise **1.2 cores** (plus d'un CPU complet)
- RÃ©sultat = **0.001** â†’ Le pod utilise **0.1%** d'un CPU (trÃ¨s peu)

**ğŸ¯ Cas d'usage courants** :

```promql
# Identifier les pods au-dessus d'un seuil (0.8 cores)
sum(rate(container_cpu_usage_seconds_total{container!="",container!="POD"}[5m])) by (pod, namespace) > 0.8

# Comparer avec les requests (nÃ©cessite kube-state-metrics)
sum(rate(container_cpu_usage_seconds_total{container!="",container!="POD"}[5m])) by (pod, namespace)
/
sum(kube_pod_container_resource_requests{resource="cpu"}) by (pod, namespace)

# CPU par node
sum(rate(container_cpu_usage_seconds_total{container!="",container!="POD"}[5m])) by (node)
```

**âš ï¸ Erreurs courantes Ã  Ã©viter** :

| âŒ Erreur | âœ… Correct |
|-----------|-----------|
| `container_cpu_usage_seconds_total` | `rate(container_cpu_usage_seconds_total[5m])` |
| `rate(...[10s])` (intervalle trop court) | `rate(...[5m])` (minimum 1-5 minutes) |
| Sans filtrer `container!="POD"` | `{container!="",container!="POD"}` |
| Pas d'agrÃ©gation (trop de sÃ©ries) | `sum(...) by (pod, namespace)` |

**Exercice 7 : RequÃªtes PromQL**

Dans l'interface web de Prometheus :

```promql
# Voir tous les pods
up

# Utilisation CPU des conteneurs (mÃ©trique brute)
container_cpu_usage_seconds_total

# â­ MEILLEURE PRATIQUE : CPU par pod (en cores)
sum(rate(container_cpu_usage_seconds_total{container!="",container!="POD"}[5m])) by (pod, namespace)

# Utilisation mÃ©moire
container_memory_usage_bytes

# MÃ©moire utilisÃ©e par pod
sum(container_memory_usage_bytes{container!="",container!="POD"}) by (pod, namespace)

# Nombre de conteneurs par pod et namespace
count(container_memory_usage_bytes{container!="",container!="POD"}) by (pod, namespace)

# Note: Pour des mÃ©triques plus complÃ¨tes sur l'Ã©tat du cluster (comme kube_pod_info),
# installez kube-state-metrics (voir section Outils complÃ©mentaires)

# Taux de requÃªtes HTTP (si mÃ©triques disponibles)
rate(http_requests_total[5m])

# Top 10 pods par CPU
topk(10, sum(rate(container_cpu_usage_seconds_total{container!="",container!="POD"}[5m])) by (pod, namespace))

# Pods utilisant plus de 50% d'un core
sum(rate(container_cpu_usage_seconds_total{container!="",container!="POD"}[5m])) by (pod, namespace) > 0.5
```

### 5.4 Installation de Grafana

CrÃ©er `05-grafana-deployment.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana
        image: grafana/grafana:10.0.0
        ports:
        - containerPort: 3000
          name: web
        env:
        - name: GF_SECURITY_ADMIN_USER
          value: "admin"
        - name: GF_SECURITY_ADMIN_PASSWORD
          value: "admin123"
        - name: GF_USERS_ALLOW_SIGN_UP
          value: "false"
        volumeMounts:
        - name: grafana-storage
          mountPath: /var/lib/grafana
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
      volumes:
      - name: grafana-storage
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: monitoring
spec:
  type: NodePort
  selector:
    app: grafana
  ports:
  - port: 3000
    targetPort: 3000
    nodePort: 30300
```

**Exercice 8 : DÃ©ployer Grafana**

```bash
# Appliquer la configuration
kubectl apply -f 05-grafana-deployment.yaml

# VÃ©rifier le dÃ©ploiement
kubectl get pods -n monitoring

# Attendre que Grafana soit prÃªt
kubectl wait --for=condition=ready pod -l app=grafana -n monitoring --timeout=120s

# Obtenir l'URL de Grafana
minikube service grafana -n monitoring --url

# Ou avec port-forward
kubectl port-forward -n monitoring svc/grafana 3000:3000
```

AccÃ©dez Ã  Grafana via `http://localhost:3000`
- **Username** : admin
- **Password** : admin123

### 5.5 Configurer Grafana avec Prometheus

**Exercice 9 : Ajouter Prometheus comme source de donnÃ©es**

1. Connectez-vous Ã  Grafana
2. Cliquez sur "Configuration" (âš™ï¸) â†’ "Data Sources"
3. Cliquez sur "Add data source"
4. SÃ©lectionnez "Prometheus"
5. Configurez :
   - **URL** : `http://prometheus.monitoring.svc.cluster.local:9090`
   - Cliquez sur "Save & Test"

### 5.6 CrÃ©er un Dashboard

**Exercice 10 : Dashboard personnalisÃ©**

1. Cliquez sur "+" â†’ "Dashboard" â†’ "Add new panel"
2. Dans "Query", sÃ©lectionnez "Prometheus"
3. Entrez une requÃªte PromQL :
   ```promql
   sum(rate(container_cpu_usage_seconds_total{container!="",container!="POD"}[5m])) by (pod, namespace)
   ```
4. Configurez la visualisation (Graph, Gauge, Table, etc.)
5. DÃ©finissez un titre : "CPU Usage per Pod (cores)"
6. Cliquez sur "Apply"
7. Ajoutez d'autres panels :
   - MÃ©moire : `sum(container_memory_usage_bytes{container!="",container!="POD"}) by (pod, namespace)`
   - RÃ©seau reÃ§u : `sum(rate(container_network_receive_bytes_total{container!="",container!="POD"}[5m])) by (pod, namespace)`
   - RÃ©seau transmis : `sum(rate(container_network_transmit_bytes_total{container!="",container!="POD"}[5m])) by (pod, namespace)`

8. Sauvegardez le dashboard : "Save dashboard" (ğŸ’¾)

### 5.7 Importer des dashboards prÃ©-configurÃ©s

**Exercice 11 : Importer un dashboard**

1. Dans Grafana, cliquez sur "+" â†’ "Import"
2. Entrez un ID de dashboard (ex: **315** pour Kubernetes cluster monitoring)
3. Cliquez sur "Load"
4. SÃ©lectionnez la source de donnÃ©es Prometheus
5. Cliquez sur "Import"

**Dashboards recommandÃ©s** :
- **315** : Kubernetes cluster monitoring (Prometheus)
- **747** : Kubernetes Deployment metrics
- **6417** : Kubernetes Cluster (Prometheus)
- **8588** : Kubernetes Deployment Statefulset Daemonset metrics

## Partie 6 : Application instrumentÃ©e

### 6.1 Application avec mÃ©triques Prometheus

CrÃ©er `06-instrumented-app.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-app
  namespace: monitoring
spec:
  replicas: 2
  selector:
    matchLabels:
      app: demo-app
  template:
    metadata:
      labels:
        app: demo-app
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: app
        image: fabxc/instrumented_app:latest
        ports:
        - containerPort: 8080
          name: metrics
---
apiVersion: v1
kind: Service
metadata:
  name: demo-app
  namespace: monitoring
spec:
  selector:
    app: demo-app
  ports:
  - port: 8080
    targetPort: 8080
```

**Exercice 12 : DÃ©ployer et monitorer l'application**

```bash
# DÃ©ployer l'application
kubectl apply -f 06-instrumented-app.yaml

# VÃ©rifier le dÃ©ploiement
kubectl get pods -n monitoring -l app=demo-app

# AccÃ©der aux mÃ©triques
kubectl port-forward -n monitoring svc/demo-app 8080:8080 &
curl http://localhost:8080/metrics
pkill -f "port-forward.*8080"

# Dans Prometheus (aprÃ¨s quelques minutes), vÃ©rifier que les mÃ©triques sont collectÃ©es
# RequÃªte : up{job="kubernetes-pods", app="demo-app"}
```

Dans Grafana, crÃ©ez un panel avec :
```promql
rate(http_requests_total{job="kubernetes-pods"}[5m])
```

## Partie 7 : Alerting

### 7.1 RÃ¨gles d'alerte Prometheus

CrÃ©er `07-prometheus-rules.yaml` :

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-rules
  namespace: monitoring
data:
  alert.rules: |
    groups:
    - name: example
      interval: 30s
      rules:
      # Alerte si un pod est down
      - alert: PodDown
        expr: up{job="kubernetes-pods"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Pod {{ $labels.pod }} is down"
          description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has been down for more than 1 minute."

      # Alerte sur CPU Ã©levÃ©
      - alert: HighCPUUsage
        expr: sum(rate(container_cpu_usage_seconds_total{container!="",container!="POD"}[5m])) by (pod, namespace) > 0.8
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on {{ $labels.namespace }}/{{ $labels.pod }}"
          description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has high CPU usage (> 0.8 cores)"

      # Alerte sur mÃ©moire Ã©levÃ©e
      - alert: HighMemoryUsage
        expr: container_memory_usage_bytes{pod!=""} > 500000000
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ $labels.pod }}"
          description: "Pod {{ $labels.pod }} is using more than 500MB of memory"

      # Alerte si trop de pods en erreur
      - alert: TooManyPodErrors
        expr: count(kube_pod_status_phase{phase="Failed"}) > 5
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Too many pods in failed state"
          description: "More than 5 pods are in failed state"
```

**Exercice 13 : Configurer les rÃ¨gles d'alerte**

```bash
# Appliquer les rÃ¨gles
kubectl apply -f 07-prometheus-rules.yaml

# VÃ©rifier que la ConfigMap est crÃ©Ã©e
kubectl get configmap prometheus-rules -n monitoring
```

Maintenant, il faut mettre Ã  jour le dÃ©ploiement Prometheus pour charger ces rÃ¨gles.

CrÃ©er `07-prometheus-with-rules.yaml` (mise Ã  jour du dÃ©ploiement) :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      serviceAccountName: prometheus
      containers:
      - name: prometheus
        image: prom/prometheus:v2.45.0
        args:
          - '--config.file=/etc/prometheus/prometheus.yml'
          - '--storage.tsdb.path=/prometheus'
          - '--web.console.libraries=/usr/share/prometheus/console_libraries'
          - '--web.console.templates=/usr/share/prometheus/consoles'
          - '--web.enable-lifecycle'
        ports:
        - containerPort: 9090
          name: web
        volumeMounts:
        - name: prometheus-config
          mountPath: /etc/prometheus
        - name: prometheus-rules
          mountPath: /etc/prometheus-rules
        - name: prometheus-storage
          mountPath: /prometheus
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
      volumes:
      - name: prometheus-config
        configMap:
          name: prometheus-config
      - name: prometheus-rules
        configMap:
          name: prometheus-rules
      - name: prometheus-storage
        emptyDir: {}
```

Mettre Ã  jour Ã©galement la ConfigMap Prometheus pour inclure les rÃ¨gles :

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s

    rule_files:
      - '/etc/prometheus-rules/alert.rules'

    scrape_configs:
      - job_name: 'kubernetes-nodes'
        kubernetes_sd_configs:
          - role: node
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)

      - job_name: 'kubernetes-cadvisor'
        kubernetes_sd_configs:
          - role: node
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __metrics_path__
            replacement: /metrics/cadvisor

      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__

      - job_name: 'kubernetes-service-endpoints'
        kubernetes_sd_configs:
          - role: endpoints
```

Appliquer les modifications :

```bash
# Mettre Ã  jour la ConfigMap Prometheus
kubectl apply -f 04-prometheus-deployment.yaml

# Mettre Ã  jour le dÃ©ploiement Prometheus
kubectl apply -f 07-prometheus-with-rules.yaml

# Attendre que le pod redÃ©marre
kubectl rollout status deployment/prometheus -n monitoring

# VÃ©rifier que Prometheus a bien chargÃ© les rÃ¨gles
kubectl logs -n monitoring -l app=prometheus | grep -i "Loading configuration file"
```

### 7.2 Visualiser les alertes

Dans l'interface Prometheus :
1. Allez dans "Alerts"
2. Vous verrez les rÃ¨gles d'alerte configurÃ©es
3. Les alertes actives apparaÃ®tront en rouge

## Partie 8 : Logging avancÃ© (Introduction)

### 8.1 Stack EFK/ELK

Pour une gestion avancÃ©e des logs, on utilise gÃ©nÃ©ralement :

**Stack EFK** :
- **E**lasticsearch : Stockage et indexation des logs
- **F**luentd/Fluent Bit : Collecte et agrÃ©gation des logs
- **K**ibana : Visualisation

**Stack ELK** :
- **E**lasticsearch
- **L**ogstash : Collecte et transformation
- **K**ibana

### 8.2 Introduction Ã  Fluentd

**Note importante** : Cette section prÃ©sente une configuration simplifiÃ©e de Fluentd pour la dÃ©monstration. Pour une installation complÃ¨te d'EFK avec Elasticsearch et Kibana, consultez les ressources complÃ©mentaires en fin de TP.

Dans cet exercice, nous dÃ©ployons Fluentd avec une sortie vers stdout pour observer la collecte des logs.

CrÃ©er `08-fluentd-daemonset.yaml` :

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: kube-system
data:
  fluent.conf: |
    <source>
      @type tail
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      tag kubernetes.*
      read_from_head true
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>

    <filter kubernetes.**>
      @type kubernetes_metadata
      @id filter_kube_metadata
    </filter>

    <match kubernetes.**>
      @type stdout
    </match>
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluentd
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - namespaces
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluentd
roleRef:
  kind: ClusterRole
  name: fluentd
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: fluentd
  namespace: kube-system
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: kube-system
  labels:
    app: fluentd
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      serviceAccountName: fluentd
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1.16-debian-1
        env:
        - name: FLUENTD_SYSTEMD_CONF
          value: "disable"
        volumeMounts:
        - name: fluentd-config
          mountPath: /fluentd/etc/fluent.conf
          subPath: fluent.conf
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
      volumes:
      - name: fluentd-config
        configMap:
          name: fluentd-config
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
```

**Exercice 14 : DÃ©ployer Fluentd**

```bash
# DÃ©ployer Fluentd
kubectl apply -f 08-fluentd-daemonset.yaml

# VÃ©rifier le DaemonSet
kubectl get daemonset fluentd -n kube-system

# VÃ©rifier les pods Fluentd (un par nÅ“ud)
kubectl get pods -n kube-system -l app=fluentd

# Voir les logs collectÃ©s par Fluentd
FLUENTD_POD=$(kubectl get pods -n kube-system -l app=fluentd -o jsonpath='{.items[0].metadata.name}')
kubectl logs -n kube-system $FLUENTD_POD --tail=50

# Observer les logs en temps rÃ©el
kubectl logs -n kube-system $FLUENTD_POD -f
```

**Note** : Cette configuration affiche simplement les logs collectÃ©s vers stdout. Pour une installation complÃ¨te avec Elasticsearch et Kibana, consultez les ressources complÃ©mentaires ci-dessous.

**Pour aller plus loin avec EFK** :
- DÃ©ployer Elasticsearch avec l'Elastic Cloud on Kubernetes (ECK) operator
- Configurer Kibana pour la visualisation
- Modifier la configuration Fluentd pour envoyer vers Elasticsearch
- Voir : [Elastic Cloud on Kubernetes](https://www.elastic.co/guide/en/cloud-on-k8s/current/index.html)

## Partie 9 : Bonnes pratiques

### 9.1 Monitoring

1. **DÃ©finir des SLIs/SLOs**
   - Service Level Indicators : MÃ©triques clÃ©s (latence, disponibilitÃ©)
   - Service Level Objectives : Objectifs quantifiÃ©s (99.9% uptime)

2. **MÃ©triques Ã  surveiller**
   - Utilisation CPU et mÃ©moire
   - Latence des requÃªtes
   - Taux d'erreur
   - Saturation rÃ©seau/disque

3. **Alertes pertinentes**
   - Ã‰viter les fausses alertes
   - Alerter sur les symptÃ´mes, pas sur les causes
   - DÃ©finir des seuils rÃ©alistes
   - Inclure des runbooks dans les alertes

4. **RÃ©tention des donnÃ©es**
   - DÃ©finir une politique de rÃ©tention
   - Archiver les mÃ©triques anciennes
   - Utiliser l'agrÃ©gation pour les donnÃ©es historiques

### 9.2 Logging

1. **Niveaux de logs structurÃ©s**
   ```
   DEBUG : Informations de dÃ©bogage dÃ©taillÃ©es
   INFO  : Ã‰vÃ©nements gÃ©nÃ©raux
   WARN  : Situations potentiellement problÃ©matiques
   ERROR : Erreurs nÃ©cessitant une attention
   FATAL : Erreurs critiques causant l'arrÃªt
   ```

2. **Format de logs structurÃ©**
   ```json
   {
     "timestamp": "2025-10-29T10:30:00Z",
     "level": "INFO",
     "service": "api-backend",
     "message": "Request processed",
     "request_id": "abc-123",
     "duration_ms": 45
   }
   ```

3. **Logs Ã  collecter**
   - Logs applicatifs
   - Logs systÃ¨me
   - Logs d'audit
   - Logs de sÃ©curitÃ©

4. **Rotation des logs**
   - Limiter la taille des logs
   - Archiver les logs anciens
   - Utiliser logrotate

### 9.3 Ressources

```yaml
# Toujours dÃ©finir des resource requests et limits
resources:
  requests:
    memory: "256Mi"
    cpu: "250m"
  limits:
    memory: "512Mi"
    cpu: "500m"
```

## Partie 10 : Exercices pratiques complets

### Exercice Final 1 : Monitoring d'une application web

**Mission** : DÃ©ployer une application web complÃ¨te avec monitoring

1. DÃ©ployez une application (nginx, redis, postgresql)
2. Configurez Prometheus pour collecter les mÃ©triques
3. CrÃ©ez un dashboard Grafana avec :
   - CPU et mÃ©moire par service
   - Nombre de pods par service
   - Latence des requÃªtes (si disponible)
4. Configurez des alertes pour :
   - Pod down
   - CPU > 80%
   - MÃ©moire > 90%

### Exercice Final 2 : Debugging avec les logs

**ScÃ©nario** : Une application crash rÃ©guliÃ¨rement

1. DÃ©ployez cette application buggy :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: buggy-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: buggy-app
  template:
    metadata:
      labels:
        app: buggy-app
    spec:
      containers:
      - name: app
        image: busybox
        command: ["/bin/sh"]
        args:
          - -c
          - >
            echo "Starting application...";
            sleep 10;
            echo "Application running";
            sleep 20;
            echo "ERROR: Out of memory!";
            exit 1
```

2. Analysez les logs pour identifier le problÃ¨me
3. Utilisez `kubectl describe` pour voir les Ã©vÃ©nements
4. Corrigez l'application (ajoutez un restart loop)
5. VÃ©rifiez que l'application redÃ©marre automatiquement

### Exercice Final 3 : Dashboard personnalisÃ©

CrÃ©ez un dashboard Grafana complet pour votre cluster :

1. **Panel 1** : Cluster overview
   - Nombre de nÅ“uds
   - Nombre total de pods
   - Pods par namespace

2. **Panel 2** : Ressources
   - CPU total utilisÃ© vs disponible
   - MÃ©moire totale utilisÃ©e vs disponible

3. **Panel 3** : Top pods
   - Top 10 pods par CPU
   - Top 10 pods par mÃ©moire

4. **Panel 4** : Network
   - Bande passante entrante
   - Bande passante sortante

5. **Panel 5** : Alerts
   - Nombre d'alertes actives
   - Liste des alertes

## Partie 11 : Nettoyage

```bash
# Supprimer les dÃ©ploiements de test
kubectl delete deployment nginx-test
kubectl delete deployment php-apache
kubectl delete hpa php-apache-hpa
kubectl delete service php-apache
kubectl delete deployment log-generator
kubectl delete pod multi-log-pod

# Supprimer le namespace monitoring (supprime tout dedans)
kubectl delete namespace monitoring

# DÃ©sactiver les addons (optionnel)
minikube addons disable metrics-server
minikube addons disable dashboard

# VÃ©rifier
kubectl get all -n monitoring
```

## RÃ©sumÃ©

Dans ce TP, vous avez appris Ã  :

- Installer et utiliser Metrics Server
- Configurer l'autoscaling horizontal (HPA)
- Utiliser le Dashboard Kubernetes
- Collecter et analyser les logs avec kubectl
- DÃ©ployer Prometheus pour la collecte de mÃ©triques
- CrÃ©er des dashboards avec Grafana
- Configurer des alertes
- Appliquer les bonnes pratiques de monitoring

### Concepts clÃ©s

- **MÃ©triques** : DonnÃ©es numÃ©riques sur l'Ã©tat du systÃ¨me
- **Logs** : Enregistrements d'Ã©vÃ©nements
- **Prometheus** : SystÃ¨me de monitoring time-series
- **Grafana** : Plateforme de visualisation
- **HPA** : Autoscaling basÃ© sur les mÃ©triques
- **PromQL** : Langage de requÃªte Prometheus
- **Alerting** : Notification sur conditions dÃ©finies

## Ressources complÃ©mentaires

### Documentation officielle

- [Kubernetes Monitoring Architecture](https://kubernetes.io/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/)
- [Prometheus Documentation](https://prometheus.io/docs/)
- [Grafana Documentation](https://grafana.com/docs/)
- [Kubernetes Logging](https://kubernetes.io/docs/concepts/cluster-administration/logging/)

### Guides avancÃ©s

- [Prometheus Operator](https://github.com/prometheus-operator/prometheus-operator)
- [EFK Stack Installation](https://www.elastic.co/guide/en/cloud-on-k8s/current/index.html)
- [Loki (alternative Ã  EFK)](https://grafana.com/docs/loki/latest/)
- [Jaeger (Distributed Tracing)](https://www.jaegertracing.io/docs/)

### Dashboards Grafana

- [Grafana Dashboard Repository](https://grafana.com/grafana/dashboards/)
- Dashboard ID 315 : Kubernetes cluster monitoring
- Dashboard ID 747 : Kubernetes Deployment metrics
- Dashboard ID 6417 : Kubernetes Cluster

### Outils complÃ©mentaires

- **kube-state-metrics** : MÃ©triques sur l'Ã©tat des ressources K8s
  - Fournit des mÃ©triques comme `kube_pod_info`, `kube_deployment_status_replicas`, etc.
  - Installation : `kubectl apply -f https://github.com/kubernetes/kube-state-metrics/releases/download/v2.10.0/standard.yaml`
  - NÃ©cessite d'ajouter un scrape config dans Prometheus pour le job `kube-state-metrics`

- **node-exporter** : MÃ©triques hardware et OS
  - Fournit des mÃ©triques dÃ©taillÃ©es sur les nÅ“uds (CPU, disque, rÃ©seau, etc.)
  - DÃ©ployÃ© gÃ©nÃ©ralement via un DaemonSet

- **Alertmanager** : Gestion avancÃ©e des alertes
  - Permet de router, grouper et gÃ©rer les notifications d'alertes
  - IntÃ©gration avec Slack, Email, PagerDuty, etc.

- **Thanos** : Stockage long terme pour Prometheus
  - Permet de conserver les mÃ©triques sur de longues pÃ©riodes
  - Offre une vue unifiÃ©e de plusieurs instances Prometheus

## Prochaines Ã©tapes

FÃ©licitations ! Vous maÃ®trisez maintenant le monitoring et la gestion des logs dans Kubernetes.

Continuez votre apprentissage avec :
- **TP5** (Ã  venir) : Mise en production et bonnes pratiques
- **Helm** : Gestionnaire de packages Kubernetes
- **GitOps** : DÃ©ploiement continu avec ArgoCD/Flux
- **Service Mesh** : Istio, Linkerd

## Questions de rÃ©vision

1. Quelle est la diffÃ©rence entre Metrics Server et Prometheus ?
2. Comment fonctionne l'Horizontal Pod Autoscaler ?
3. Quels sont les trois piliers de l'observabilitÃ© ?
4. Comment voir les logs d'un conteneur qui a crashÃ© ?
5. Qu'est-ce que PromQL ?
6. Pourquoi utiliser Grafana en plus de Prometheus ?
7. Quelle est la diffÃ©rence entre logs et mÃ©triques ?
8. Comment configurer une alerte dans Prometheus ?

## Solutions des questions

<details>
<summary>Cliquez pour voir les rÃ©ponses</summary>

1. Metrics Server collecte uniquement CPU/mÃ©moire pour kubectl top et HPA. Prometheus collecte des mÃ©triques plus dÃ©taillÃ©es avec rÃ©tention et requÃªtes avancÃ©es.
2. HPA surveille les mÃ©triques (CPU, mÃ©moire, custom) et ajuste automatiquement le nombre de replicas entre min et max dÃ©finis.
3. MÃ©triques (donnÃ©es numÃ©riques agrÃ©gÃ©es), Logs (Ã©vÃ©nements), Traces (suivi de requÃªtes distribuÃ©es).
4. `kubectl logs <pod-name> --previous` pour voir les logs du conteneur prÃ©cÃ©dent avant le crash.
5. Prometheus Query Language : langage de requÃªte pour interroger les mÃ©triques time-series.
6. Grafana offre une meilleure visualisation, des dashboards personnalisables, et peut agrÃ©ger plusieurs sources de donnÃ©es.
7. Logs = Ã©vÃ©nements discrets avec contexte. MÃ©triques = valeurs numÃ©riques agrÃ©gÃ©es dans le temps.
8. CrÃ©er une ConfigMap avec des rÃ¨gles d'alerte en YAML, la monter dans Prometheus, et dÃ©finir expr, for, labels, annotations.

</details>

---

**DurÃ©e estimÃ©e du TP :** 5-6 heures
**Niveau :** IntermÃ©diaire/AvancÃ©

**Excellent travail ! Vous Ãªtes maintenant prÃªt Ã  monitorer efficacement vos clusters Kubernetes !**
