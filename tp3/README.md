# TP3 - Persistance des donnÃ©es dans Kubernetes

## Objectifs du TP

Ã€ la fin de ce TP, vous serez capable de :
- Comprendre les diffÃ©rents types de volumes Kubernetes
- CrÃ©er et gÃ©rer des PersistentVolumes (PV) et PersistentVolumeClaims (PVC)
- Utiliser les StorageClasses pour le provisionnement dynamique
- DÃ©ployer une base de donnÃ©es avec persistance des donnÃ©es
- GÃ©rer le cycle de vie des volumes
- Appliquer les bonnes pratiques de gestion du stockage

## PrÃ©requis

- Avoir complÃ©tÃ© le TP1 et TP2
- Un cluster Kubernetes fonctionnel (**minikube** ou **kubeadm**)
- Connaissance de base des manifests YAML

**Note :** Les concepts de persistance (PV, PVC, StorageClass) sont identiques pour minikube et kubeadm. Les diffÃ©rences se situent principalement au niveau des provisioners de stockage disponibles.

## Partie 1 : Introduction aux volumes Kubernetes

### 1.1 Pourquoi les volumes ?

Par dÃ©faut, les donnÃ©es dans un conteneur sont Ã©phÃ©mÃ¨res : elles disparaissent quand le conteneur s'arrÃªte. Les volumes Kubernetes permettent de :

- **Persister les donnÃ©es** au-delÃ  du cycle de vie d'un pod
- **Partager des donnÃ©es** entre conteneurs d'un mÃªme pod
- **Stocker des configurations** et des secrets
- **Monter des systÃ¨mes de fichiers externes**

### 1.2 Types de volumes

Kubernetes supporte plusieurs types de volumes :

- **emptyDir** : Volume temporaire, vie du pod
- **hostPath** : Monte un rÃ©pertoire du nÅ“ud (dÃ©veloppement uniquement)
- **persistentVolumeClaim** : Demande de stockage persistant
- **configMap/secret** : Pour les configurations
- **nfs, iscsi, cephfs** : Stockage rÃ©seau
- **cloud providers** : awsElasticBlockStore, gcePersistentDisk, azureDisk

### 1.3 Volume emptyDir

Le volume le plus simple, crÃ©Ã© quand un pod est assignÃ© Ã  un nÅ“ud.

CrÃ©er le fichier `01-emptydir-pod.yaml` :

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: emptydir-demo
  labels:
    app: demo
spec:
  containers:
  - name: writer
    image: busybox
    command: ["/bin/sh"]
    args:
      - -c
      - >
        while true; do
          echo "$(date): Writing data" >> /data/log.txt;
          sleep 5;
        done
    volumeMounts:
    - name: shared-storage
      mountPath: /data

  - name: reader
    image: busybox
    command: ["/bin/sh"]
    args:
      - -c
      - >
        while true; do
          echo "=== Latest logs ===";
          tail -n 5 /data/log.txt;
          sleep 10;
        done
    volumeMounts:
    - name: shared-storage
      mountPath: /data

  volumes:
  - name: shared-storage
    emptyDir: {}
```

**Exercice 1 : Tester emptyDir**

```bash
# Appliquer le manifest
kubectl apply -f 01-emptydir-pod.yaml

# VÃ©rifier que le pod tourne
kubectl get pods

# Observer les logs du writer
kubectl logs emptydir-demo -c writer

# Observer les logs du reader
kubectl logs emptydir-demo -c reader -f

# Supprimer le pod
kubectl delete -f 01-emptydir-pod.yaml
```

**Question** : Que se passe-t-il si vous recrÃ©ez le pod ? Les donnÃ©es sont-elles toujours lÃ  ?

## Partie 2 : Comprendre l'infrastructure de stockage Kubernetes

### 2.1 Architecture du stockage dans Kubernetes

Avant de plonger dans les PersistentVolumes, il est crucial de comprendre l'architecture globale du stockage dans Kubernetes et les diffÃ©rentes options disponibles en production.

#### 2.1.1 Les couches de l'architecture de stockage

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Application (Pod)                       â”‚
â”‚  Utilise le volume via un point de montage          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       PersistentVolumeClaim (PVC)                   â”‚
â”‚  Demande de stockage avec spÃ©cifications            â”‚
â”‚  - Taille: 10Gi                                      â”‚
â”‚  - Mode: ReadWriteOnce                               â”‚
â”‚  - StorageClass: fast-ssd                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          StorageClass                                â”‚
â”‚  DÃ©finit le type et les paramÃ¨tres du stockage      â”‚
â”‚  - Provisioner: csi-driver                          â”‚
â”‚  - Parameters: type=ssd, iops=3000                   â”‚
â”‚  - ReclaimPolicy: Delete                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       PersistentVolume (PV)                         â”‚
â”‚  Ressource de stockage rÃ©elle dans le cluster       â”‚
â”‚  - CrÃ©Ã© dynamiquement ou manuellement                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Backend de Stockage Physique                  â”‚
â”‚  - Disque local (hostPath)                          â”‚
â”‚  - NFS / iSCSI / Ceph                               â”‚
â”‚  - Cloud (EBS, Azure Disk, GCE PD)                  â”‚
â”‚  - Distributed (Longhorn, Rook/Ceph)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2.1.2 Types de backends de stockage en production

##### A. Stockage Local (hostPath, local)

**Cas d'usage :** DÃ©veloppement, tests, donnÃ©es temporaires haute performance

**Avantages :**
- Performance maximale (pas de latence rÃ©seau)
- SimplicitÃ© de configuration
- CoÃ»t nul

**InconvÃ©nients :**
- Pas de haute disponibilitÃ©
- DonnÃ©es liÃ©es Ã  un nÅ“ud spÃ©cifique
- Perte de donnÃ©es si le nÅ“ud tombe

**Exemple de scÃ©nario :**
```yaml
# Base de donnÃ©es de cache temporaire sur un nÅ“ud spÃ©cifique
apiVersion: v1
kind: PersistentVolume
metadata:
  name: cache-local-pv
spec:
  capacity:
    storage: 50Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  local:
    path: /mnt/fast-ssd/cache
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - worker-node-1
```

**âš ï¸ SÃ©curitÃ© :** Ne jamais utiliser hostPath en production sauf cas trÃ¨s spÃ©cifiques. C'est une faille de sÃ©curitÃ© majeure car cela donne accÃ¨s au systÃ¨me de fichiers du nÅ“ud.

##### B. Stockage RÃ©seau (NFS)

**Cas d'usage :** Partage de fichiers entre plusieurs pods, fichiers de configuration, assets statiques

**Avantages :**
- Support ReadWriteMany (plusieurs pods simultanÃ©s)
- SimplicitÃ© de mise en Å“uvre
- CoÃ»t modÃ©rÃ©

**InconvÃ©nients :**
- Performances limitÃ©es pour I/O intensif
- Point de dÃ©faillance unique (le serveur NFS)
- Latence rÃ©seau

**Architecture typique :**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Pod 1      â”‚     â”‚   Pod 2      â”‚     â”‚   Pod 3      â”‚
â”‚ (Node A)     â”‚     â”‚ (Node B)     â”‚     â”‚ (Node C)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚                    â”‚
       â”‚        Network (TCP/IP - NFS)           â”‚
       â”‚                    â”‚                    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Serveur NFS     â”‚
                   â”‚  /exports/data   â”‚
                   â”‚  100Gi SSD       â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Exemple concret :**
```yaml
# Serveur NFS: 192.168.1.100
# Export: /exports/shared-data
# Permissions: rw,sync,no_subtree_check,no_root_squash

apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-shared-storage
spec:
  capacity:
    storage: 100Gi
  accessModes:
    - ReadWriteMany  # Plusieurs pods peuvent lire/Ã©crire
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs
  mountOptions:
    - hard
    - nfsvers=4.1
    - rsize=1048576
    - wsize=1048576
  nfs:
    server: 192.168.1.100
    path: "/exports/shared-data"
```

**ğŸ”’ SÃ©curitÃ© NFS :**
- Utiliser NFSv4.1 minimum avec Kerberos
- Configurer des exports restrictifs (pas de no_root_squash sauf nÃ©cessitÃ©)
- Isoler le rÃ©seau NFS (VLAN dÃ©diÃ©)
- Chiffrer le trafic avec stunnel ou VPN

##### C. Stockage Block (iSCSI, Fibre Channel)

**Cas d'usage :** Bases de donnÃ©es, applications nÃ©cessitant des performances Ã©levÃ©es

**Avantages :**
- Haute performance
- Support des snapshots et clones
- FonctionnalitÃ©s entreprise (rÃ©plication, dÃ©duplication)

**InconvÃ©nients :**
- CoÃ»t Ã©levÃ© (SAN)
- ComplexitÃ© de configuration
- GÃ©nÃ©ralement ReadWriteOnce uniquement

**Architecture iSCSI :**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cluster Kubernetes                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚Worker 1â”‚  â”‚Worker 2â”‚  â”‚Worker 3â”‚     â”‚
â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â”‚
â”‚      â”‚ iSCSI    â”‚ iSCSI    â”‚ iSCSI      â”‚
â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚          â”‚          â”‚
    â”Œâ”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”
    â”‚   RÃ©seau iSCSI (VLAN)       â”‚
    â”‚   10Gb/s Ethernet           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   SAN Storage       â”‚
         â”‚   - LUN 1: 500Gi   â”‚
         â”‚   - LUN 2: 1Ti     â”‚
         â”‚   - RAID 10        â”‚
         â”‚   - SSD Tier       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Exemple iSCSI avec authentification CHAP :**
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: iscsi-pv-database
spec:
  capacity:
    storage: 500Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: iscsi-fast
  iscsi:
    targetPortal: 192.168.1.200:3260
    iqn: iqn.2024-01.com.enterprise:storage.lun1
    lun: 1
    fsType: ext4
    readOnly: false
    chapAuthDiscovery: true
    chapAuthSession: true
    secretRef:
      name: iscsi-chap-secret
---
apiVersion: v1
kind: Secret
metadata:
  name: iscsi-chap-secret
type: kubernetes.io/iscsi-chap
data:
  node.session.auth.username: <base64-encoded-username>
  node.session.auth.password: <base64-encoded-password>
```

##### D. Stockage Cloud (AWS EBS, Azure Disk, GCP PD)

**Cas d'usage :** Clusters sur cloud providers, applications cloud-native

**Avantages :**
- Haute disponibilitÃ© gÃ©rÃ©e par le cloud
- Snapshots automatiques
- Scaling facile
- IntÃ©gration native Kubernetes

**InconvÃ©nients :**
- CoÃ»t par GB/mois
- Performances variables selon le type
- Lock-in du cloud provider

**Comparaison des options cloud :**

| Provider | Type | Performance | Use Case |
|----------|------|-------------|----------|
| AWS | gp3 (SSD) | 3000 IOPS baseline | Usage gÃ©nÃ©ral |
| AWS | io2 (SSD) | Jusqu'Ã  64000 IOPS | Bases de donnÃ©es |
| AWS | st1 (HDD) | Throughput optimized | Big data, logs |
| Azure | Premium SSD | 7500+ IOPS | Production DB |
| Azure | Standard SSD | 500 IOPS | Dev/Test |
| GCP | pd-balanced | 6000 IOPS | Ã‰quilibrÃ© |
| GCP | pd-ssd | 30000 IOPS | Haute performance |

**Exemple AWS EBS avec chiffrement :**
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: encrypted-gp3
  annotations:
    storageclass.kubernetes.io/is-default-class: "false"
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
  iops: "3000"
  throughput: "125"
  encrypted: "true"  # Chiffrement EBS obligatoire
  kmsKeyId: "arn:aws:kms:us-east-1:123456789:key/abcd-1234"
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-data
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: encrypted-gp3
  resources:
    requests:
      storage: 100Gi
```

##### E. Stockage DistribuÃ© (Ceph, Longhorn, GlusterFS)

**Cas d'usage :** Clusters on-premise nÃ©cessitant HA, multi-cloud, bare metal

**Avantages :**
- Haute disponibilitÃ© native
- RÃ©plication automatique
- Pas de vendor lock-in
- Support RWX

**InconvÃ©nients :**
- ComplexitÃ© opÃ©rationnelle Ã©levÃ©e
- NÃ©cessite plusieurs nÅ“uds (min 3)
- Overhead rÃ©seau et CPU

**Architecture Longhorn (exemple) :**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Cluster Kubernetes (3+ nodes)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Node 1  â”‚      â”‚ Node 2  â”‚      â”‚ Node 3  â”‚     â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚      â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚      â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚     â”‚
â”‚  â”‚ â”‚Repliâ”‚ â”‚â—„â”€â”€â”€â”€â–ºâ”‚ â”‚Repliâ”‚ â”‚â—„â”€â”€â”€â”€â–ºâ”‚ â”‚Repliâ”‚ â”‚     â”‚
â”‚  â”‚ â”‚ca 1 â”‚ â”‚      â”‚ â”‚ca 2 â”‚ â”‚      â”‚ â”‚ca 3 â”‚ â”‚     â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚      â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚      â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚       â”‚                 â”‚                 â”‚         â”‚
â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”     â”‚
â”‚  â”‚Disk 100Gâ”‚      â”‚Disk 100Gâ”‚      â”‚Disk 100Gâ”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Volume logique: 100Gi avec 3 rÃ©plicas
Si un nÅ“ud tombe, les 2 autres rÃ©plicas assurent la continuitÃ©
```

**Installation Longhorn avec sÃ©curitÃ© renforcÃ©e :**
```bash
# Installer Longhorn avec Helm
helm repo add longhorn https://charts.longhorn.io
helm repo update

# Configuration sÃ©curisÃ©e
cat > longhorn-values.yaml <<EOF
defaultSettings:
  backupTarget: s3://backups@us-east-1/longhorn  # Sauvegardes S3
  defaultReplicaCount: 3  # 3 rÃ©plicas pour HA
  guaranteedInstanceManagerCPU: 12
  storageMinimalAvailablePercentage: 15
  upgradeChecker: false  # DÃ©sactiver les appels externes

ingress:
  enabled: true
  host: longhorn.internal.company.com
  tls: true
  tlsSecret: longhorn-tls
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: longhorn-basic-auth

persistence:
  defaultClass: true
  defaultClassReplicaCount: 3
  reclaimPolicy: Retain
EOF

kubectl create namespace longhorn-system
helm install longhorn longhorn/longhorn --namespace longhorn-system -f longhorn-values.yaml

# CrÃ©er l'authentification basique pour l'UI
htpasswd -c auth admin
kubectl -n longhorn-system create secret generic longhorn-basic-auth --from-file=auth
```

**Exemple de StorageClass Longhorn :**
```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: longhorn-crypto-global
provisioner: driver.longhorn.io
allowVolumeExpansion: true
reclaimPolicy: Retain
volumeBindingMode: Immediate
parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "2880"  # 48 heures
  fromBackup: ""
  fsType: "ext4"
  dataLocality: "best-effort"  # PrÃ©fÃ©rer le nÅ“ud local si possible
  # Chiffrement des volumes
  encrypted: "true"
  # paramÃ¨tres de performance
  diskSelector: "ssd,fast"
  nodeSelector: "storage,production"
```

#### 2.1.3 Matrice de comparaison des solutions de stockage

| Solution | HA | Performance | CoÃ»t | ComplexitÃ© | ReadWriteMany | Use Case Principal |
|----------|----|----|------|------------|---------------|-------------------|
| hostPath | âŒ | â­â­â­â­â­ | Gratuit | Faible | âŒ | Dev/Test uniquement |
| local | âŒ | â­â­â­â­â­ | Gratuit | Faible | âŒ | Cache, donnÃ©es temporaires |
| NFS | âš ï¸ | â­â­ | Faible | Moyenne | âœ… | Fichiers partagÃ©s |
| iSCSI | âš ï¸ | â­â­â­â­ | Ã‰levÃ© | Ã‰levÃ©e | âŒ | Bases de donnÃ©es |
| AWS EBS | âœ… | â­â­â­â­ | Moyen | Faible | âŒ | Cloud, production |
| Longhorn | âœ… | â­â­â­ | Moyen | Ã‰levÃ©e | âœ… | On-premise, HA |
| Ceph/Rook | âœ… | â­â­â­â­ | Moyen | TrÃ¨s Ã©levÃ©e | âœ… | Enterprise, scale |

#### 2.1.4 Choisir la bonne solution de stockage

**Pour le dÃ©veloppement local :**
```yaml
hostPath ou emptyDir
â†’ Rapide, simple, pas de configuration
â†’ âš ï¸ JAMAIS en production
```

**Pour une petite application web (stateless avec assets) :**
```yaml
NFS
â†’ Partage facile des assets entre pods
â†’ Support ReadWriteMany
â†’ Exemple: Images uploadÃ©es, fichiers CSS/JS compilÃ©s
```

**Pour une base de donnÃ©es en production on-premise :**
```yaml
iSCSI (si SAN disponible) OU Longhorn/Ceph
â†’ Performance + HA
â†’ Snapshots pour backups
â†’ Exemple: PostgreSQL, MySQL, MongoDB
```

**Pour une application cloud-native :**
```yaml
StorageClass du cloud provider (EBS, Azure Disk, GCP PD)
â†’ IntÃ©gration native
â†’ Snapshots automatiques
â†’ Scaling facile
â†’ Exemple: Applications SaaS, microservices
```

**Pour un data lake / analytics :**
```yaml
S3 / Object Storage via CSI
â†’ CapacitÃ© illimitÃ©e
â†’ CoÃ»t optimisÃ©
â†’ AccÃ¨s concurrent
â†’ Exemple: Spark, Presto, donnÃ©es brutes
```

### 2.2 Concepts fondamentaux des PersistentVolumes

**PersistentVolume (PV)** :
- Ressource de stockage dans le cluster
- IndÃ©pendant du cycle de vie des pods
- ProvisionnÃ© par l'administrateur ou dynamiquement

**PersistentVolumeClaim (PVC)** :
- Demande de stockage par un utilisateur
- SpÃ©cifie la taille et le mode d'accÃ¨s
- Se lie automatiquement Ã  un PV compatible

**Cycle de vie** :
1. **Provisioning** : CrÃ©ation du PV (statique ou dynamique)
2. **Binding** : Liaison PVC â†’ PV
3. **Using** : Utilisation dans un pod
4. **Reclaiming** : LibÃ©ration et recyclage

### 2.2 Modes d'accÃ¨s

- **ReadWriteOnce (RWO)** : Lecture-Ã©criture par un seul nÅ“ud
- **ReadOnlyMany (ROX)** : Lecture seule par plusieurs nÅ“uds
- **ReadWriteMany (RWX)** : Lecture-Ã©criture par plusieurs nÅ“uds

### 2.3 CrÃ©er la StorageClass pour le provisionnement manuel

Avant de crÃ©er des PersistentVolumes avec un provisionnement manuel, nous devons crÃ©er une StorageClass appropriÃ©e. Sans cette StorageClass, le binding entre le PV et le PVC Ã©chouera.

CrÃ©er `02-storage-class-manual.yaml` :

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: manual
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
```

**Explications importantes** :

- **provisioner: kubernetes.io/no-provisioner** : Indique qu'il n'y a pas de provisionnement automatique. Les PV doivent Ãªtre crÃ©Ã©s manuellement par un administrateur.
- **volumeBindingMode: WaitForFirstConsumer** : Le binding du PVC au PV ne se fera que lorsqu'un pod utilisera le PVC. Cela Ã©vite de lier un volume Ã  un nÅ“ud avant de savoir oÃ¹ le pod sera schedulÃ©.

```bash
# CrÃ©er la StorageClass
kubectl apply -f 02-storage-class-manual.yaml

# VÃ©rifier la crÃ©ation
kubectl get storageclass manual
kubectl describe storageclass manual
```

**Note** : Cette Ã©tape est cruciale. Sans cette StorageClass, vous obtiendrez une erreur lors du binding du PVC car Kubernetes ne trouvera pas la StorageClass "manual" rÃ©fÃ©rencÃ©e dans les manifests.

### 2.4 CrÃ©er un PersistentVolume

CrÃ©er `03-persistent-volume.yaml` :

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-demo
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
```

**Exercice 2 : CrÃ©er un PV**

**Avec minikube :**
```bash
# CrÃ©er le rÃ©pertoire sur le nÅ“ud minikube
minikube ssh "sudo mkdir -p /mnt/data"
```

**Avec kubeadm :**
```bash
# CrÃ©er le rÃ©pertoire sur chaque worker (adapter le nom d'utilisateur et l'IP)
ssh user@worker-node-1 "sudo mkdir -p /mnt/data"
ssh user@worker-node-2 "sudo mkdir -p /mnt/data"

# Ou sur tous les nÅ“uds si vous autorisez le scheduling sur le master
for node in master-node worker-node-1 worker-node-2; do
  ssh user@$node "sudo mkdir -p /mnt/data"
done
```

**CrÃ©ation du PV (identique pour minikube et kubeadm) :**
```bash
# CrÃ©er le PV
kubectl apply -f 03-persistent-volume.yaml

# VÃ©rifier le PV
kubectl get pv
kubectl describe pv pv-demo
```

Vous devriez voir le statut **Available**.

### 2.5 CrÃ©er un PersistentVolumeClaim

CrÃ©er `04-persistent-volume-claim.yaml` :

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-demo
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
```

**Note importante sur le binding** :

Le PVC va chercher un PV compatible avec les critÃ¨res suivants :
- MÃªme `storageClassName` (ici: "manual")
- Mode d'accÃ¨s compatible (ici: ReadWriteOnce)
- CapacitÃ© suffisante (ici: 500Mi, le PV a 1Gi donc c'est OK)

âš ï¸ **ProblÃ¨me courant** : Si vous voyez le PVC rester en Ã©tat "Pending" indÃ©finiment, vÃ©rifiez que :
1. La StorageClass "manual" a bien Ã©tÃ© crÃ©Ã©e (section 2.3)
2. Un PV avec `storageClassName: manual` existe et est en Ã©tat "Available"
3. Les modes d'accÃ¨s et la capacitÃ© correspondent

Sans la StorageClass "manual", le binding Ã©chouera et vous verrez une erreur du type : "storageclass.storage.k8s.io 'manual' not found".

**Exercice 3 : CrÃ©er un PVC**

```bash
# CrÃ©er le PVC
kubectl apply -f 04-persistent-volume-claim.yaml

# VÃ©rifier le PVC
kubectl get pvc
kubectl describe pvc pvc-demo

# RevÃ©rifier le PV
kubectl get pv
```

Le PV devrait maintenant Ãªtre **Bound** au PVC.

### 2.6 Utiliser le PVC dans un Pod

CrÃ©er `05-pod-with-pvc.yaml` :

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-pvc
spec:
  containers:
  - name: app
    image: nginx:alpine
    volumeMounts:
    - name: persistent-storage
      mountPath: /usr/share/nginx/html

  volumes:
  - name: persistent-storage
    persistentVolumeClaim:
      claimName: pvc-demo
```

**Exercice 4 : Tester la persistance**

```bash
# CrÃ©er le pod
kubectl apply -f 05-pod-with-pvc.yaml

# Attendre que le pod soit prÃªt
kubectl wait --for=condition=ready pod/pod-with-pvc --timeout=60s

# Ã‰crire un fichier HTML dans le volume
kubectl exec pod-with-pvc -- sh -c 'echo "<h1>Persistent Data!</h1>" > /usr/share/nginx/html/index.html'

# VÃ©rifier avec port-forward
kubectl port-forward pod/pod-with-pvc 8080:80 &
curl localhost:8080
pkill -f "port-forward"

# Supprimer le pod
kubectl delete pod pod-with-pvc

# RecrÃ©er le pod
kubectl apply -f 05-pod-with-pvc.yaml

# Attendre que le pod soit prÃªt
kubectl wait --for=condition=ready pod/pod-with-pvc --timeout=60s

# VÃ©rifier que les donnÃ©es sont toujours lÃ 
kubectl exec pod-with-pvc -- cat /usr/share/nginx/html/index.html
```

Les donnÃ©es persistent malgrÃ© la suppression du pod !

## Partie 3 : StorageClass et provisionnement dynamique

### 3.1 Qu'est-ce qu'une StorageClass ?

Une StorageClass permet de dÃ©finir diffÃ©rentes classes de stockage avec provisionnement automatique des PV.

**Avantages** :
- Pas besoin de crÃ©er les PV manuellement
- Provisionnement Ã  la demande
- DiffÃ©rentes classes pour diffÃ©rents besoins (SSD, HDD, etc.)

### 3.2 StorageClass par dÃ©faut

```bash
# Lister les StorageClasses disponibles
kubectl get storageclass

# DÃ©crire la StorageClass par dÃ©faut
kubectl describe storageclass standard
```

**Avec minikube :** La StorageClass `standard` utilise le provisioner `k8s.io/minikube-hostpath`.

**Avec kubeadm :** La StorageClass par dÃ©faut dÃ©pend de votre installation. Avec l'installation de base kubeadm, **aucune StorageClass** n'est crÃ©Ã©e par dÃ©faut. Vous devez :
- Soit installer un provisioner comme [local-path-provisioner](https://github.com/rancher/local-path-provisioner)
- Soit utiliser un provisioner cloud si vous Ãªtes sur un cloud provider
- Soit crÃ©er manuellement les PV (provisionnement statique)

**Installation de local-path-provisioner pour kubeadm :**
```bash
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.26/deploy/local-path-storage.yaml

# DÃ©finir comme StorageClass par dÃ©faut
kubectl patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

# VÃ©rifier
kubectl get storageclass
```

### 3.3 CrÃ©er une StorageClass personnalisÃ©e

**Pour minikube :**

CrÃ©er `06-storage-class-minikube.yaml` :

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-storage
provisioner: k8s.io/minikube-hostpath
parameters:
  type: pd-ssd
reclaimPolicy: Delete
volumeBindingMode: Immediate
allowVolumeExpansion: true
```

**Pour kubeadm (avec local-path-provisioner) :**

CrÃ©er `06-storage-class-kubeadm.yaml` :

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-storage
provisioner: rancher.io/local-path
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
```

```bash
# CrÃ©er la StorageClass (adapter le nom du fichier selon votre environnement)
kubectl apply -f 06-storage-class-minikube.yaml  # Pour minikube
# OU
kubectl apply -f 06-storage-class-kubeadm.yaml   # Pour kubeadm

# VÃ©rifier
kubectl get storageclass
```

### 3.4 PVC avec provisionnement dynamique

CrÃ©er `07-dynamic-pvc.yaml` :

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: dynamic-pvc
spec:
  storageClassName: standard
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
```

**Exercice 5 : Provisionnement dynamique**

```bash
# CrÃ©er le PVC
kubectl apply -f 07-dynamic-pvc.yaml

# Observer la crÃ©ation automatique du PV
kubectl get pvc dynamic-pvc
kubectl get pv

# Un PV a Ã©tÃ© crÃ©Ã© automatiquement !
```

## Partie 4 : Cas pratique - Base de donnÃ©es MySQL

### 4.1 DÃ©ploiement MySQL avec persistance

CrÃ©er `08-mysql-deployment.yaml` :

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc
spec:
  storageClassName: standard
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
---
apiVersion: v1
kind: Secret
metadata:
  name: mysql-secret
type: Opaque
stringData:
  mysql-root-password: "MotDePasseSecurise123"
  mysql-database: "app_db"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        ports:
        - containerPort: 3306
          name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: mysql-root-password
        - name: MYSQL_DATABASE
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: mysql-database
        volumeMounts:
        - name: mysql-storage
          mountPath: /var/lib/mysql
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
      volumes:
      - name: mysql-storage
        persistentVolumeClaim:
          claimName: mysql-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  selector:
    app: mysql
  ports:
  - port: 3306
    targetPort: 3306
  clusterIP: None  # Headless service
```

**Exercice 6 : DÃ©ployer MySQL**

```bash
# Appliquer le manifest complet
kubectl apply -f 08-mysql-deployment.yaml

# VÃ©rifier les ressources crÃ©Ã©es
kubectl get pvc mysql-pvc
kubectl get deployment mysql
kubectl get pods -l app=mysql
kubectl get svc mysql

# Attendre que MySQL soit prÃªt
kubectl wait --for=condition=ready pod -l app=mysql --timeout=120s

# Voir les logs de MySQL
kubectl logs -l app=mysql
```

### 4.2 Tester MySQL

```bash
# Se connecter Ã  MySQL
kubectl exec -it deployment/mysql -- mysql -uroot -pMotDePasseSecurise123

# Dans le shell MySQL, exÃ©cuter :
# SHOW DATABASES;
# USE app_db;
# CREATE TABLE users (id INT PRIMARY KEY, name VARCHAR(50));
# INSERT INTO users VALUES (1, 'Alice'), (2, 'Bob');
# SELECT * FROM users;
# EXIT;
```

**Exercice 7 : VÃ©rifier la persistance**

```bash
# Supprimer le pod MySQL
kubectl delete pod -l app=mysql

# Attendre que le deployment recrÃ©e le pod
kubectl wait --for=condition=ready pod -l app=mysql --timeout=120s

# Se reconnecter
kubectl exec -it deployment/mysql -- mysql -uroot -pMotDePasseSecurise123

# VÃ©rifier que les donnÃ©es sont toujours lÃ  :
# USE app_db;
# SELECT * FROM users;
# EXIT;
```

Les donnÃ©es ont survÃ©cu Ã  la suppression du pod !

### 4.3 Client MySQL pour tester

CrÃ©er `09-mysql-client.yaml` :

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mysql-client
spec:
  containers:
  - name: mysql-client
    image: mysql:8.0
    command: ['sh', '-c', 'sleep 3600']
```

```bash
# CrÃ©er le client
kubectl apply -f 09-mysql-client.yaml

# Se connecter depuis le client
kubectl exec -it mysql-client -- mysql -h mysql -uroot -pMotDePasseSecurise123

# Dans MySQL :
# USE app_db;
# SELECT * FROM users;
# EXIT;
```

## Partie 5 : Gestion avancÃ©e du stockage

### 5.1 Expansion de volume

L'expansion de volume permet d'augmenter la taille d'un PVC existant sans recrÃ©er le volume. Cette fonctionnalitÃ© dÃ©pend de deux conditions :

1. La StorageClass doit avoir `allowVolumeExpansion: true`
2. Le driver de stockage doit supporter l'expansion

**Ã‰tape 1 : VÃ©rifier que la StorageClass permet l'expansion**

```bash
# VÃ©rifier la StorageClass standard de minikube
kubectl get storageclass standard -o yaml | grep allowVolumeExpansion
```

**Important** : Si `allowVolumeExpansion` n'est pas prÃ©sent ou est `false`, vous avez deux options :

**Option A** : Utiliser la StorageClass `fast-storage` crÃ©Ã©e dans la partie 3.3 qui supporte l'expansion :

```bash
# CrÃ©er un nouveau PVC avec fast-storage
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: expandable-pvc
spec:
  storageClassName: fast-storage
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
EOF

# VÃ©rifier que le PVC est bien crÃ©Ã©
kubectl get pvc expandable-pvc

# Ã‰diter le PVC pour augmenter la taille
kubectl edit pvc expandable-pvc

# Modifier storage: 2Gi en storage: 5Gi
# Sauvegarder et quitter

# VÃ©rifier l'expansion
kubectl get pvc expandable-pvc
kubectl describe pvc expandable-pvc
```

**Option B** : Activer l'expansion sur la StorageClass standard (si vous avez les permissions) :

```bash
# Ã‰diter la StorageClass standard
kubectl patch storageclass standard -p '{"allowVolumeExpansion": true}'

# VÃ©rifier la modification
kubectl get storageclass standard -o yaml | grep allowVolumeExpansion

# Maintenant vous pouvez Ã©diter le PVC dynamic-pvc
kubectl edit pvc dynamic-pvc

# Modifier storage: 2Gi en storage: 5Gi
# Sauvegarder et quitter

# VÃ©rifier l'expansion
kubectl get pvc dynamic-pvc
kubectl describe pvc dynamic-pvc
```

**Note** : L'expansion de volume peut nÃ©cessiter un redÃ©marrage du pod utilisant le PVC pour que la nouvelle taille soit reconnue par le systÃ¨me de fichiers.

#### 5.1.1 Troubleshooting : La taille n'a pas changÃ©

Si aprÃ¨s l'expansion du PVC, la taille ne se reflÃ¨te pas dans le pod, voici les Ã©tapes de diagnostic et rÃ©solution :

**Ã‰tape 1 : VÃ©rifier le statut du PVC**

```bash
# VÃ©rifier l'Ã©tat de l'expansion
kubectl get pvc <nom-du-pvc>
kubectl describe pvc <nom-du-pvc>

# Chercher des messages comme :
# - "Waiting for user to (re-)start a pod to finish file system resize"
# - "FileSystemResizePending"
```

**Ã‰tape 2 : VÃ©rifier la taille dans le pod**

```bash
# VÃ©rifier la taille actuelle du volume dans le pod
kubectl exec <nom-du-pod> -- df -h <point-de-montage>

# Exemple avec le PVC montÃ© sur /data :
kubectl exec my-pod -- df -h /data
```

**Solutions selon le problÃ¨me identifiÃ© :**

**Solution 1 : RedÃ©marrer le pod (le plus courant)**

```bash
# Si c'est un pod autonome
kubectl delete pod <nom-du-pod>
kubectl apply -f <fichier-du-pod>.yaml

# Si c'est un Deployment
kubectl rollout restart deployment <nom-du-deployment>

# Attendre que le nouveau pod soit prÃªt
kubectl wait --for=condition=ready pod -l app=<label> --timeout=120s

# VÃ©rifier Ã  nouveau la taille
kubectl exec <nom-du-pod> -- df -h <point-de-montage>
```

**Solution 2 : Redimensionner manuellement le systÃ¨me de fichiers**

Si le redÃ©marrage du pod ne suffit pas, il faut redimensionner manuellement le systÃ¨me de fichiers :

```bash
# Pour un systÃ¨me de fichiers ext4
kubectl exec <nom-du-pod> -- resize2fs <device>

# Exemple avec le device par dÃ©faut
kubectl exec my-pod -- sh -c 'df -h /data && resize2fs $(df /data | tail -1 | cut -d" " -f1) && df -h /data'

# Pour un systÃ¨me de fichiers XFS
kubectl exec <nom-du-pod> -- xfs_growfs <point-de-montage>

# Exemple
kubectl exec my-pod -- xfs_growfs /data
```

**Solution 3 : VÃ©rifier les conditions du PVC**

```bash
# Afficher les dÃ©tails complets du PVC
kubectl get pvc <nom-du-pvc> -o yaml

# Chercher dans status.conditions pour des erreurs
# VÃ©rifier status.capacity vs spec.resources.requests.storage
```

**Solution 4 : VÃ©rifier les logs du contrÃ´leur**

```bash
# VÃ©rifier les logs du provisioner de stockage
kubectl logs -n kube-system -l app=storage-provisioner

# Pour minikube spÃ©cifiquement
minikube logs | grep -i "resize\|expand"
```

**Exemple complet de test d'expansion :**

```bash
# 1. CrÃ©er un pod de test avec le PVC
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: test-expansion
spec:
  containers:
  - name: busybox
    image: busybox
    command: ['sh', '-c', 'sleep 3600']
    volumeMounts:
    - name: storage
      mountPath: /data
  volumes:
  - name: storage
    persistentVolumeClaim:
      claimName: expandable-pvc
EOF

# 2. VÃ©rifier la taille initiale
kubectl exec test-expansion -- df -h /data

# 3. Ã‰tendre le PVC
kubectl patch pvc expandable-pvc -p '{"spec":{"resources":{"requests":{"storage":"5Gi"}}}}'

# 4. VÃ©rifier le statut de l'expansion
kubectl describe pvc expandable-pvc

# 5. RedÃ©marrer le pod
kubectl delete pod test-expansion
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: test-expansion
spec:
  containers:
  - name: busybox
    image: busybox
    command: ['sh', '-c', 'sleep 3600']
    volumeMounts:
    - name: storage
      mountPath: /data
  volumes:
  - name: storage
    persistentVolumeClaim:
      claimName: expandable-pvc
EOF

# 6. Attendre et vÃ©rifier la nouvelle taille
kubectl wait --for=condition=ready pod/test-expansion --timeout=60s
kubectl exec test-expansion -- df -h /data

# Nettoyage
kubectl delete pod test-expansion
```

**Limitations connues :**

- Certains drivers de stockage ne supportent que l'expansion en ligne (sans redÃ©marrage)
- D'autres nÃ©cessitent obligatoirement un redÃ©marrage du pod
- L'expansion n'est jamais possible en rÃ©duction (shrink), seulement en augmentation
- Le provisioner `k8s.io/minikube-hostpath` supporte l'expansion mais nÃ©cessite un redÃ©marrage

### 5.2 Installation du driver CSI

Pour utiliser des fonctionnalitÃ©s avancÃ©es comme les snapshots de volumes, il est nÃ©cessaire d'installer le driver CSI (Container Storage Interface).

**Pourquoi installer le CSI driver ?**

Le driver CSI `csi-hostpath-driver` permet :
- La crÃ©ation de snapshots de volumes
- La restauration de volumes Ã  partir de snapshots
- Le clonage de volumes
- Une gestion plus avancÃ©e du stockage

#### Option A : Avec minikube

```bash
# Activer l'addon csi-hostpath-driver sur minikube
minikube addons enable csi-hostpath-driver

# VÃ©rifier que l'addon est activÃ©
minikube addons list | grep csi-hostpath-driver

# Attendre que les pods CSI soient prÃªts
kubectl wait --for=condition=ready pod -n kube-system -l app=csi-hostpath-driver --timeout=120s
```

#### Option B : Avec kubeadm

**Installation manuelle du csi-hostpath-driver :**

```bash
# Cloner le repo du driver CSI hostpath
git clone https://github.com/kubernetes-csi/csi-driver-host-path.git
cd csi-driver-host-path

# DÃ©ployer le driver
./deploy/kubernetes-latest/deploy.sh

# VÃ©rifier le dÃ©ploiement
kubectl get pods -n default | grep csi

# Attendre que les pods soient prÃªts
kubectl wait --for=condition=ready pod -l app=csi-hostpathplugin --timeout=120s
```

**Alternative : Utiliser le manifest direct :**

```bash
# Installer les CRDs pour les snapshots
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml

# Installer le snapshot controller
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml

# Installer le driver hostpath
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/csi-driver-host-path/master/deploy/kubernetes-latest/hostpath/csi-hostpath-driverinfo.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/csi-driver-host-path/master/deploy/kubernetes-latest/hostpath/csi-hostpath-plugin.yaml
```

**VÃ©rification de l'installation**

```bash
# VÃ©rifier les pods CSI dans kube-system
kubectl get pods -n kube-system | grep csi

# VÃ©rifier la VolumeSnapshotClass crÃ©Ã©e automatiquement
kubectl get volumesnapshotclass

# VÃ©rifier le driver CSI
kubectl get csidrivers
```

Vous devriez voir :
- Les pods `csi-hostpath-driver-*` en Ã©tat `Running`
- Une `VolumeSnapshotClass` nommÃ©e `csi-hostpath-snapclass`
- Le driver `hostpath.csi.k8s.io` dans la liste des CSI drivers

**Note** : Sur minikube, le driver CSI utilise Ã©galement le stockage local du nÅ“ud, mais offre des fonctionnalitÃ©s supplÃ©mentaires par rapport au provisioner standard.

### 5.3 Politiques de rÃ©clamation (Reclaim Policies)

Les PV ont diffÃ©rentes politiques de rÃ©clamation :

- **Retain** : Conserver les donnÃ©es aprÃ¨s suppression du PVC
- **Delete** : Supprimer le PV et les donnÃ©es (dÃ©faut pour provisionnement dynamique)
- **Recycle** : Effacer les donnÃ©es et rendre le PV disponible (obsolÃ¨te)

CrÃ©er `10-pv-retain.yaml` :

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-retain
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: manual
  hostPath:
    path: "/mnt/data-retain"
```

### 5.4 Snapshots de volumes (avancÃ©)

Les snapshots permettent de crÃ©er des sauvegardes ponctuelles de vos volumes. GrÃ¢ce au driver CSI installÃ© dans la section prÃ©cÃ©dente, vous pouvez maintenant crÃ©er des snapshots.

**Installation de la VolumeSnapshotClass sur minikube**

Sur minikube, mÃªme aprÃ¨s avoir activÃ© l'addon `csi-hostpath-driver`, la `VolumeSnapshotClass` nÃ©cessaire pour crÃ©er des snapshots n'est pas automatiquement crÃ©Ã©e. Il faut activer un addon supplÃ©mentaire.

```bash
# Activer l'addon volumesnapshots (qui inclut csi-hostpath-snapclass)
minikube addons enable volumesnapshots

# VÃ©rifier que l'addon est activÃ©
minikube addons list | grep volumesnapshots

# VÃ©rifier que la VolumeSnapshotClass a Ã©tÃ© crÃ©Ã©e
kubectl get volumesnapshotclass

# Vous devriez voir : csi-hostpath-snapclass
```

**Alternative : CrÃ©er manuellement la VolumeSnapshotClass**

Si l'addon `volumesnapshots` n'est pas disponible, vous pouvez crÃ©er manuellement la VolumeSnapshotClass :

```bash
kubectl apply -f - <<EOF
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: csi-hostpath-snapclass
driver: hostpath.csi.k8s.io
deletionPolicy: Delete
EOF

# VÃ©rifier la crÃ©ation
kubectl get volumesnapshotclass
kubectl describe volumesnapshotclass csi-hostpath-snapclass
```

**Note importante** : Sans la VolumeSnapshotClass, vous obtiendrez une erreur lors de la crÃ©ation de snapshots indiquant que la classe n'existe pas.

**CrÃ©ation d'un snapshot**

CrÃ©er `11-volume-snapshot.yaml` :

```yaml
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: mysql-snapshot
spec:
  volumeSnapshotClassName: csi-hostpath-snapclass
  source:
    persistentVolumeClaimName: mysql-pvc
```

**Exercice 8 : CrÃ©er et utiliser un snapshot**

```bash
# 1. CrÃ©er le snapshot du PVC MySQL
kubectl apply -f 11-volume-snapshot.yaml

# 2. VÃ©rifier le snapshot
kubectl get volumesnapshot
kubectl describe volumesnapshot mysql-snapshot

# 3. Restaurer depuis un snapshot - crÃ©er un nouveau PVC
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc-restored
spec:
  storageClassName: csi-hostpath-sc
  dataSource:
    name: mysql-snapshot
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
EOF

# 4. VÃ©rifier le nouveau PVC
kubectl get pvc mysql-pvc-restored
```

**Note** : Les snapshots sont utiles pour :
- Sauvegardes avant modifications importantes
- Clonage rapide de volumes
- Tests et dÃ©veloppement
- RÃ©cupÃ©ration aprÃ¨s incident

## Partie 6 : Bonnes pratiques

### 6.1 Bonnes pratiques gÃ©nÃ©rales

1. **Utiliser le provisionnement dynamique** quand possible
   - Ã‰vite la gestion manuelle des PV
   - Simplifie les dÃ©ploiements

2. **DÃ©finir des limites de ressources**
   - SpÃ©cifier la taille exacte nÃ©cessaire
   - Ã‰viter le gaspillage de stockage

3. **Choisir le bon mode d'accÃ¨s**
   - RWO pour bases de donnÃ©es
   - RWX pour applications multi-nÅ“uds

4. **Utiliser des StorageClasses appropriÃ©es**
   - SSD pour performance
   - HDD pour stockage Ã©conomique

5. **Sauvegarder rÃ©guliÃ¨rement**
   - Utiliser des snapshots
   - Exporter les donnÃ©es critiques

### 6.2 SÃ©curitÃ© du stockage - Guide complet

La sÃ©curitÃ© du stockage est critique dans Kubernetes. Un volume mal configurÃ© peut exposer des donnÃ©es sensibles, compromettre le cluster ou donner accÃ¨s au systÃ¨me de fichiers de l'hÃ´te.

#### 6.2.1 Chiffrement des donnÃ©es

##### A. Chiffrement at-rest (donnÃ©es au repos)

**ğŸ”’ RÃ¨gle d'or :** Toutes les donnÃ©es sensibles doivent Ãªtre chiffrÃ©es au repos, que ce soit dans le cloud ou on-premise.

**Option 1 : Chiffrement au niveau du cloud provider**

```yaml
# AWS EBS avec chiffrement KMS
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: encrypted-gp3
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
  encrypted: "true"
  kmsKeyId: "arn:aws:kms:us-east-1:123456789:key/your-key-id"
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
```

```yaml
# Azure Disk avec chiffrement
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: encrypted-premium
provisioner: disk.csi.azure.com
parameters:
  storageaccounttype: Premium_LRS
  kind: Managed
  diskEncryptionSetID: "/subscriptions/{sub}/resourceGroups/{rg}/providers/Microsoft.Compute/diskEncryptionSets/{des}"
allowVolumeExpansion: true
```

```yaml
# GCP Persistent Disk avec chiffrement CMEK
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: encrypted-pd-ssd
provisioner: pd.csi.storage.gke.io
parameters:
  type: pd-ssd
  disk-encryption-kms-key: "projects/PROJECT_ID/locations/LOCATION/keyRings/RING_NAME/cryptoKeys/KEY_NAME"
allowVolumeExpansion: true
```

**Option 2 : Chiffrement au niveau de l'application (LUKS)**

```yaml
# StatefulSet avec init container pour chiffrement LUKS
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: secure-database
spec:
  serviceName: secure-db
  replicas: 1
  selector:
    matchLabels:
      app: secure-db
  template:
    metadata:
      labels:
        app: secure-db
    spec:
      # Init container pour configurer LUKS
      initContainers:
      - name: luks-setup
        image: alpine:latest
        command:
        - sh
        - -c
        - |
          apk add --no-cache cryptsetup
          if [ ! -e /dev/mapper/encrypted ]; then
            echo "Setting up LUKS encryption..."
            # RÃ©cupÃ©rer la clÃ© depuis un Secret
            LUKS_KEY=$(cat /secrets/luks-key)
            echo -n "$LUKS_KEY" | cryptsetup luksFormat /dev/xvdf -
            echo -n "$LUKS_KEY" | cryptsetup luksOpen /dev/xvdf encrypted -
            mkfs.ext4 /dev/mapper/encrypted
          else
            echo "LUKS already configured"
          fi
        securityContext:
          privileged: true  # NÃ©cessaire pour cryptsetup
        volumeMounts:
        - name: luks-key
          mountPath: /secrets
          readOnly: true
        - name: raw-storage
          mountPath: /dev/xvdf

      containers:
      - name: database
        image: postgres:15-alpine
        env:
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: password
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        volumeMounts:
        - name: encrypted-storage
          mountPath: /var/lib/postgresql/data
        securityContext:
          runAsNonRoot: true
          runAsUser: 999
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true

      volumes:
      - name: luks-key
        secret:
          secretName: luks-encryption-key
      - name: raw-storage
        persistentVolumeClaim:
          claimName: raw-pvc
      - name: encrypted-storage
        emptyDir: {}
```

##### B. Chiffrement in-transit (donnÃ©es en transit)

Pour NFS et autres protocoles rÃ©seau:

```yaml
# NFS avec stunnel pour chiffrement TLS
apiVersion: v1
kind: ConfigMap
metadata:
  name: stunnel-config
data:
  stunnel.conf: |
    [nfs]
    client = yes
    accept = 127.0.0.1:2049
    connect = nfs-server.internal:2050
    cert = /etc/stunnel/certs/client.pem
    key = /etc/stunnel/certs/client.key
    CAfile = /etc/stunnel/certs/ca.pem
    verify = 2
---
apiVersion: v1
kind: Pod
metadata:
  name: secure-nfs-client
spec:
  containers:
  # Sidecar stunnel pour chiffrer le trafic NFS
  - name: stunnel
    image: dweomer/stunnel:latest
    volumeMounts:
    - name: stunnel-config
      mountPath: /etc/stunnel/stunnel.conf
      subPath: stunnel.conf
    - name: stunnel-certs
      mountPath: /etc/stunnel/certs
      readOnly: true
    securityContext:
      runAsNonRoot: true
      runAsUser: 65534
      capabilities:
        drop:
        - ALL

  # Application qui utilise NFS via stunnel
  - name: app
    image: myapp:latest
    volumeMounts:
    - name: secure-nfs
      mountPath: /data

  volumes:
  - name: stunnel-config
    configMap:
      name: stunnel-config
  - name: stunnel-certs
    secret:
      secretName: stunnel-tls-certs
  - name: secure-nfs
    nfs:
      server: 127.0.0.1  # Via stunnel local
      path: /exports/data
```

#### 6.2.2 ContrÃ´le d'accÃ¨s et permissions

##### A. SecurityContext pour les volumes

**ğŸ”’ RÃ¨gle :** Toujours spÃ©cifier un SecurityContext pour contrÃ´ler les permissions des volumes.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: secure-pod-with-volume
spec:
  # SecurityContext au niveau Pod
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000  # Groupe propriÃ©taire des volumes montÃ©s
    fsGroupChangePolicy: "OnRootMismatch"
    seccompProfile:
      type: RuntimeDefault

  containers:
  - name: app
    image: nginx:alpine
    # SecurityContext au niveau container
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
      readOnlyRootFilesystem: true

    volumeMounts:
    - name: data
      mountPath: /data
      readOnly: false  # Lecture-Ã©criture
    - name: config
      mountPath: /etc/nginx/conf.d
      readOnly: true  # Lecture seule pour les configs
    - name: cache
      mountPath: /var/cache/nginx
    - name: run
      mountPath: /var/run

  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: app-data
  - name: config
    configMap:
      name: nginx-config
      defaultMode: 0440  # r--r-----
  - name: cache
    emptyDir:
      sizeLimit: 500Mi
  - name: run
    emptyDir:
      medium: Memory  # tmpfs pour les fichiers runtime
      sizeLimit: 100Mi
```

##### B. Isolation des volumes avec SELinux/AppArmor

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: selinux-secured-pod
  annotations:
    # AppArmor profile (sur Ubuntu/Debian)
    container.apparmor.security.beta.kubernetes.io/app: localhost/k8s-apparmor-example
spec:
  securityContext:
    # SELinux (sur RHEL/CentOS)
    seLinuxOptions:
      level: "s0:c123,c456"
      role: "object_r"
      type: "svirt_sandbox_file_t"
      user: "system_u"

  containers:
  - name: app
    image: myapp:latest
    volumeMounts:
    - name: data
      mountPath: /data
    securityContext:
      seLinuxOptions:
        level: "s0:c123,c456"

  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: selinux-pvc
```

#### 6.2.3 Limitation des ressources et quotas

##### A. ResourceQuotas pour le stockage

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage-quota
  namespace: production
spec:
  hard:
    # Limite le nombre de PVC
    persistentvolumeclaims: "10"

    # Limite la capacitÃ© totale demandÃ©e
    requests.storage: "500Gi"

    # Limite par StorageClass
    requests.storage.storageclass.storage.k8s.io/fast-ssd: "100Gi"
    requests.storage.storageclass.storage.k8s.io/standard: "400Gi"

    # Limite le nombre de PVC par classe
    persistentvolumeclaims.storageclass.storage.k8s.io/fast-ssd: "5"
```

##### B. LimitRange pour les PVC

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: storage-limits
  namespace: production
spec:
  limits:
  - type: PersistentVolumeClaim
    max:
      storage: 100Gi  # Taille max par PVC
    min:
      storage: 1Gi    # Taille min par PVC
    default:
      storage: 10Gi   # Taille par dÃ©faut
```

#### 6.2.4 Network Policies pour le stockage

```yaml
# Limiter l'accÃ¨s au serveur NFS
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-nfs-access
  namespace: production
spec:
  podSelector:
    matchLabels:
      role: nfs-server
  policyTypes:
  - Ingress
  ingress:
  - from:
    # Seulement les pods avec ce label peuvent accÃ©der au NFS
    - podSelector:
        matchLabels:
          access-nfs: "true"
    # Seulement depuis le namespace production
    - namespaceSelector:
        matchLabels:
          name: production
    ports:
    - protocol: TCP
      port: 2049
    - protocol: TCP
      port: 111
```

#### 6.2.5 Audit et surveillance

##### A. Audit des accÃ¨s aux volumes

```yaml
# Configuration d'audit pour surveiller les accÃ¨s aux volumes
apiVersion: audit.k8s.io/v1
kind: Policy
rules:
# Auditer toutes les opÃ©rations sur les PV/PVC
- level: RequestResponse
  resources:
  - group: ""
    resources: ["persistentvolumes", "persistentvolumeclaims"]

# Auditer les modifications de StorageClass
- level: RequestResponse
  resources:
  - group: "storage.k8s.io"
    resources: ["storageclasses"]
  verbs: ["create", "update", "patch", "delete"]

# Auditer les accÃ¨s aux Secrets (souvent utilisÃ©s pour les credentials de stockage)
- level: Metadata
  resources:
  - group: ""
    resources: ["secrets"]
  verbs: ["get", "list", "watch"]
```

##### B. Monitoring de l'utilisation du stockage

```yaml
# ServiceMonitor pour Prometheus (avec kube-state-metrics)
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: storage-monitoring
spec:
  selector:
    matchLabels:
      app: kube-state-metrics
  endpoints:
  - port: http-metrics
    interval: 30s

---
# PrometheusRule pour alertes de stockage
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: storage-alerts
spec:
  groups:
  - name: storage
    interval: 30s
    rules:
    # Alerte si PVC presque plein
    - alert: PVCAlmostFull
      expr: |
        (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) > 0.85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "PVC {{ $labels.persistentvolumeclaim }} presque plein"
        description: "Le PVC {{ $labels.persistentvolumeclaim }} est utilisÃ© Ã  {{ $value | humanizePercentage }}"

    # Alerte si PVC en Ã©tat Pending
    - alert: PVCPending
      expr: |
        kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "PVC {{ $labels.persistentvolumeclaim }} en attente de binding"

    # Alerte si PV pas utilisÃ© depuis longtemps
    - alert: UnusedPV
      expr: |
        kube_persistentvolume_status_phase{phase="Available"} == 1
      for: 7d
      labels:
        severity: info
      annotations:
        summary: "PV {{ $labels.persistentvolume }} non utilisÃ© depuis 7 jours"
```

#### 6.2.6 Checklist de sÃ©curitÃ© pour le stockage

##### âœ… Avant de dÃ©ployer en production

**Chiffrement:**
- [ ] DonnÃ©es at-rest chiffrÃ©es (KMS, LUKS, ou chiffrement provider)
- [ ] DonnÃ©es in-transit chiffrÃ©es (TLS, stunnel pour NFS)
- [ ] Rotation des clÃ©s de chiffrement configurÃ©e

**AccÃ¨s et permissions:**
- [ ] SecurityContext dÃ©fini avec runAsNonRoot: true
- [ ] fsGroup et fsGroupChangePolicy configurÃ©s
- [ ] Volumes en readOnly quand possible
- [ ] Pas de hostPath en production (sauf cas exceptionnel documentÃ©)
- [ ] Pas de volumes montÃ©s avec privileged: true

**Isolation:**
- [ ] NetworkPolicies limitant l'accÃ¨s aux backends de stockage
- [ ] Namespaces sÃ©parÃ©s pour environnements diffÃ©rents
- [ ] RBAC limitant qui peut crÃ©er/modifier les PV/PVC
- [ ] SELinux ou AppArmor configurÃ©

**Quotas et limites:**
- [ ] ResourceQuota dÃ©fini par namespace
- [ ] LimitRange configurÃ© pour les PVC
- [ ] Taille maximale des PVC limitÃ©e

**Sauvegardes et rÃ©cupÃ©ration:**
- [ ] Snapshots rÃ©guliers configurÃ©s
- [ ] Backup hors cluster (S3, backup systÃ¨me)
- [ ] Plan de disaster recovery testÃ©
- [ ] ReclaimPolicy appropriÃ©e (Retain pour production)

**Monitoring:**
- [ ] MÃ©triques de stockage collectÃ©es
- [ ] Alertes configurÃ©es (espace disque, PVC pending, etc.)
- [ ] Audit logs activÃ©s pour les opÃ©rations sensibles
- [ ] Dashboard de visualisation dÃ©ployÃ©

**Documentation:**
- [ ] Architecture de stockage documentÃ©e
- [ ] ProcÃ©dures de backup/restore documentÃ©es
- [ ] Politique de rÃ©tention dÃ©finie
- [ ] Contacts et escalade en cas d'incident

##### âŒ Anti-patterns Ã  Ã©viter

```yaml
# âŒ MAUVAIS : hostPath avec accÃ¨s root
apiVersion: v1
kind: Pod
metadata:
  name: dangerous-pod
spec:
  containers:
  - name: app
    image: myapp
    securityContext:
      privileged: true  # âŒ AccÃ¨s complet au systÃ¨me
    volumeMounts:
    - name: host-root
      mountPath: /host
  volumes:
  - name: host-root
    hostPath:
      path: /  # âŒ Monte la racine de l'hÃ´te !
      type: Directory
```

```yaml
# âŒ MAUVAIS : Secret en clair dans le YAML
apiVersion: v1
kind: Secret
metadata:
  name: bad-secret
type: Opaque
stringData:
  password: "SuperSecretPassword123"  # âŒ En clair dans Git !
```

```yaml
# âŒ MAUVAIS : PVC sans limite de taille
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: unlimited-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1000Ti  # âŒ Demande Ã©norme sans justification
  storageClassName: expensive-ssd
```

```yaml
# âŒ MAUVAIS : Volume partagÃ© entre namespaces sans contrÃ´le
apiVersion: v1
kind: PersistentVolume
metadata:
  name: shared-pv
spec:
  capacity:
    storage: 100Gi
  accessModes:
  - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  nfs:
    server: nfs-server
    path: /shared  # âŒ Accessible depuis tous les namespaces
  # âŒ Pas de restrictions d'accÃ¨s
```

##### âœ… Bonnes pratiques

```yaml
# âœ… BON : Pod sÃ©curisÃ© avec volume
apiVersion: v1
kind: Pod
metadata:
  name: secure-pod
  namespace: production
spec:
  serviceAccountName: limited-sa
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
    seccompProfile:
      type: RuntimeDefault

  containers:
  - name: app
    image: myapp:1.2.3  # âœ… Version prÃ©cise
    securityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
      capabilities:
        drop:
        - ALL

    volumeMounts:
    - name: data
      mountPath: /data
    - name: tmp
      mountPath: /tmp

    resources:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "256Mi"
        cpu: "200m"

  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: encrypted-pvc  # âœ… PVC avec chiffrement
  - name: tmp
    emptyDir:
      sizeLimit: 100Mi  # âœ… Limite de taille
```

```yaml
# âœ… BON : StorageClass sÃ©curisÃ©e avec chiffrement
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: secure-storage
  labels:
    environment: production
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
  iops: "3000"
  encrypted: "true"  # âœ… Chiffrement activÃ©
  kmsKeyId: "arn:aws:kms:region:account:key/key-id"
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Retain  # âœ… Prevent accidental data loss
```

```yaml
# âœ… BON : RBAC restrictif pour le stockage
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pvc-user
  namespace: production
rules:
- apiGroups: [""]
  resources: ["persistentvolumeclaims"]
  verbs: ["get", "list"]  # âœ… Read-only pour les utilisateurs
- apiGroups: [""]
  resources: ["persistentvolumeclaims"]
  verbs: ["create", "delete"]
  resourceNames: []  # âœ… Pas de delete sans review
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pvc-admin
  namespace: production
rules:
- apiGroups: [""]
  resources: ["persistentvolumeclaims"]
  verbs: ["*"]  # âœ… Full access pour les admins seulement
```

### 6.3 Monitoring du stockage

```bash
# VÃ©rifier l'utilisation des PV
kubectl get pv

# VÃ©rifier l'utilisation des PVC
kubectl get pvc --all-namespaces

# DÃ©crire un PVC pour voir l'utilisation
kubectl describe pvc mysql-pvc

# Voir l'utilisation dans un pod
kubectl exec <pod-name> -- df -h /mount/path
```

## Partie 7 : Exercices pratiques

### Exercice Final 1 : DÃ©ployer MySQL avec sÃ©curitÃ© renforcÃ©e

Utilisez le fichier `12-mysql-deployment-secure.yaml` pour dÃ©ployer MySQL avec toutes les bonnes pratiques de sÃ©curitÃ©.

**Objectifs :**
- Comprendre les SecurityContext et leur impact sur les volumes
- ImplÃ©menter des NetworkPolicies pour isoler la base de donnÃ©es
- Utiliser des init containers pour prÃ©parer les volumes
- Configurer des probes de santÃ©
- Monitorer l'utilisation du stockage

**Ã‰tapes :**

```bash
# 1. CrÃ©er un namespace dÃ©diÃ©
kubectl create namespace production

# 2. Appliquer le dÃ©ploiement sÃ©curisÃ©
kubectl apply -f 12-mysql-deployment-secure.yaml

# 3. VÃ©rifier le dÃ©ploiement
kubectl get all -n production -l app=mysql
kubectl get pvc -n production
kubectl get networkpolicies -n production

# 4. VÃ©rifier les SecurityContext
kubectl describe pod -n production -l app=mysql | grep -A 10 "Security Context"

# 5. Tester la connexion (crÃ©er un pod client autorisÃ©)
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: mysql-client
  namespace: production
  labels:
    access-mysql: "true"  # Important pour la NetworkPolicy
spec:
  containers:
  - name: mysql-client
    image: mysql:8.0
    command: ['sh', '-c', 'sleep 3600']
  securityContext:
    runAsNonRoot: true
    runAsUser: 999
EOF

# 6. Se connecter Ã  MySQL
kubectl exec -it -n production mysql-client -- mysql -h mysql-secure-svc -uroot -pVotreMotDePasseComplexe!2024

# Dans MySQL:
# SHOW DATABASES;
# USE app_production;
# CREATE TABLE test_security (id INT PRIMARY KEY, data VARCHAR(100));
# INSERT INTO test_security VALUES (1, 'DonnÃ©es sÃ©curisÃ©es');
# SELECT * FROM test_security;
# EXIT;

# 7. VÃ©rifier la persistance: supprimer le pod MySQL
kubectl delete pod -n production -l app=mysql

# 8. Attendre la recrÃ©ation et vÃ©rifier les donnÃ©es
kubectl wait --for=condition=ready pod -n production -l app=mysql --timeout=120s
kubectl exec -it -n production mysql-client -- mysql -h mysql-secure-svc -uroot -pVotreMotDePasseComplexe!2024 -e "SELECT * FROM app_production.test_security;"

# 9. VÃ©rifier les mÃ©triques (si Prometheus installÃ©)
kubectl port-forward -n production svc/mysql-secure-svc 9104:9104 &
curl localhost:9104/metrics | grep mysql_
pkill -f "port-forward"

# 10. Nettoyage
kubectl delete namespace production
```

**Questions de rÃ©flexion :**
1. Pourquoi utilise-t-on `fsGroup: 999` dans le SecurityContext ?
2. Quel est l'avantage d'une NetworkPolicy pour MySQL ?
3. Pourquoi `readOnlyRootFilesystem: true` n'est pas possible pour MySQL ?
4. Comment les init containers amÃ©liorent-ils la sÃ©curitÃ© ?

### Exercice Final 2 : Comparer diffÃ©rentes StorageClasses

Utilisez le fichier `13-storage-class-examples-secure.yaml` pour comprendre les diffÃ©rences entre les types de stockage.

**Objectifs :**
- Comprendre les paramÃ¨tres de chaque StorageClass
- Apprendre Ã  choisir la bonne classe selon le cas d'usage
- ImplÃ©menter des quotas et limites
- Configurer le chiffrement

**Ã‰tapes :**

```bash
# 1. Analyser les StorageClasses disponibles
kubectl get storageclass
kubectl describe storageclass standard

# 2. CrÃ©er une StorageClass personnalisÃ©e (adapter selon votre environnement)
# Pour minikube:
cat > my-storage-class.yaml <<EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-encrypted
  labels:
    environment: production
provisioner: k8s.io/minikube-hostpath
parameters:
  type: local
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
reclaimPolicy: Retain
EOF

kubectl apply -f my-storage-class.yaml

# 3. CrÃ©er des PVC avec diffÃ©rentes StorageClasses
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-standard
spec:
  storageClassName: standard
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-fast-encrypted
spec:
  storageClassName: fast-encrypted
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
EOF

# 4. Comparer les PV crÃ©Ã©s
kubectl get pv
kubectl describe pv | grep -E "Name:|StorageClass:|Reclaim Policy:"

# 5. Appliquer des quotas (crÃ©er un namespace de test)
kubectl create namespace quota-test
kubectl apply -f - <<EOF
apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage-quota
  namespace: quota-test
spec:
  hard:
    requests.storage: "20Gi"
    persistentvolumeclaims: "5"
EOF

# 6. Tester le quota
kubectl get resourcequota -n quota-test storage-quota

# Essayer de crÃ©er 6 PVC de 5Gi chacun (devrait Ã©chouer au 5Ã¨me)
for i in {1..6}; do
  kubectl apply -n quota-test -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-test-$i
spec:
  storageClassName: standard
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
EOF
done

# VÃ©rifier les erreurs
kubectl describe resourcequota -n quota-test storage-quota

# 7. Nettoyage
kubectl delete namespace quota-test
kubectl delete pvc pvc-standard pvc-fast-encrypted
```

**Questions de rÃ©flexion :**
1. Quelle est la diffÃ©rence entre `volumeBindingMode: Immediate` et `WaitForFirstConsumer` ?
2. Pourquoi utiliser `reclaimPolicy: Retain` en production ?
3. Comment le chiffrement est-il configurÃ© dans les diffÃ©rents clouds ?
4. Quel est l'impact des quotas sur la gestion des ressources ?

### Exercice Final 3 : Stockage rÃ©seau et partage de donnÃ©es

Utilisez le fichier `14-network-storage-examples-secure.yaml` pour explorer les options de stockage rÃ©seau.

**Objectifs :**
- Configurer un stockage NFS partagÃ©
- Comprendre les modes d'accÃ¨s (RWO, ROX, RWX)
- ImplÃ©menter le partage de donnÃ©es entre pods
- SÃ©curiser l'accÃ¨s au stockage rÃ©seau

**Ã‰tapes :**

```bash
# 1. DÃ©ployer un serveur NFS de test (UNIQUEMENT pour dev/test)
kubectl create namespace storage-system
kubectl apply -f 14-network-storage-examples-secure.yaml

# Attendre que le serveur NFS soit prÃªt
kubectl wait --for=condition=ready pod -n storage-system -l app=nfs-server --timeout=120s

# 2. VÃ©rifier le service NFS
kubectl get svc -n storage-system nfs-server
kubectl describe svc -n storage-system nfs-server

# 3. CrÃ©er un PV NFS pointant vers notre serveur
NFS_SERVER_IP=$(kubectl get svc -n storage-system nfs-server -o jsonpath='{.spec.clusterIP}')

kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-test-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  nfs:
    server: ${NFS_SERVER_IP}
    path: "/"
  mountOptions:
    - nfsvers=4.1
    - hard
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-test-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  storageClassName: ""
  volumeName: nfs-test-pv
EOF

# 4. DÃ©ployer un writer (Ã©crit des fichiers)
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-writer
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nfs-writer
  template:
    metadata:
      labels:
        app: nfs-writer
    spec:
      containers:
      - name: writer
        image: busybox
        command:
        - sh
        - -c
        - |
          while true; do
            echo "\$(date): Message from writer" >> /data/shared.log
            sleep 5
          done
        volumeMounts:
        - name: nfs-storage
          mountPath: /data
      volumes:
      - name: nfs-storage
        persistentVolumeClaim:
          claimName: nfs-test-pvc
EOF

# 5. DÃ©ployer plusieurs readers (lisent les fichiers)
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-reader
spec:
  replicas: 3  # 3 replicas qui lisent en mÃªme temps
  selector:
    matchLabels:
      app: nfs-reader
  template:
    metadata:
      labels:
        app: nfs-reader
    spec:
      containers:
      - name: reader
        image: busybox
        command:
        - sh
        - -c
        - |
          while true; do
            echo "=== Latest logs from \$(hostname) ==="
            tail -n 3 /data/shared.log
            sleep 10
          done
        volumeMounts:
        - name: nfs-storage
          mountPath: /data
          readOnly: true  # Lecture seule
      volumes:
      - name: nfs-storage
        persistentVolumeClaim:
          claimName: nfs-test-pvc
EOF

# 6. VÃ©rifier que les readers lisent les donnÃ©es du writer
kubectl logs -l app=nfs-reader --tail=10

# 7. VÃ©rifier le partage: Ã©crire depuis un reader (devrait Ã©chouer car readOnly)
READER_POD=$(kubectl get pod -l app=nfs-reader -o jsonpath='{.items[0].metadata.name}')
kubectl exec $READER_POD -- sh -c 'echo "test" >> /data/shared.log' 2>&1 | grep "Read-only"

# 8. VÃ©rifier que tous les readers voient les mÃªmes donnÃ©es
for pod in $(kubectl get pods -l app=nfs-reader -o jsonpath='{.items[*].metadata.name}'); do
  echo "=== Pod: $pod ==="
  kubectl exec $pod -- tail -n 2 /data/shared.log
done

# 9. Test de performance: Ã©crire beaucoup de donnÃ©es
WRITER_POD=$(kubectl get pod -l app=nfs-writer -o jsonpath='{.items[0].metadata.name}')
kubectl exec $WRITER_POD -- sh -c 'dd if=/dev/zero of=/data/testfile bs=1M count=100'

# 10. VÃ©rifier l'utilisation du stockage
kubectl exec $WRITER_POD -- df -h /data

# 11. Nettoyage
kubectl delete deployment nfs-writer nfs-reader
kubectl delete pvc nfs-test-pvc
kubectl delete pv nfs-test-pv
kubectl delete namespace storage-system
```

**Questions de rÃ©flexion :**
1. Quelle est la diffÃ©rence entre ReadWriteOnce et ReadWriteMany ?
2. Pourquoi monter le volume en readOnly pour les readers ?
3. Quels sont les avantages et inconvÃ©nients du NFS ?
4. Comment sÃ©curiser davantage l'accÃ¨s au serveur NFS ?

### Exercice Final 4 : Application web avec Redis et persistance

CrÃ©ez un dÃ©ploiement complet avec :
- Un StatefulSet Redis avec PVC
- Un service pour exposer Redis
- Un deployment d'application web qui utilise Redis
- VÃ©rifiez la persistance des donnÃ©es Redis

**Solution :**

```bash
# 1. DÃ©ployer Redis avec persistance
kubectl apply -f - <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-config
data:
  redis.conf: |
    appendonly yes
    appendfsync everysec
    save 900 1
    save 300 10
    save 60 10000
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis
spec:
  serviceName: redis
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 999
        fsGroup: 999
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: redis
        image: redis:7-alpine
        command:
        - redis-server
        - /etc/redis/redis.conf
        ports:
        - containerPort: 6379
          name: redis
        volumeMounts:
        - name: data
          mountPath: /data
        - name: config
          mountPath: /etc/redis
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
      volumes:
      - name: config
        configMap:
          name: redis-config
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
        - ReadWriteOnce
      storageClassName: standard
      resources:
        requests:
          storage: 5Gi
---
apiVersion: v1
kind: Service
metadata:
  name: redis
spec:
  selector:
    app: redis
  ports:
  - port: 6379
    targetPort: 6379
  clusterIP: None
EOF

# 2. Attendre que Redis soit prÃªt
kubectl wait --for=condition=ready pod -l app=redis --timeout=120s

# 3. Tester Redis et insÃ©rer des donnÃ©es
kubectl exec -it redis-0 -- redis-cli SET mykey "Hello from Kubernetes Storage TP!"
kubectl exec -it redis-0 -- redis-cli GET mykey

# 4. VÃ©rifier la persistance: supprimer le pod
kubectl delete pod redis-0

# 5. Attendre la recrÃ©ation
kubectl wait --for=condition=ready pod redis-0 --timeout=120s

# 6. VÃ©rifier que les donnÃ©es sont toujours lÃ 
kubectl exec -it redis-0 -- redis-cli GET mykey

# 7. DÃ©ployer une application web qui utilise Redis
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
    spec:
      containers:
      - name: app
        image: redis:7-alpine
        command:
        - sh
        - -c
        - |
          while true; do
            COUNTER=\$(redis-cli -h redis.default.svc.cluster.local INCR page_views)
            echo "Page views: \$COUNTER"
            sleep 2
          done
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"
EOF

# 8. Observer les compteurs
kubectl logs -l app=web-app --tail=5

# 9. Nettoyage
kubectl delete deployment web-app
kubectl delete statefulset redis
kubectl delete svc redis
kubectl delete pvc data-redis-0
kubectl delete configmap redis-config
```

### Exercice Final 5 : Migration et backup de donnÃ©es

1. CrÃ©ez un pod avec un PVC
2. Ã‰crivez des donnÃ©es dans le volume
3. CrÃ©ez un snapshot (si disponible)
4. Simulez une catastrophe (suppression du pod et du PVC)
5. Restaurez depuis le snapshot
6. VÃ©rifiez que les donnÃ©es sont intactes

**Solution :**

```bash
# 1. CrÃ©er un PVC et un pod
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: app-data
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: standard
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: data-pod
spec:
  containers:
  - name: app
    image: busybox
    command: ['sh', '-c', 'sleep 3600']
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: app-data
EOF

# 2. Attendre et Ã©crire des donnÃ©es importantes
kubectl wait --for=condition=ready pod/data-pod --timeout=60s
kubectl exec data-pod -- sh -c 'echo "DonnÃ©es critiques - backup test" > /data/important.txt'
kubectl exec data-pod -- sh -c 'date >> /data/important.txt'
kubectl exec data-pod -- cat /data/important.txt

# 3. CrÃ©er un snapshot (si CSI driver supporte les snapshots)
# VÃ©rifier si les VolumeSnapshotClass existent
kubectl get volumesnapshotclass

# Si disponible:
kubectl apply -f - <<EOF
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: app-data-snapshot
spec:
  volumeSnapshotClassName: csi-hostpath-snapclass
  source:
    persistentVolumeClaimName: app-data
EOF

# Attendre que le snapshot soit prÃªt
sleep 10
kubectl get volumesnapshot app-data-snapshot

# 4. Sauvegarder manuellement si snapshots non disponibles
kubectl exec data-pod -- tar czf /data/backup.tar.gz /data/important.txt
kubectl cp data-pod:/data/backup.tar.gz ./backup.tar.gz

# 5. Simuler une catastrophe
kubectl delete pod data-pod
kubectl delete pvc app-data

# 6. Restaurer depuis le snapshot
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: app-data-restored
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: standard
  dataSource:
    name: app-data-snapshot
    kind: VolumeSnapshot
    apiGroup: snapshot.storage.k8s.io
  resources:
    requests:
      storage: 1Gi
EOF

# Ou restaurer manuellement
kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: app-data-restored
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: standard
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: data-pod-restored
spec:
  containers:
  - name: app
    image: busybox
    command: ['sh', '-c', 'sleep 3600']
    volumeMounts:
    - name: data
      mountPath: /data
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: app-data-restored
EOF

# Copier le backup
kubectl wait --for=condition=ready pod/data-pod-restored --timeout=60s
kubectl cp ./backup.tar.gz data-pod-restored:/data/backup.tar.gz
kubectl exec data-pod-restored -- tar xzf /data/backup.tar.gz -C /

# 7. VÃ©rifier les donnÃ©es restaurÃ©es
kubectl exec data-pod-restored -- cat /data/important.txt

# 8. Nettoyage
kubectl delete pod data-pod-restored
kubectl delete pvc app-data-restored
kubectl delete volumesnapshot app-data-snapshot 2>/dev/null || true
rm -f backup.tar.gz
```

## Partie 8 : Nettoyage

```bash
# Supprimer tous les pods, deployments et services
kubectl delete deployment mysql
kubectl delete service mysql
kubectl delete pod mysql-client
kubectl delete pod pod-with-pvc
kubectl delete pod emptydir-demo

# Supprimer les PVC
kubectl delete pvc mysql-pvc
kubectl delete pvc dynamic-pvc
kubectl delete pvc pvc-demo

# Supprimer les PV (si Retain)
kubectl delete pv pv-demo

# Supprimer les StorageClasses personnalisÃ©es
kubectl delete storageclass fast-storage

# Supprimer les secrets
kubectl delete secret mysql-secret

# VÃ©rifier que tout est nettoyÃ©
kubectl get all
kubectl get pvc
kubectl get pv
```

## RÃ©sumÃ©

Dans ce TP, vous avez appris Ã  :

- Utiliser diffÃ©rents types de volumes (emptyDir, hostPath, PVC)
- CrÃ©er et gÃ©rer des PersistentVolumes et PersistentVolumeClaims
- Utiliser le provisionnement dynamique avec StorageClasses
- DÃ©ployer une base de donnÃ©es avec persistance
- Appliquer les bonnes pratiques de gestion du stockage

### Concepts clÃ©s

- **Volume** : Abstraction de stockage
- **PV** : Ressource de stockage cluster-wide
- **PVC** : Demande de stockage par un utilisateur
- **StorageClass** : Classe de stockage avec provisionnement dynamique
- **Modes d'accÃ¨s** : RWO, ROX, RWX
- **Reclaim Policy** : Retain, Delete, Recycle

## Ressources complÃ©mentaires

### Documentation officielle
- [Volumes Kubernetes](https://kubernetes.io/docs/concepts/storage/volumes/)
- [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
- [Storage Classes](https://kubernetes.io/docs/concepts/storage/storage-classes/)
- [Volume Snapshots](https://kubernetes.io/docs/concepts/storage/volume-snapshots/)

### Tutoriels avancÃ©s
- [CSI Drivers](https://kubernetes-csi.github.io/docs/)
- [Rook (stockage distribuÃ©)](https://rook.io/)
- [Longhorn (stockage cloud-native)](https://longhorn.io/)

### Prochaines Ã©tapes

FÃ©licitations ! Vous maÃ®trisez maintenant la persistance des donnÃ©es dans Kubernetes.

Passez au **TP4** pour apprendre le monitoring et la gestion des logs.

## Questions de rÃ©vision

1. Quelle est la diffÃ©rence entre un Volume et un PersistentVolume ?
2. Quand utiliser emptyDir vs PVC ?
3. Qu'est-ce que le provisionnement dynamique ?
4. Quels sont les trois modes d'accÃ¨s disponibles ?
5. Que se passe-t-il avec une Reclaim Policy "Delete" ?
6. Pourquoi utiliser un Headless Service pour MySQL ?
7. Comment vÃ©rifier qu'un volume est correctement montÃ© dans un pod ?
8. Quelle est la diffÃ©rence entre requests et limits pour le stockage ?

## Solutions des questions

<details>
<summary>Cliquez pour voir les rÃ©ponses</summary>

1. Un Volume est liÃ© au cycle de vie d'un pod, un PV est une ressource cluster-wide indÃ©pendante
2. emptyDir pour donnÃ©es temporaires partagÃ©es entre conteneurs, PVC pour donnÃ©es persistantes
3. CrÃ©ation automatique de PV Ã  la demande via une StorageClass
4. ReadWriteOnce, ReadOnlyMany, ReadWriteMany
5. Le PV et les donnÃ©es sont supprimÃ©s automatiquement
6. Pour accÃ¨s direct aux pods sans load balancing
7. `kubectl describe pod <name>` et vÃ©rifier la section Mounts
8. Pour le stockage, requests = taille demandÃ©e, limits n'existe pas (la taille est fixe)

</details>

---

**DurÃ©e estimÃ©e du TP :** 4-5 heures
**Niveau :** IntermÃ©diaire

**Bon travail !**
