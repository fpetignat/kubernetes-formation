# TP10 - Projet de Synth√®se : Application TaskFlow avec Auto-scaling et Monitoring

## üéØ Objectifs du TP

Ce TP de synth√®se vous permet de mettre en pratique **toutes les notions importantes** vues dans les TPs pr√©c√©dents :

- ‚úÖ **Deployments** : D√©ploiement d'une stack applicative compl√®te multi-tiers
- ‚úÖ **HPA (HorizontalPodAutoscaler)** : Auto-scaling bas√© sur les m√©triques CPU/m√©moire
- ‚úÖ **initContainers** : Initialisation de base de donn√©es avec donn√©es de test
- ‚úÖ **Services** : ClusterIP, LoadBalancer pour l'exposition
- ‚úÖ **Volumes (PVC)** : Persistance des donn√©es (PostgreSQL, Prometheus)
- ‚úÖ **ConfigMaps/Secrets** : Configuration externalis√©e
- ‚úÖ **Monitoring** : Prometheus + Grafana pour observer le comportement
- ‚úÖ **Load Testing** : G√©n√©rateur de charge pour tester l'autoscaling
- ‚úÖ **RBAC** : ServiceAccounts pour Prometheus

√Ä la fin de ce TP, vous aurez d√©ploy√© une **application web compl√®te** avec auto-scaling et monitoring en temps r√©el.

**Dur√©e estim√©e :** 3-4 heures
**Niveau :** Synth√®se (tous les TPs pr√©c√©dents)

## üìã Pr√©requis

- Avoir compl√©t√© les TP1 √† TP9 (ou au minimum TP1, TP2, TP3, TP4)
- Cluster Kubernetes fonctionnel (**minikube** ou **kubeadm**)
- **Minimum 4 Go de RAM** disponibles pour le cluster
- kubectl install√© et configur√©
- Metrics Server install√© (pour HPA)

## üèóÔ∏è Architecture de l'application TaskFlow

### Vue d'ensemble

TaskFlow est une application web de gestion de t√¢ches (Todo List) avec les composants suivants :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        Utilisateurs                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ  LoadBalancer (SVC)  ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ               ‚îÇ               ‚îÇ
         ‚ñº               ‚ñº               ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇFrontend ‚îÇ    ‚îÇBackend  ‚îÇ    ‚îÇBackend  ‚îÇ  ‚óÑ‚îÄ‚îÄ HPA (auto-scaling)
    ‚îÇ (Nginx) ‚îÇ    ‚îÇ  API    ‚îÇ    ‚îÇ  API    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ              ‚îÇ              ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ              ‚îÇ              ‚îÇ
         ‚ñº              ‚ñº              ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Redis  ‚îÇ   ‚îÇPostgreSQL‚îÇ   ‚îÇPrometheus‚îÇ
    ‚îÇ (Cache) ‚îÇ   ‚îÇ   (DB)   ‚îÇ   ‚îÇ(Metrics)‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ              ‚îÇ
                       ‚ñº              ‚ñº
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ  PVC   ‚îÇ     ‚îÇ   PVC   ‚îÇ
                  ‚îÇ  (DB)  ‚îÇ     ‚îÇ(Metrics)‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚ñ≤
                       ‚îÇ
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇinitContainer‚îÇ
                ‚îÇ  (SQL Init) ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Composants de l'application

| Composant | Description | Type de Service | R√©plicas | Scaling |
|-----------|-------------|-----------------|----------|---------|
| **Frontend** | Interface web (HTML/CSS/JS) | LoadBalancer | 1 | Fixe |
| **Backend API** | API REST Flask (Python) | ClusterIP | 2-10 | **HPA activ√©** |
| **PostgreSQL** | Base de donn√©es | ClusterIP | 1 | Fixe |
| **Redis** | Cache en m√©moire | ClusterIP | 1 | Fixe |
| **Prometheus** | Collecte de m√©triques | ClusterIP | 1 | Fixe |
| **Grafana** | Visualisation | LoadBalancer | 1 | Fixe |
| **Load Generator** | G√©n√©rateur de charge | Job | - | On-demand |

### Flux de donn√©es

1. **Initialisation** :
   - L'**initContainer** de PostgreSQL cr√©e le sch√©ma de la base de donn√©es
   - Il charge **1000 t√¢ches de test** pour simuler une application en production

2. **Fonctionnement normal** :
   - Les utilisateurs acc√®dent au **Frontend** via LoadBalancer
   - Le Frontend envoie les requ√™tes √† l'**API Backend**
   - L'API interroge **PostgreSQL** pour les donn√©es
   - L'API utilise **Redis** pour mettre en cache les r√©sultats fr√©quents

3. **Auto-scaling** :
   - Le **HPA** surveille l'utilisation CPU/m√©moire des pods Backend
   - Quand la charge augmente, le HPA **scale automatiquement** de 2 √† 10 replicas
   - Quand la charge diminue, il **descale** progressivement

4. **Monitoring** :
   - **Prometheus** collecte les m√©triques des pods (CPU, m√©moire, requ√™tes)
   - **Grafana** affiche des dashboards en temps r√©el
   - Vous pouvez observer l'autoscaling en action

## üöÄ Partie 1 : Pr√©paration de l'environnement

### 1.1 V√©rifier Metrics Server

Le HPA n√©cessite Metrics Server pour obtenir les m√©triques CPU/m√©moire :

```bash
# V√©rifier si Metrics Server est install√©
kubectl get deployment metrics-server -n kube-system
```

**Si non install√© (minikube)** :
```bash
minikube addons enable metrics-server
```

**Si non install√© (kubeadm)** :
```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

Attendre que Metrics Server soit pr√™t :
```bash
kubectl wait --for=condition=available --timeout=300s deployment/metrics-server -n kube-system
```

V√©rifier que les m√©triques sont disponibles :
```bash
kubectl top nodes
kubectl top pods -A
```

### 1.2 Cr√©er le namespace du projet

```bash
kubectl create namespace taskflow
kubectl config set-context --current --namespace=taskflow
```

### 1.3 V√©rifier les ressources disponibles

```bash
# V√©rifier la RAM disponible
kubectl top nodes

# Minimum recommand√© : 4 Go de RAM libre
```

### 1.4 Construire l'image Docker Backend (REQUIS)

**IMPORTANT** : Le deployment backend utilise maintenant une image Docker construite localement avec toutes les d√©pendances pr√©-install√©es.

**Avant de d√©ployer l'application**, vous devez construire l'image :

```bash
cd tp10

# Construire l'image backend avec le script automatis√©
./build-image.sh
```

Le script `build-image.sh` effectue les op√©rations suivantes :
1. ‚úÖ D√©tecte automatiquement si Minikube est disponible et d√©marr√©
2. ‚úÖ Configure l'environnement Docker appropri√© (Minikube ou Docker local)
3. ‚úÖ Construit l'image `taskflow-backend:latest` avec le Dockerfile
4. ‚úÖ Rend l'image disponible directement dans Minikube

**V√©rifier que l'image est construite** :
```bash
# Configurer le shell pour utiliser Docker de Minikube
eval $(minikube docker-env)

# Lister les images disponibles
docker images | grep taskflow-backend
```

**Avantages de cette approche** :
- ‚úÖ **D√©marrage instantan√©** des pods (d√©pendances d√©j√† install√©es)
- ‚úÖ **Pas d'installation √† la vol√©e** : pas de `pip install` au d√©marrage
- ‚úÖ **Image optimis√©e** : ~250 MB avec toutes les d√©pendances
- ‚úÖ **S√©curit√© renforc√©e** : utilisateur non-root (UID 1000) pr√©-configur√©
- ‚úÖ **Conforme aux bonnes pratiques de production**

**Structure des fichiers** :
```
tp10/
‚îú‚îÄ‚îÄ Dockerfile                   # D√©finition de l'image backend
‚îú‚îÄ‚îÄ app.py                       # Code Python de l'API backend
‚îú‚îÄ‚îÄ requirements.txt             # D√©pendances Python
‚îú‚îÄ‚îÄ build-image.sh               # Script de build automatis√©
‚îî‚îÄ‚îÄ 09-backend-deployment.yaml   # Utilise taskflow-backend:latest
```

**Configuration du Deployment** :
Le fichier `09-backend-deployment.yaml` est configur√© pour utiliser l'image locale :
```yaml
containers:
- name: api
  image: taskflow-backend:latest  # Image construite localement
  imagePullPolicy: Never           # Ne pas chercher sur Docker Hub
```

## üì¶ Partie 2 : D√©ploiement de la base de donn√©es PostgreSQL avec initContainer

### 2.1 Comprendre l'objectif

Nous allons d√©ployer PostgreSQL avec un **initContainer** qui :
- Cr√©e le sch√©ma de la base de donn√©es (table `tasks`)
- Ins√®re **1000 t√¢ches de test** pour simuler une application en production
- S'ex√©cute **avant** le d√©marrage du conteneur principal PostgreSQL

Ceci illustre un cas d'usage r√©el : **initialiser une base de donn√©es** avant le d√©marrage de l'application.

### 2.2 ConfigMap pour le script d'initialisation

Cr√©er `01-postgres-init-script.yaml` :

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-init-script
  namespace: taskflow
data:
  init.sql: |
    -- Cr√©er la table tasks
    CREATE TABLE IF NOT EXISTS tasks (
        id SERIAL PRIMARY KEY,
        title VARCHAR(255) NOT NULL,
        description TEXT,
        completed BOOLEAN DEFAULT FALSE,
        priority VARCHAR(20) DEFAULT 'medium',
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );

    -- Cr√©er un index pour les performances
    CREATE INDEX IF NOT EXISTS idx_tasks_completed ON tasks(completed);
    CREATE INDEX IF NOT EXISTS idx_tasks_priority ON tasks(priority);

    -- G√©n√©rer 1000 t√¢ches de test
    INSERT INTO tasks (title, description, completed, priority)
    SELECT
        'Task ' || generate_series,
        'Description for task ' || generate_series,
        (random() > 0.7)::boolean,  -- 30% de t√¢ches compl√©t√©es
        CASE
            WHEN random() < 0.2 THEN 'low'
            WHEN random() < 0.7 THEN 'medium'
            ELSE 'high'
        END
    FROM generate_series(1, 1000);

    -- Afficher les statistiques
    SELECT
        COUNT(*) as total_tasks,
        SUM(CASE WHEN completed THEN 1 ELSE 0 END) as completed_tasks,
        SUM(CASE WHEN NOT completed THEN 1 ELSE 0 END) as pending_tasks
    FROM tasks;
```

Appliquer :
```bash
kubectl apply -f 01-postgres-init-script.yaml
```

### 2.3 Secret pour les credentials PostgreSQL

Cr√©er `02-postgres-secret.yaml` :

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: postgres-secret
  namespace: taskflow
type: Opaque
stringData:
  POSTGRES_USER: taskflow
  POSTGRES_PASSWORD: taskflow2024
  POSTGRES_DB: taskflow_db
```

Appliquer :
```bash
kubectl apply -f 02-postgres-secret.yaml
```

### 2.4 PersistentVolumeClaim pour PostgreSQL

Cr√©er `03-postgres-pvc.yaml` :

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
  namespace: taskflow
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
  storageClassName: standard  # Ajuster selon votre environnement
```

Appliquer :
```bash
kubectl apply -f 03-postgres-pvc.yaml
```

### 2.5 Deployment PostgreSQL avec initContainer

Cr√©er `04-postgres-deployment.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
  namespace: taskflow
  labels:
    app: postgres
    tier: database
spec:
  replicas: 1  # IMPORTANT : Une seule instance pour √©viter la corruption de donn√©es
  selector:
    matchLabels:
      app: postgres
  strategy:
    type: Recreate  # IMPORTANT : Arr√™ter l'ancien pod avant de d√©marrer le nouveau
  template:
    metadata:
      labels:
        app: postgres
        tier: database
    spec:
      # initContainer : s'ex√©cute AVANT le conteneur principal
      initContainers:
      - name: init-db-schema
        image: postgres:16-alpine
        command:
        - sh
        - -c
        - |
          echo "Waiting for PostgreSQL to be ready..."
          # Attendre que PostgreSQL soit pr√™t dans le conteneur principal
          # (ce script s'ex√©cute en premier mais le volume est partag√©)
          sleep 10
          echo "InitContainer completed successfully"
        volumeMounts:
        - name: init-script
          mountPath: /docker-entrypoint-initdb.d
        envFrom:
        - secretRef:
            name: postgres-secret

      # Conteneur principal PostgreSQL
      containers:
      - name: postgres
        image: postgres:16-alpine
        ports:
        - containerPort: 5432
          name: postgres
        envFrom:
        - secretRef:
            name: postgres-secret
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
          subPath: postgres  # √âviter les probl√®mes de permissions
        - name: init-script
          mountPath: /docker-entrypoint-initdb.d
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          exec:
            command:
            - pg_isready
            - -U
            - taskflow
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - pg_isready
            - -U
            - taskflow
          initialDelaySeconds: 5
          periodSeconds: 5

      volumes:
      - name: postgres-storage
        persistentVolumeClaim:
          claimName: postgres-pvc
      - name: init-script
        configMap:
          name: postgres-init-script
```

**Points cl√©s √† comprendre** :
- Le **initContainer** `init-db-schema` s'ex√©cute en premier
- Il monte le m√™me script SQL que le conteneur principal
- PostgreSQL ex√©cute automatiquement les scripts dans `/docker-entrypoint-initdb.d/`
- Les **1000 t√¢ches** sont cr√©√©es au premier d√©marrage
- Le **PVC** garantit la persistance des donn√©es

**‚ö†Ô∏è Important sur `replicas: 1` et `strategy: Recreate`** :

**Pourquoi une seule replica ?**
- PostgreSQL est une base de donn√©es **stateful** (avec √©tat)
- Plusieurs replicas √©crivant sur le **m√™me PVC** causeraient une **corruption de donn√©es**
- PostgreSQL ne supporte pas nativement l'√©criture multi-master
- Pour la haute disponibilit√©, il faut configurer une r√©plication PostgreSQL complexe (streaming replication, patroni, etc.)

**Pourquoi `strategy: Recreate` ?**
- `Recreate` **arr√™te** l'ancien pod **avant** de d√©marrer le nouveau
- √âvite que 2 pods PostgreSQL tentent d'acc√©der au m√™me PVC simultan√©ment
- Garantit qu'un seul pod √©crit dans la base √† la fois
- Alternative : `RollingUpdate` causerait des erreurs car le nouveau pod ne pourrait pas d√©marrer tant que l'ancien utilise le volume

**Pour la production** :
- ‚úÖ PostgreSQL en `replicas: 1` avec PVC pour un TP/dev
- ‚úÖ Pour la production : utiliser un **StatefulSet** avec r√©plication PostgreSQL
- ‚úÖ Ou utiliser un service manag√© (AWS RDS, Google Cloud SQL, Azure Database)
- ‚ùå Ne JAMAIS mettre `replicas: 2+` avec un Deployment + PVC unique

Appliquer :
```bash
kubectl apply -f 04-postgres-deployment.yaml
```

### 2.6 Service PostgreSQL

Cr√©er `05-postgres-service.yaml` :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: taskflow
  labels:
    app: postgres
spec:
  type: ClusterIP
  ports:
  - port: 5432
    targetPort: 5432
    protocol: TCP
    name: postgres
  selector:
    app: postgres
```

Appliquer :
```bash
kubectl apply -f 05-postgres-service.yaml
```

### 2.7 V√©rifier le d√©ploiement PostgreSQL

```bash
# Voir le d√©ploiement
kubectl get deployment postgres

# Voir les pods (y compris l'initContainer)
kubectl get pods -l app=postgres

# Voir les logs de l'initContainer
kubectl logs -l app=postgres -c init-db-schema

# Voir les logs du conteneur principal
kubectl logs -l app=postgres -c postgres

# Se connecter √† PostgreSQL et v√©rifier les donn√©es
kubectl exec -it deployment/postgres -- psql -U taskflow -d taskflow_db -c "SELECT COUNT(*) FROM tasks;"
```

Vous devriez voir **1000 t√¢ches** dans la base de donn√©es !

## üì¶ Partie 3 : D√©ploiement de Redis (Cache)

### 3.1 Deployment Redis

Cr√©er `06-redis-deployment.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: taskflow
  labels:
    app: redis
    tier: cache
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
        tier: cache
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
          name: redis
        command:
        - redis-server
        - --maxmemory
        - "128mb"
        - --maxmemory-policy
        - "allkeys-lru"
        resources:
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "200m"
        livenessProbe:
          tcpSocket:
            port: 6379
          initialDelaySeconds: 15
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - redis-cli
            - ping
          initialDelaySeconds: 5
          periodSeconds: 5
```

Appliquer :
```bash
kubectl apply -f 06-redis-deployment.yaml
```

### 3.2 Service Redis

Cr√©er `07-redis-service.yaml` :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: redis
  namespace: taskflow
  labels:
    app: redis
spec:
  type: ClusterIP
  ports:
  - port: 6379
    targetPort: 6379
    protocol: TCP
    name: redis
  selector:
    app: redis
```

Appliquer :
```bash
kubectl apply -f 07-redis-service.yaml
```

## üîß Partie 4 : Backend API avec HPA (Auto-scaling)

### 4.1 ConfigMap pour la configuration API

Cr√©er `08-backend-config.yaml` :

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: backend-config
  namespace: taskflow
data:
  DATABASE_HOST: postgres
  DATABASE_PORT: "5432"
  DATABASE_NAME: taskflow_db
  REDIS_HOST: redis
  REDIS_PORT: "6379"
  CACHE_TTL: "300"
  LOG_LEVEL: "INFO"
```

Appliquer :
```bash
kubectl apply -f 08-backend-config.yaml
```

### 4.2 Deployment Backend API

Cr√©er `09-backend-deployment.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-api
  namespace: taskflow
  labels:
    app: backend-api
    tier: application
spec:
  replicas: 2  # Nombre initial (HPA va ajuster)
  selector:
    matchLabels:
      app: backend-api
  template:
    metadata:
      labels:
        app: backend-api
        tier: application
    spec:
      containers:
      - name: api
        image: python:3.11-slim  # Image de base (ou taskflow-backend-api:latest si construite localement)
        ports:
        - containerPort: 5000
          name: http
        env:
        - name: DATABASE_USER
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: POSTGRES_USER
        - name: DATABASE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: POSTGRES_PASSWORD
        envFrom:
        - configMapRef:
            name: backend-config
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"  # Important pour HPA
          limits:
            memory: "256Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 5000
          initialDelaySeconds: 10
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 5000
          initialDelaySeconds: 5
          periodSeconds: 5
```

**Note** : Les `requests.cpu` et `requests.memory` sont **essentiels** pour le HPA.

Appliquer :
```bash
kubectl apply -f 09-backend-deployment.yaml
```

### 4.3 Service Backend API

Cr√©er `10-backend-service.yaml` :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: backend-api
  namespace: taskflow
  labels:
    app: backend-api
spec:
  type: ClusterIP
  ports:
  - port: 5000
    targetPort: 5000
    protocol: TCP
    name: http
  selector:
    app: backend-api
```

Appliquer :
```bash
kubectl apply -f 10-backend-service.yaml
```

### 4.4 HorizontalPodAutoscaler (HPA)

**C'est ici que la magie op√®re !** Le HPA va surveiller l'utilisation CPU/m√©moire et scaler automatiquement.

Cr√©er `11-backend-hpa.yaml` :

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: backend-api-hpa
  namespace: taskflow
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: backend-api
  minReplicas: 2   # Minimum de pods
  maxReplicas: 10  # Maximum de pods
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50  # Scale quand CPU > 50%
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 70  # Scale quand m√©moire > 70%
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 60  # Attendre 60s avant de descaler
      policies:
      - type: Percent
        value: 50  # Descaler max 50% des pods √† la fois
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0  # Scaler imm√©diatement
      policies:
      - type: Percent
        value: 100  # Doubler le nombre de pods si n√©cessaire
        periodSeconds: 15
      - type: Pods
        value: 4  # Ajouter max 4 pods √† la fois
        periodSeconds: 15
      selectPolicy: Max  # Choisir la politique la plus agressive
```

**Explication des param√®tres** :
- **minReplicas: 2** : Jamais moins de 2 pods (haute disponibilit√©)
- **maxReplicas: 10** : Maximum 10 pods (limite de ressources)
- **CPU 50%** : Si l'utilisation CPU moyenne d√©passe 50%, scale up
- **Memory 70%** : Si l'utilisation m√©moire d√©passe 70%, scale up
- **scaleUp** : R√©action rapide (15 secondes, max 4 pods √† la fois)
- **scaleDown** : R√©action lente (60 secondes, max 50% √† la fois)

Appliquer :
```bash
kubectl apply -f 11-backend-hpa.yaml
```

V√©rifier le HPA :
```bash
# Voir l'√©tat du HPA
kubectl get hpa backend-api-hpa

# Observer en temps r√©el (watch mode)
kubectl get hpa backend-api-hpa -w
```

## üåê Partie 5 : Frontend et Exposition

**üìå Note importante sur l'architecture Frontend/Backend** :

Le frontend est une application HTML/JavaScript statique servie par Nginx. Lorsqu'un utilisateur acc√®de au frontend depuis son navigateur, le JavaScript s'ex√©cute **c√¥t√© client** (dans le navigateur).

**Probl√®me** : Les URLs internes Kubernetes (comme `http://backend-api.taskflow.svc.cluster.local:5000`) ne sont pas accessibles depuis le navigateur du client car :
- Le navigateur ne peut pas r√©soudre les DNS `.svc.cluster.local` (internes √† Kubernetes)
- Le navigateur ne peut pas atteindre les IPs internes du cluster

**Solution** : Nous configurons **Nginx comme reverse proxy**. Le frontend utilise une URL relative (`/api`) et Nginx redirige les requ√™tes vers le service backend interne.

```
Navigateur ‚Üí /api ‚Üí Nginx (reverse proxy) ‚Üí http://backend-api:5000
```

### 5.1 Configuration Nginx avec Reverse Proxy

Cr√©er `12-frontend-nginx-config.yaml` :

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: frontend-nginx-config
  namespace: taskflow
  labels:
    app: frontend
data:
  nginx.conf: |
    user nginx;
    worker_processes auto;
    pid /var/run/nginx.pid;

    events {
        worker_connections 1024;
    }

    http {
        include /etc/nginx/mime.types;
        default_type application/octet-stream;

        access_log /dev/stdout;
        error_log /dev/stderr warn;

        sendfile on;
        tcp_nopush on;
        keepalive_timeout 65;

        gzip on;
        gzip_vary on;
        gzip_min_length 1000;
        gzip_types text/plain text/css application/json application/javascript text/xml;

        server {
            listen 80;
            server_name _;

            root /usr/share/nginx/html;
            index index.html;

            # Frontend - Servir l'application HTML/JS
            location / {
                try_files $uri $uri/ /index.html;
                add_header X-Content-Type-Options "nosniff" always;
                add_header X-Frame-Options "SAMEORIGIN" always;
                add_header X-XSS-Protection "1; mode=block" always;
            }

            # API Backend - Reverse proxy vers le service backend-api
            location /api/ {
                # Supprimer le pr√©fixe /api avant de transf√©rer
                rewrite ^/api/(.*) /$1 break;

                # Proxy vers le service Kubernetes backend-api
                proxy_pass http://backend-api:5000;

                # Headers de proxy standards
                proxy_set_header Host $host;
                proxy_set_header X-Real-IP $remote_addr;
                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
                proxy_set_header X-Forwarded-Proto $scheme;

                # Timeouts
                proxy_connect_timeout 30s;
                proxy_send_timeout 30s;
                proxy_read_timeout 30s;
            }

            # Health check endpoint
            location /health {
                access_log off;
                return 200 "healthy\n";
                add_header Content-Type text/plain;
            }
        }
    }
```

**Explication de la configuration** :
- `location /` : Sert les fichiers statiques du frontend (HTML/CSS/JS)
- `location /api/` : Reverse proxy vers le backend
  - `rewrite ^/api/(.*) /$1 break` : Supprime le pr√©fixe `/api` (ex: `/api/tasks` ‚Üí `/tasks`)
  - `proxy_pass http://backend-api:5000` : Redirige vers le service backend interne
  - Headers de proxy pour pr√©server l'information du client

Appliquer :
```bash
kubectl apply -f 12-frontend-nginx-config.yaml
```

### 5.2 ConfigMap pour le Frontend HTML

Cr√©er `12-frontend-config.yaml` :

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: frontend-html
  namespace: taskflow
data:
  index.html: |
    <!DOCTYPE html>
    <html lang="fr">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>TaskFlow - Gestion de T√¢ches</title>
        <style>
            * { margin: 0; padding: 0; box-sizing: border-box; }
            body {
                font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                min-height: 100vh;
                padding: 20px;
            }
            .container {
                max-width: 1200px;
                margin: 0 auto;
                background: white;
                border-radius: 15px;
                box-shadow: 0 20px 60px rgba(0,0,0,0.3);
                overflow: hidden;
            }
            header {
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white;
                padding: 30px;
                text-align: center;
            }
            h1 { font-size: 2.5em; margin-bottom: 10px; }
            .stats {
                display: flex;
                justify-content: space-around;
                padding: 20px;
                background: #f8f9fa;
                border-bottom: 1px solid #dee2e6;
            }
            .stat-box {
                text-align: center;
                padding: 15px;
            }
            .stat-number {
                font-size: 2em;
                font-weight: bold;
                color: #667eea;
            }
            .stat-label {
                color: #6c757d;
                margin-top: 5px;
            }
            .tasks {
                padding: 20px;
                max-height: 600px;
                overflow-y: auto;
            }
            .task {
                background: white;
                border: 1px solid #dee2e6;
                border-radius: 8px;
                padding: 15px;
                margin-bottom: 10px;
                display: flex;
                justify-content: space-between;
                align-items: center;
                transition: all 0.3s;
            }
            .task:hover {
                box-shadow: 0 4px 12px rgba(0,0,0,0.1);
                transform: translateY(-2px);
            }
            .task.completed {
                opacity: 0.6;
                text-decoration: line-through;
            }
            .priority {
                display: inline-block;
                padding: 4px 12px;
                border-radius: 12px;
                font-size: 0.85em;
                font-weight: bold;
                margin-left: 10px;
            }
            .priority-high { background: #dc3545; color: white; }
            .priority-medium { background: #ffc107; color: black; }
            .priority-low { background: #28a745; color: white; }
            .loading {
                text-align: center;
                padding: 40px;
                font-size: 1.2em;
                color: #6c757d;
            }
            .error {
                background: #f8d7da;
                color: #721c24;
                padding: 20px;
                margin: 20px;
                border-radius: 8px;
                border: 1px solid #f5c6cb;
            }
            .controls {
                padding: 20px;
                background: #f8f9fa;
                border-top: 1px solid #dee2e6;
                text-align: center;
            }
            button {
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white;
                border: none;
                padding: 12px 30px;
                border-radius: 25px;
                font-size: 1em;
                cursor: pointer;
                margin: 5px;
                transition: transform 0.2s;
            }
            button:hover {
                transform: scale(1.05);
            }
            button:active {
                transform: scale(0.95);
            }
        </style>
    </head>
    <body>
        <div class="container">
            <header>
                <h1>üìã TaskFlow</h1>
                <p>Projet de Synth√®se Kubernetes - Auto-scaling et Monitoring</p>
            </header>

            <div class="stats" id="stats">
                <div class="stat-box">
                    <div class="stat-number" id="totalTasks">-</div>
                    <div class="stat-label">Total T√¢ches</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number" id="completedTasks">-</div>
                    <div class="stat-label">Compl√©t√©es</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number" id="pendingTasks">-</div>
                    <div class="stat-label">En cours</div>
                </div>
                <div class="stat-box">
                    <div class="stat-number" id="apiPods">-</div>
                    <div class="stat-label">Pods API (HPA)</div>
                </div>
            </div>

            <div class="controls">
                <button onclick="loadTasks()">üîÑ Rafra√Æchir</button>
                <button onclick="loadTasks('high')">üî¥ Priorit√© Haute</button>
                <button onclick="loadTasks('medium')">üü° Priorit√© Moyenne</button>
                <button onclick="loadTasks('low')">üü¢ Priorit√© Basse</button>
            </div>

            <div class="tasks" id="tasksList">
                <div class="loading">Chargement des t√¢ches...</div>
            </div>
        </div>

        <script>
            // Utiliser une URL relative car Nginx proxie /api vers le backend
            const API_URL = '/api';

            async function loadTasks(priority = null) {
                const tasksList = document.getElementById('tasksList');
                tasksList.innerHTML = '<div class="loading">Chargement...</div>';

                try {
                    const url = priority ? `${API_URL}/tasks?priority=${priority}` : `${API_URL}/tasks`;
                    const response = await fetch(url);

                    if (!response.ok) {
                        throw new Error(`HTTP ${response.status}`);
                    }

                    const data = await response.json();
                    displayTasks(data.tasks);
                    updateStats(data.stats);
                } catch (error) {
                    tasksList.innerHTML = `
                        <div class="error">
                            <strong>Erreur de connexion √† l'API</strong><br>
                            ${error.message}<br>
                            <small>V√©rifiez que le backend est d√©ploy√© et accessible</small>
                        </div>
                    `;
                }
            }

            function displayTasks(tasks) {
                const tasksList = document.getElementById('tasksList');

                if (!tasks || tasks.length === 0) {
                    tasksList.innerHTML = '<div class="loading">Aucune t√¢che trouv√©e</div>';
                    return;
                }

                tasksList.innerHTML = tasks.map(task => `
                    <div class="task ${task.completed ? 'completed' : ''}">
                        <div>
                            <strong>${task.title}</strong>
                            <span class="priority priority-${task.priority}">${task.priority}</span>
                            <div style="color: #6c757d; margin-top: 5px; font-size: 0.9em;">
                                ${task.description}
                            </div>
                        </div>
                        <div>
                            ${task.completed ? '‚úÖ' : '‚è≥'}
                        </div>
                    </div>
                `).join('');
            }

            function updateStats(stats) {
                if (stats) {
                    document.getElementById('totalTasks').textContent = stats.total || 0;
                    document.getElementById('completedTasks').textContent = stats.completed || 0;
                    document.getElementById('pendingTasks').textContent = stats.pending || 0;
                }

                // Simuler le nombre de pods (en production, r√©cup√©rer via une API)
                document.getElementById('apiPods').textContent = '~';
            }

            // Charger les t√¢ches au d√©marrage
            loadTasks();

            // Auto-refresh toutes les 30 secondes
            setInterval(() => loadTasks(), 30000);
        </script>
    </body>
    </html>
```

Appliquer :
```bash
kubectl apply -f 12-frontend-config.yaml
```

### 5.3 Deployment Frontend

Cr√©er `13-frontend-deployment.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: taskflow
  labels:
    app: frontend
    tier: presentation
spec:
  replicas: 1
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
        tier: presentation
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 101
        fsGroup: 101
        seccompProfile:
          type: RuntimeDefault

      containers:
      - name: nginx
        image: nginx:1.25-alpine
        ports:
        - containerPort: 80
          name: http
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 101
          capabilities:
            drop:
            - ALL
        volumeMounts:
        - name: html
          mountPath: /usr/share/nginx/html
        - name: nginx-config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
        - name: cache
          mountPath: /var/cache/nginx
        - name: run
          mountPath: /var/run
        - name: tmp
          mountPath: /tmp
        resources:
          requests:
            memory: "32Mi"
            cpu: "50m"
          limits:
            memory: "64Mi"
            cpu: "100m"

      volumes:
      - name: html
        configMap:
          name: frontend-html
      - name: nginx-config
        configMap:
          name: frontend-nginx-config
      - name: cache
        emptyDir: {}
      - name: run
        emptyDir: {}
      - name: tmp
        emptyDir: {}
```

**Points importants** :
- Le volume `nginx-config` monte la configuration Nginx personnalis√©e avec le reverse proxy
- Les volumes `emptyDir` sont n√©cessaires car `readOnlyRootFilesystem: true` est activ√© pour la s√©curit√©
- Le securityContext suit les meilleures pratiques Kubernetes (voir `.claude/SECURITY.md`)

Appliquer :
```bash
kubectl apply -f 13-frontend-deployment.yaml
```

### 5.4 Service Frontend (LoadBalancer)

Cr√©er `14-frontend-service.yaml` :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend
  namespace: taskflow
  labels:
    app: frontend
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 80
    protocol: TCP
    name: http
  selector:
    app: frontend
```

Appliquer :
```bash
kubectl apply -f 14-frontend-service.yaml
```

Obtenir l'URL du frontend :
```bash
# Minikube
minikube service frontend -n taskflow --url

# Kubeadm (NodePort)
kubectl get svc frontend -n taskflow
```

## üìä Partie 6 : Monitoring avec Prometheus et Grafana

### 6.1 D√©ployer Prometheus

Nous allons utiliser une configuration simplifi√©e de Prometheus pour ce TP.

Cr√©er `15-prometheus-config.yaml` :

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: taskflow
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s

    scrape_configs:
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
        - role: pod
          namespaces:
            names:
            - taskflow
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_label_app]
          action: keep
          regex: backend-api|postgres|redis
        - source_labels: [__meta_kubernetes_pod_name]
          target_label: pod
        - source_labels: [__meta_kubernetes_pod_label_app]
          target_label: app
```

Appliquer :
```bash
kubectl apply -f 15-prometheus-config.yaml
```

### 6.2 RBAC pour Prometheus

Cr√©er `16-prometheus-rbac.yaml` :

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: taskflow
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: taskflow
```

Appliquer :
```bash
kubectl apply -f 16-prometheus-rbac.yaml
```

### 6.3 PVC pour Prometheus

Cr√©er `17-prometheus-pvc.yaml` :

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: prometheus-pvc
  namespace: taskflow
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: standard
```

Appliquer :
```bash
kubectl apply -f 17-prometheus-pvc.yaml
```

### 6.4 Deployment Prometheus

Cr√©er `18-prometheus-deployment.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: taskflow
  labels:
    app: prometheus
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      serviceAccountName: prometheus
      containers:
      - name: prometheus
        image: prom/prometheus:v2.48.0
        args:
        - '--config.file=/etc/prometheus/prometheus.yml'
        - '--storage.tsdb.path=/prometheus'
        - '--storage.tsdb.retention.time=7d'
        ports:
        - containerPort: 9090
          name: http
        volumeMounts:
        - name: config
          mountPath: /etc/prometheus
        - name: storage
          mountPath: /prometheus
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
      volumes:
      - name: config
        configMap:
          name: prometheus-config
      - name: storage
        persistentVolumeClaim:
          claimName: prometheus-pvc
```

Appliquer :
```bash
kubectl apply -f 18-prometheus-deployment.yaml
```

### 6.5 Service Prometheus

Cr√©er `19-prometheus-service.yaml` :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: taskflow
  labels:
    app: prometheus
spec:
  type: ClusterIP
  ports:
  - port: 9090
    targetPort: 9090
    protocol: TCP
    name: http
  selector:
    app: prometheus
```

Appliquer :
```bash
kubectl apply -f 19-prometheus-service.yaml
```

### 6.6 D√©ployer Grafana

Cr√©er `20-grafana-deployment.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: taskflow
  labels:
    app: grafana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana
        image: grafana/grafana:10.2.0
        ports:
        - containerPort: 3000
          name: http
        env:
        - name: GF_SECURITY_ADMIN_USER
          value: admin
        - name: GF_SECURITY_ADMIN_PASSWORD
          value: admin2024
        - name: GF_SERVER_ROOT_URL
          value: "%(protocol)s://%(domain)s:%(http_port)s/"
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"
        volumeMounts:
        - name: grafana-storage
          mountPath: /var/lib/grafana
      volumes:
      - name: grafana-storage
        emptyDir: {}
```

Appliquer :
```bash
kubectl apply -f 20-grafana-deployment.yaml
```

### 6.7 Service Grafana (LoadBalancer)

Cr√©er `21-grafana-service.yaml` :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: taskflow
  labels:
    app: grafana
spec:
  type: LoadBalancer
  ports:
  - port: 3000
    targetPort: 3000
    protocol: TCP
    name: http
  selector:
    app: grafana
```

Appliquer :
```bash
kubectl apply -f 21-grafana-service.yaml
```

Obtenir l'URL de Grafana :
```bash
# Minikube
minikube service grafana -n taskflow --url

# Kubeadm
kubectl get svc grafana -n taskflow
```

**Credentials par d√©faut** :
- Username: `admin`
- Password: `admin2024`

## üöÄ Partie 7 : Load Generator (G√©n√©rateur de Charge)

### 7.1 Comprendre l'objectif

Le Load Generator va **simuler du trafic** vers l'API Backend pour :
- Augmenter l'utilisation CPU/m√©moire des pods
- **D√©clencher l'autoscaling** du HPA
- Observer le comportement en temps r√©el dans Grafana

### 7.2 Job Load Generator

Cr√©er `22-load-generator.yaml` :

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: load-generator
  namespace: taskflow
spec:
  parallelism: 5  # 5 pods en parall√®le pour g√©n√©rer de la charge
  completions: 5
  template:
    metadata:
      labels:
        app: load-generator
    spec:
      restartPolicy: Never
      containers:
      - name: load-generator
        image: busybox:1.36
        command:
        - /bin/sh
        - -c
        - |
          echo "Starting load generator..."
          API_URL="http://backend-api.taskflow.svc.cluster.local:5000"

          # Boucle infinie de requ√™tes
          while true; do
            # GET /tasks
            wget -q -O- $API_URL/tasks > /dev/null 2>&1

            # GET /tasks?priority=high
            wget -q -O- $API_URL/tasks?priority=high > /dev/null 2>&1

            # GET /tasks?completed=false
            wget -q -O- $API_URL/tasks?completed=false > /dev/null 2>&1

            # GET /stats
            wget -q -O- $API_URL/stats > /dev/null 2>&1

            # Petite pause pour ne pas surcharger imm√©diatement
            sleep 0.1
          done
        resources:
          requests:
            memory: "32Mi"
            cpu: "100m"
          limits:
            memory: "64Mi"
            cpu: "200m"
```

**Ne PAS appliquer tout de suite** ! Nous allons d'abord tout v√©rifier.

## ‚úÖ Partie 8 : V√©rification et Tests

### 8.1 V√©rifier tous les composants

```bash
# Voir tous les d√©ploiements
kubectl get deployments -n taskflow

# Voir tous les pods
kubectl get pods -n taskflow

# Voir tous les services
kubectl get svc -n taskflow

# Voir le HPA
kubectl get hpa -n taskflow

# Voir les PVC
kubectl get pvc -n taskflow
```

Tous les pods doivent √™tre en √©tat **Running** :
- `postgres-xxx`
- `redis-xxx`
- `backend-api-xxx` (2 replicas initialement)
- `frontend-xxx`
- `prometheus-xxx`
- `grafana-xxx`

### 8.2 Tester l'API Backend

```bash
# Port-forward pour tester localement
kubectl port-forward -n taskflow svc/backend-api 5000:5000 &

# Tester l'API
curl http://localhost:5000/health
curl http://localhost:5000/tasks | jq '.tasks | length'
curl http://localhost:5000/stats

# Arr√™ter le port-forward
pkill -f "kubectl.*port-forward.*backend-api"
```

Vous devriez voir **1000 t√¢ches** dans la base de donn√©es.

### 8.3 Acc√©der au Frontend

```bash
# Minikube
minikube service frontend -n taskflow

# Kubeadm
kubectl get svc frontend -n taskflow
# Puis naviguer vers http://<NODE-IP>:<NODE-PORT>
```

Vous devriez voir l'interface web avec les 1000 t√¢ches.

### 8.4 Configurer Grafana

1. Acc√©der √† Grafana :
```bash
minikube service grafana -n taskflow
# Ou kubectl port-forward svc/grafana 3000:3000 -n taskflow
```

2. Se connecter :
   - Username: `admin`
   - Password: `admin2024`

3. Ajouter Prometheus comme Data Source :
   - Aller dans **Configuration** ‚Üí **Data Sources**
   - Cliquer **Add data source**
   - S√©lectionner **Prometheus**
   - URL: `http://prometheus.taskflow.svc.cluster.local:9090`
   - Cliquer **Save & Test**

4. Cr√©er un dashboard :
   - Aller dans **Dashboards** ‚Üí **New** ‚Üí **New Dashboard**
   - Ajouter des panels pour :
     - CPU usage des pods backend
     - Memory usage des pods backend
     - Nombre de replicas du deployment backend-api
     - Requ√™tes par seconde

### 8.5 Observer le HPA (avant charge)

```bash
# Voir l'√©tat actuel du HPA
kubectl get hpa backend-api-hpa -n taskflow

# Devrait afficher quelque chose comme :
# NAME               REFERENCE                TARGETS         MINPODS   MAXPODS   REPLICAS
# backend-api-hpa    Deployment/backend-api   5%/50%, 12%/70%   2         10        2
```

Les 2 m√©triques affich√©es sont :
- `5%/50%` : CPU actuel / cible (5% sur 50%)
- `12%/70%` : Memory actuel / cible (12% sur 70%)

## üî• Partie 9 : Test de l'Auto-scaling

### 9.1 Lancer le Load Generator

```bash
# D√©ployer le g√©n√©rateur de charge
kubectl apply -f 22-load-generator.yaml

# V√©rifier qu'il tourne
kubectl get jobs -n taskflow
kubectl get pods -n taskflow -l app=load-generator
```

Vous devriez voir **5 pods** de load-generator en √©tat Running.

### 9.2 Observer l'autoscaling en temps r√©el

**Terminal 1** : Observer le HPA
```bash
watch -n 2 'kubectl get hpa backend-api-hpa -n taskflow'
```

**Terminal 2** : Observer les pods
```bash
watch -n 2 'kubectl get pods -n taskflow -l app=backend-api'
```

**Terminal 3** : Observer les m√©triques
```bash
watch -n 5 'kubectl top pods -n taskflow -l app=backend-api'
```

### 9.3 Ce que vous devriez observer

**Phase 1 : Mont√©e en charge (0-2 minutes)**
- L'utilisation CPU des pods backend passe de ~5% √† 60-80%
- Le HPA d√©tecte la charge excessive

**Phase 2 : Scale up (2-5 minutes)**
- Le HPA cr√©e de nouveaux pods (3, 4, 5, 6...)
- Les nouveaux pods d√©marrent et deviennent Ready
- La charge se r√©partit sur plus de pods
- L'utilisation CPU par pod redescend

**Phase 3 : Stabilisation (5-10 minutes)**
- Le nombre de pods se stabilise (g√©n√©ralement 6-8 pods)
- L'utilisation CPU se maintient autour de 50%

**Phase 4 : Arr√™t du load generator**
```bash
# Arr√™ter la charge
kubectl delete job load-generator -n taskflow
```

**Phase 5 : Scale down (10-15 minutes)**
- L'utilisation CPU chute
- Le HPA attend 60 secondes (stabilizationWindow)
- Il r√©duit progressivement le nombre de pods
- Retour √† 2 pods (minReplicas)

### 9.4 Observer dans Grafana

Pendant le test, observer dans Grafana :
1. Le **CPU usage** monter puis se stabiliser
2. Le **nombre de pods** augmenter de 2 √† 8-10
3. Les **requ√™tes par seconde** augmenter
4. La **latence** rester stable gr√¢ce √† l'autoscaling

## üìä Partie 10 : Analyse et Nettoyage

### 10.1 Analyser les logs

```bash
# Logs du HPA (events)
kubectl describe hpa backend-api-hpa -n taskflow

# Logs des pods backend
kubectl logs -n taskflow -l app=backend-api --tail=100

# Events du namespace
kubectl get events -n taskflow --sort-by='.lastTimestamp'
```

### 10.2 Questions de r√©flexion

1. **Combien de temps le HPA a-t-il mis pour scaler de 2 √† 10 pods ?**
2. **Pourquoi le scale-down est-il plus lent que le scale-up ?**
3. **Quelle est l'utilisation CPU moyenne par pod pendant la charge ?**
4. **Combien de requ√™tes par seconde l'API peut-elle g√©rer avec 10 pods ?**

### 10.3 Nettoyer les ressources

```bash
# Option 1 : Supprimer tout le namespace (tout effacer)
kubectl delete namespace taskflow

# Option 2 : Supprimer ressource par ressource
kubectl delete -f 22-load-generator.yaml
kubectl delete -f 21-grafana-service.yaml
kubectl delete -f 20-grafana-deployment.yaml
kubectl delete -f 19-prometheus-service.yaml
kubectl delete -f 18-prometheus-deployment.yaml
kubectl delete -f 17-prometheus-pvc.yaml
kubectl delete -f 16-prometheus-rbac.yaml
kubectl delete -f 15-prometheus-config.yaml
kubectl delete -f 14-frontend-service.yaml
kubectl delete -f 13-frontend-deployment.yaml
kubectl delete -f 12-frontend-config.yaml
kubectl delete -f 11-backend-hpa.yaml
kubectl delete -f 10-backend-service.yaml
kubectl delete -f 09-backend-deployment.yaml
kubectl delete -f 08-backend-config.yaml
kubectl delete -f 07-redis-service.yaml
kubectl delete -f 06-redis-deployment.yaml
kubectl delete -f 05-postgres-service.yaml
kubectl delete -f 04-postgres-deployment.yaml
kubectl delete -f 03-postgres-pvc.yaml
kubectl delete -f 02-postgres-secret.yaml
kubectl delete -f 01-postgres-init-script.yaml
```

## üéì Concepts cl√©s appris

### 1. initContainers
- S'ex√©cutent **avant** les conteneurs principaux
- Utiles pour l'initialisation (DB schema, configuration, t√©l√©chargements)
- Doivent se terminer avec succ√®s pour que le pod d√©marre

### 2. HorizontalPodAutoscaler (HPA)
- Scale automatiquement bas√© sur CPU/m√©moire
- Param√®tres importants : `minReplicas`, `maxReplicas`, `targetAverageUtilization`
- Comportements : `scaleUp` (rapide) vs `scaleDown` (lent et prudent)

### 3. LoadBalancer Services
- Exposent l'application √† l'ext√©rieur du cluster
- Sur Minikube : utiliser `minikube tunnel` ou `minikube service`
- Sur cloud providers : cr√©ent automatiquement un load balancer externe

### 4. Monitoring avec Prometheus
- **Prometheus** collecte les m√©triques (scraping)
- **Grafana** visualise les donn√©es
- RBAC n√©cessaire pour que Prometheus interroge l'API Kubernetes

### 5. PersistentVolumeClaim (PVC)
- Permettent la persistance des donn√©es
- PostgreSQL : stocke la base de donn√©es
- Prometheus : stocke les m√©triques historiques

### 6. ConfigMaps et Secrets
- **ConfigMap** : configuration non sensible (URLs, ports)
- **Secret** : donn√©es sensibles (passwords, tokens)
- Mont√©s comme volumes ou variables d'environnement

## üìö Exercices suppl√©mentaires

### Exercice 1 : Modifier les seuils du HPA
Modifier `11-backend-hpa.yaml` pour scaler plus agressivement :
- CPU target: 30% (au lieu de 50%)
- MaxReplicas: 15 (au lieu de 10)

Observer la diff√©rence de comportement.

### Exercice 2 : Ajouter une NetworkPolicy
Cr√©er une NetworkPolicy qui :
- Permet uniquement au frontend de contacter le backend
- Permet uniquement au backend de contacter PostgreSQL et Redis
- Bloque tout le reste

### Exercice 3 : Monitoring avanc√©
Ajouter au dashboard Grafana :
- Taux d'erreur HTTP (4xx, 5xx)
- Latence P50, P95, P99
- Nombre de connexions √† PostgreSQL

### Exercice 4 : Haute disponibilit√©
Modifier pour avoir :
- 3 replicas de PostgreSQL (avec r√©plication)
- 3 replicas de Redis (Redis Cluster)
- PodDisruptionBudget pour garantir la disponibilit√©

## üéØ Checklist de r√©ussite

- [ ] Tous les pods sont en √©tat Running
- [ ] La base de donn√©es contient 1000 t√¢ches
- [ ] Le frontend est accessible via LoadBalancer
- [ ] Le HPA montre 2 replicas au repos
- [ ] Prometheus collecte les m√©triques
- [ ] Grafana affiche les dashboards
- [ ] Le load generator augmente la charge
- [ ] Le HPA scale de 2 √† 8-10 pods
- [ ] L'utilisation CPU se stabilise autour de 50%
- [ ] Le scale-down fonctionne apr√®s arr√™t de la charge

## üìñ Ressources

- [HPA Documentation](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
- [Init Containers](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/)
- [Prometheus Operator](https://prometheus-operator.dev/)
- [Grafana Dashboards](https://grafana.com/grafana/dashboards/)

## üéâ Conclusion

F√©licitations ! Vous avez d√©ploy√© une application compl√®te avec :
- ‚úÖ **Auto-scaling** intelligent bas√© sur les m√©triques r√©elles
- ‚úÖ **Initialisation** automatique avec initContainers
- ‚úÖ **Monitoring** en temps r√©el avec Prometheus et Grafana
- ‚úÖ **Persistance** des donn√©es avec PVC
- ‚úÖ **Exposition** s√©curis√©e avec Services et LoadBalancer

Ce projet de synth√®se d√©montre votre ma√Ætrise de Kubernetes et des concepts avanc√©s n√©cessaires pour d√©ployer des applications en production.

**Prochaines √©tapes** :
- Ajouter un Ingress pour g√©rer le routage HTTP
- Impl√©menter un CI/CD avec ArgoCD (TP6)
- Ajouter des Network Policies (TP5, TP8)
- D√©ployer sur un cluster multi-n≈ìuds (TP9)
