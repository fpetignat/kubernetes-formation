apiVersion: v1
kind: ConfigMap
metadata:
  name: backend-app-code
  namespace: taskflow
data:
  app.py: |
    #!/usr/bin/env python3
    import os
    import json
    import time
    import psycopg2
    import redis
    from flask import Flask, jsonify, request, Response
    from psycopg2.extras import RealDictCursor
    from prometheus_client import Counter, Histogram, Gauge, generate_latest, CollectorRegistry, CONTENT_TYPE_LATEST

    app = Flask(__name__)

    # ============================================
    # PROMETHEUS METRICS
    # ============================================
    registry = CollectorRegistry()

    # Compteurs de requêtes HTTP
    http_requests_total = Counter(
        'http_requests_total',
        'Total des requêtes HTTP',
        ['method', 'endpoint', 'status'],
        registry=registry
    )

    # Histogramme de latence des requêtes
    http_request_duration_seconds = Histogram(
        'http_request_duration_seconds',
        'Durée des requêtes HTTP en secondes',
        ['method', 'endpoint'],
        registry=registry
    )

    # Gauges pour les métriques de base de données
    database_connections_active = Gauge(
        'database_connections_active',
        'Nombre de connexions actives à la base de données',
        registry=registry
    )

    tasks_total = Gauge(
        'tasks_total',
        'Nombre total de tâches',
        registry=registry
    )

    tasks_completed = Gauge(
        'tasks_completed',
        'Nombre de tâches complétées',
        registry=registry
    )

    tasks_pending = Gauge(
        'tasks_pending',
        'Nombre de tâches en attente',
        registry=registry
    )

    # Compteur de hits/miss du cache Redis
    cache_hits_total = Counter(
        'cache_hits_total',
        'Nombre de hits du cache Redis',
        registry=registry
    )

    cache_misses_total = Counter(
        'cache_misses_total',
        'Nombre de miss du cache Redis',
        registry=registry
    )

    # Configuration depuis les variables d'environnement
    DB_CONFIG = {
        'host': os.getenv('DATABASE_HOST', 'postgres'),
        'port': int(os.getenv('DATABASE_PORT', 5432)),
        'database': os.getenv('DATABASE_NAME', 'taskflow_db'),
        'user': os.getenv('DATABASE_USER', 'taskflow'),
        'password': os.getenv('DATABASE_PASSWORD', 'taskflow2024')
    }

    REDIS_CONFIG = {
        'host': os.getenv('REDIS_HOST', 'redis'),
        'port': int(os.getenv('REDIS_PORT', 6379)),
        'decode_responses': True
    }

    CACHE_TTL = int(os.getenv('CACHE_TTL', 300))

    # Connexion Redis
    try:
        redis_client = redis.Redis(**REDIS_CONFIG)
        redis_client.ping()
    except Exception as e:
        print(f"Redis connection failed: {e}")
        redis_client = None

    def get_db_connection():
        """Créer une connexion à PostgreSQL"""
        return psycopg2.connect(**DB_CONFIG, cursor_factory=RealDictCursor)

    def get_cached(key):
        """Récupérer depuis le cache Redis"""
        if redis_client is None:
            cache_misses_total.inc()
            return None
        try:
            data = redis_client.get(key)
            if data:
                cache_hits_total.inc()
                return json.loads(data)
            else:
                cache_misses_total.inc()
        except Exception as e:
            print(f"Cache get error: {e}")
            cache_misses_total.inc()
        return None

    def set_cached(key, value, ttl=CACHE_TTL):
        """Sauvegarder dans le cache Redis"""
        if redis_client is None:
            return
        try:
            redis_client.setex(key, ttl, json.dumps(value))
        except Exception as e:
            print(f"Cache set error: {e}")

    @app.route('/metrics')
    def metrics():
        """Endpoint pour exposer les métriques Prometheus"""
        # Mettre à jour les métriques de tâches avant d'exposer
        try:
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute("""
                SELECT
                    COUNT(*) as total,
                    SUM(CASE WHEN completed THEN 1 ELSE 0 END) as completed,
                    SUM(CASE WHEN NOT completed THEN 1 ELSE 0 END) as pending
                FROM tasks
            """)
            stats = cursor.fetchone()
            cursor.close()
            conn.close()

            tasks_total.set(int(stats['total']))
            tasks_completed.set(int(stats['completed']))
            tasks_pending.set(int(stats['pending']))
        except Exception as e:
            print(f"Error updating task metrics: {e}")

        return Response(generate_latest(registry), mimetype=CONTENT_TYPE_LATEST)

    @app.route('/health')
    def health():
        """Health check endpoint"""
        http_requests_total.labels(method='GET', endpoint='/health', status='200').inc()
        return jsonify({'status': 'healthy', 'service': 'taskflow-api'}), 200

    @app.route('/ready')
    def ready():
        """Readiness check endpoint"""
        try:
            # Vérifier PostgreSQL
            conn = get_db_connection()
            cursor = conn.cursor()
            cursor.execute('SELECT 1')
            cursor.close()
            conn.close()
            http_requests_total.labels(method='GET', endpoint='/ready', status='200').inc()
            return jsonify({'status': 'ready', 'database': 'connected'}), 200
        except Exception as e:
            http_requests_total.labels(method='GET', endpoint='/ready', status='503').inc()
            return jsonify({'status': 'not ready', 'error': str(e)}), 503

    @app.route('/tasks', methods=['GET'])
    def get_tasks():
        """Récupérer les tâches (avec filtres optionnels)"""
        start_time = time.time()

        # Paramètres de requête
        priority = request.args.get('priority')
        completed = request.args.get('completed')
        limit = min(int(request.args.get('limit', 100)), 1000)

        # Créer une clé de cache basée sur les paramètres
        cache_key = f"tasks:priority={priority}:completed={completed}:limit={limit}"

        # Essayer de récupérer depuis le cache
        cached_data = get_cached(cache_key)
        if cached_data:
            cached_data['from_cache'] = True
            return jsonify(cached_data), 200

        # Requête SQL
        conn = get_db_connection()
        cursor = conn.cursor()

        query = "SELECT * FROM tasks WHERE 1=1"
        params = []

        if priority:
            query += " AND priority = %s"
            params.append(priority)

        if completed is not None:
            query += " AND completed = %s"
            params.append(completed.lower() == 'true')

        query += f" ORDER BY created_at DESC LIMIT {limit}"

        cursor.execute(query, params)
        tasks = cursor.fetchall()

        # Récupérer les stats
        cursor.execute("""
            SELECT
                COUNT(*) as total,
                SUM(CASE WHEN completed THEN 1 ELSE 0 END) as completed,
                SUM(CASE WHEN NOT completed THEN 1 ELSE 0 END) as pending
            FROM tasks
        """)
        stats = cursor.fetchone()

        cursor.close()
        conn.close()

        # Convertir les résultats
        result = {
            'tasks': [dict(task) for task in tasks],
            'stats': {
                'total': int(stats['total']),
                'completed': int(stats['completed']),
                'pending': int(stats['pending'])
            },
            'count': len(tasks),
            'from_cache': False
        }

        # Mettre en cache
        set_cached(cache_key, result)

        # Enregistrer les métriques
        duration = time.time() - start_time
        http_request_duration_seconds.labels(method='GET', endpoint='/tasks').observe(duration)
        http_requests_total.labels(method='GET', endpoint='/tasks', status='200').inc()

        return jsonify(result), 200

    @app.route('/tasks/<int:task_id>', methods=['GET'])
    def get_task(task_id):
        """Récupérer une tâche spécifique"""
        cache_key = f"task:{task_id}"

        cached_data = get_cached(cache_key)
        if cached_data:
            return jsonify(cached_data), 200

        conn = get_db_connection()
        cursor = conn.cursor()

        cursor.execute("SELECT * FROM tasks WHERE id = %s", (task_id,))
        task = cursor.fetchone()

        cursor.close()
        conn.close()

        if task is None:
            return jsonify({'error': 'Task not found'}), 404

        result = dict(task)
        set_cached(cache_key, result)

        return jsonify(result), 200

    @app.route('/stats', methods=['GET'])
    def get_stats():
        """Récupérer les statistiques globales"""
        start_time = time.time()
        cache_key = "stats:global"

        cached_data = get_cached(cache_key)
        if cached_data:
            return jsonify(cached_data), 200

        conn = get_db_connection()
        cursor = conn.cursor()

        cursor.execute("""
            SELECT
                COUNT(*) as total_tasks,
                SUM(CASE WHEN completed THEN 1 ELSE 0 END) as completed_tasks,
                SUM(CASE WHEN NOT completed THEN 1 ELSE 0 END) as pending_tasks,
                SUM(CASE WHEN priority = 'high' THEN 1 ELSE 0 END) as high_priority,
                SUM(CASE WHEN priority = 'medium' THEN 1 ELSE 0 END) as medium_priority,
                SUM(CASE WHEN priority = 'low' THEN 1 ELSE 0 END) as low_priority
            FROM tasks
        """)
        stats = cursor.fetchone()

        cursor.close()
        conn.close()

        result = {
            'total_tasks': int(stats['total_tasks']),
            'completed_tasks': int(stats['completed_tasks']),
            'pending_tasks': int(stats['pending_tasks']),
            'by_priority': {
                'high': int(stats['high_priority']),
                'medium': int(stats['medium_priority']),
                'low': int(stats['low_priority'])
            }
        }

        set_cached(cache_key, result, ttl=60)

        # Enregistrer les métriques
        duration = time.time() - start_time
        http_request_duration_seconds.labels(method='GET', endpoint='/stats').observe(duration)
        http_requests_total.labels(method='GET', endpoint='/stats', status='200').inc()

        return jsonify(result), 200

    @app.route('/stress', methods=['GET'])
    def stress_test():
        """Endpoint pour tester la charge CPU"""
        duration = min(int(request.args.get('duration', 1)), 10)

        start = time.time()
        result = 0
        while (time.time() - start) < duration:
            result += sum(i * i for i in range(1000))

        return jsonify({
            'message': 'Stress test completed',
            'duration': duration,
            'result': result
        }), 200

    if __name__ == '__main__':
        app.run(host='0.0.0.0', port=5000, debug=False)
