# TP8 - RÃ©seau Kubernetes : Services, DNS et ConnectivitÃ©

## Objectifs du TP

Ce TP vous permettra de maÃ®triser le rÃ©seau dans Kubernetes de maniÃ¨re approfondie et pratique. Vous apprendrez :

- Le modÃ¨le rÃ©seau Kubernetes et ses principes fondamentaux
- Les diffÃ©rents types de Services et leurs cas d'usage
- Le DNS Kubernetes et la dÃ©couverte de services
- Les NetworkPolicies pour sÃ©curiser les communications
- Le dÃ©bogage rÃ©seau avec les outils appropriÃ©s
- L'implÃ©mentation d'architectures rÃ©seau complexes

**DurÃ©e estimÃ©e :** 6-8 heures
**Niveau :** IntermÃ©diaire Ã  AvancÃ©

## PrÃ©requis

- Avoir complÃ©tÃ© le TP1 (bases Kubernetes) et TP2 (manifests)
- Cluster Kubernetes fonctionnel (**minikube** ou **kubeadm**)
- kubectl installÃ© et configurÃ©
- Notions de rÃ©seau (IP, ports, DNS)

**Note pour kubeadm :** Les concepts rÃ©seau (Services, Ingress, Network Policies) sont identiques. Pour l'Ingress Controller, consultez le [guide kubeadm](../docs/KUBEADM_SETUP.md#113-ingress-controller-nginx-ingress).

## Table des matiÃ¨res

- [Partie 1 : Le modÃ¨le rÃ©seau Kubernetes](#partie-1--le-modÃ¨le-rÃ©seau-kubernetes)
- [Partie 2 : Services et types d'exposition](#partie-2--services-et-types-dexposition)
- [Partie 3 : DNS et Service Discovery](#partie-3--dns-et-service-discovery)
- [Partie 4 : NetworkPolicies et sÃ©curitÃ© rÃ©seau](#partie-4--networkpolicies-et-sÃ©curitÃ©-rÃ©seau)
- [Partie 5 : DÃ©bogage rÃ©seau](#partie-5--dÃ©bogage-rÃ©seau)
- [Partie 6 : Architectures rÃ©seau avancÃ©es](#partie-6--architectures-rÃ©seau-avancÃ©es)
- [Exercices pratiques](#exercices-pratiques)

---

## Partie 1 : Le modÃ¨le rÃ©seau Kubernetes

### 1.1 Principes fondamentaux

Kubernetes implÃ©mente un modÃ¨le rÃ©seau "flat" basÃ© sur plusieurs principes clÃ©s :

1. **Chaque Pod obtient sa propre adresse IP**
   - Pas de NAT entre Pods
   - Les conteneurs dans un mÃªme Pod partagent le mÃªme namespace rÃ©seau

2. **Communication directe entre Pods**
   - Tous les Pods peuvent communiquer entre eux sans NAT
   - Les Pods sur diffÃ©rents nÅ“uds peuvent se parler directement

3. **Communication Pod-Ã -Node**
   - Les Pods peuvent communiquer avec tous les nÅ“uds sans NAT
   - Les nÅ“uds peuvent communiquer avec tous les Pods sans NAT

### 1.2 Architecture rÃ©seau

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Cluster Kubernetes                        â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   Node 1         â”‚         â”‚   Node 2         â”‚          â”‚
â”‚  â”‚                  â”‚         â”‚                  â”‚          â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚          â”‚
â”‚  â”‚  â”‚ Pod A     â”‚   â”‚         â”‚  â”‚ Pod C     â”‚   â”‚          â”‚
â”‚  â”‚  â”‚ IP: 10.1.1.1 â”‚         â”‚  â”‚ IP: 10.1.2.1 â”‚          â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚          â”‚
â”‚  â”‚       â”‚          â”‚         â”‚       â”‚          â”‚          â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚          â”‚
â”‚  â”‚  â”‚ Pod B     â”‚   â”‚         â”‚  â”‚ Pod D     â”‚   â”‚          â”‚
â”‚  â”‚  â”‚ IP: 10.1.1.2 â”‚         â”‚  â”‚ IP: 10.1.2.2 â”‚          â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚          â”‚
â”‚  â”‚       â”‚          â”‚         â”‚       â”‚          â”‚          â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”‚         â”‚  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”‚          â”‚
â”‚  â”‚  â”‚  Bridge   â”‚   â”‚         â”‚  â”‚  Bridge   â”‚   â”‚          â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â”‚         â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚           â”‚                            â”‚                     â”‚
â”‚      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”               â”‚
â”‚      â”‚      Overlay Network (CNI)            â”‚               â”‚
â”‚      â”‚   (Calico, Flannel, Weave, etc.)      â”‚               â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 Container Network Interface (CNI)

Les plugins CNI les plus courants :

| Plugin | Description | FonctionnalitÃ©s |
|--------|-------------|-----------------|
| **Calico** | Solution rÃ©seau et NetworkPolicy | Routing BGP, NetworkPolicies avancÃ©es |
| **Flannel** | Overlay network simple | Simple, lÃ©ger, orientÃ© overlay |
| **Weave** | Mesh network automatique | Chiffrement, service discovery |
| **Cilium** | BasÃ© sur eBPF | Haute performance, observabilitÃ© |
| **Canal** | Calico + Flannel | Networking Flannel + NetworkPolicies Calico |

**ğŸ” VÃ©rifier le plugin CNI actuel :**

```bash
# Voir les pods du systÃ¨me rÃ©seau
kubectl get pods -n kube-system | grep -E 'calico|flannel|weave|cilium'

# VÃ©rifier la configuration CNI
ls /etc/cni/net.d/

# Voir les dÃ©tails d'un pod pour identifier le rÃ©seau
kubectl describe pod <pod-name> | grep "cni"
```

### 1.4 Exercice pratique : Explorer le rÃ©seau

**Exercice 1.1 : Visualiser l'adressage IP**

```bash
# CrÃ©er plusieurs pods
kubectl create deployment web --image=nginx:alpine --replicas=3

# Voir les IPs des pods
kubectl get pods -o wide

# Voir les dÃ©tails rÃ©seau d'un pod
kubectl describe pod <pod-name> | grep IP
```

**Questions :**
- Quelle est la plage d'adresses IP utilisÃ©e pour les Pods ?
- Les Pods sur le mÃªme nÅ“ud ont-ils des IPs consÃ©cutives ?
- Que se passe-t-il si vous supprimez et recrÃ©ez un Pod ?

**Exercice 1.2 : Communication inter-pods**

```bash
# CrÃ©er deux pods
kubectl run pod-a --image=nginx:alpine
kubectl run pod-b --image=nginx:alpine

# RÃ©cupÃ©rer l'IP du pod-b
POD_B_IP=$(kubectl get pod pod-b -o jsonpath='{.status.podIP}')

# Tester la communication depuis pod-a vers pod-b
kubectl exec pod-a -- wget -qO- http://$POD_B_IP

# VÃ©rifier que la communication fonctionne sans NAT
kubectl exec pod-a -- ping -c 3 $POD_B_IP
```

---

## Partie 2 : Services et types d'exposition

### 2.1 Pourquoi les Services ?

Les Pods sont **Ã©phÃ©mÃ¨res** : ils peuvent Ãªtre crÃ©Ã©s, dÃ©truits, et leurs IPs changent. Les Services fournissent :

- **Abstraction stable** : Une IP et un DNS qui ne changent pas
- **Load balancing** : Distribution du trafic entre plusieurs Pods
- **Service discovery** : DÃ©couverte automatique via DNS

### 2.2 Service ClusterIP (par dÃ©faut)

Expose le Service sur une IP interne au cluster.

**Cas d'usage :**
- Communication entre microservices
- Bases de donnÃ©es internes
- APIs backend

**Exemple complet :**

```yaml
# deployment-backend.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
  labels:
    app: backend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
  template:
    metadata:
      labels:
        app: backend
    spec:
      containers:
      - name: backend
        image: nginx:alpine
        ports:
        - containerPort: 80
---
# service-backend.yaml
apiVersion: v1
kind: Service
metadata:
  name: backend-svc
spec:
  type: ClusterIP
  selector:
    app: backend
  ports:
  - protocol: TCP
    port: 80        # Port du Service
    targetPort: 80  # Port du conteneur
```

**DÃ©ploiement et test :**

```bash
# CrÃ©er les ressources
kubectl apply -f deployment-backend.yaml

# VÃ©rifier le service
kubectl get svc backend-svc
# NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)
# backend-svc   ClusterIP   10.96.123.45   <none>        80/TCP

# Tester depuis un pod temporaire
kubectl run tmp --image=busybox --rm -it -- wget -qO- http://backend-svc

# Tester avec le FQDN complet
kubectl run tmp --image=busybox --rm -it -- wget -qO- http://backend-svc.default.svc.cluster.local
```

### 2.3 Service NodePort

Expose le Service sur un port statique de chaque nÅ“ud.

**Cas d'usage :**
- DÃ©veloppement et tests
- AccÃ¨s externe sans load balancer cloud
- Applications nÃ©cessitant un port spÃ©cifique

**Exemple :**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: web-nodeport
spec:
  type: NodePort
  selector:
    app: web
  ports:
  - protocol: TCP
    port: 80          # Port du Service (interne)
    targetPort: 80    # Port du conteneur
    nodePort: 30080   # Port sur chaque nÅ“ud (30000-32767)
```

**AccÃ¨s :**

```bash
# CrÃ©er le deployment et service
kubectl create deployment web --image=nginx:alpine
kubectl apply -f service-nodeport.yaml

# Obtenir l'IP du nÅ“ud
NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')

# AccÃ©der au service (si minikube)
minikube service web-nodeport --url

# Ou directement avec curl
curl http://$NODE_IP:30080
```

### 2.4 Service LoadBalancer

Expose le Service via un load balancer cloud (AWS ELB, GCP LB, etc.).

**Cas d'usage :**
- Production sur cloud provider
- Applications exposÃ©es publiquement
- Haute disponibilitÃ©

**Exemple :**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: web-lb
spec:
  type: LoadBalancer
  selector:
    app: web
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
```

**Note pour minikube :**

```bash
# Sur minikube, utiliser le tunnel pour simuler un LoadBalancer
minikube tunnel

# Dans un autre terminal
kubectl get svc web-lb
# EXTERNAL-IP passera de <pending> Ã  une IP
```

### 2.5 Service ExternalName

CrÃ©e un alias DNS vers un service externe.

**Cas d'usage :**
- IntÃ©gration avec services externes
- Migration progressive vers Kubernetes
- Abstraction des dÃ©pendances externes

**Exemple :**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: external-db
spec:
  type: ExternalName
  externalName: database.example.com
```

**Utilisation :**

```bash
# Les pods peuvent maintenant utiliser "external-db" au lieu de "database.example.com"
kubectl run tmp --image=busybox --rm -it -- nslookup external-db
```

### 2.6 Headless Service

Service sans IP de cluster (ClusterIP: None), pour un accÃ¨s direct aux IPs des Pods.

**Cas d'usage :**
- StatefulSets et bases de donnÃ©es
- Communication P2P entre Pods
- Service discovery personnalisÃ©

**Exemple :**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: db-headless
spec:
  clusterIP: None
  selector:
    app: database
  ports:
  - port: 3306
```

**Test :**

```bash
# CrÃ©er des pods avec le label app=database
kubectl create deployment db --image=mysql:8 --replicas=3
kubectl set env deployment/db MYSQL_ROOT_PASSWORD=secret

# CrÃ©er le headless service
kubectl apply -f headless-service.yaml

# Faire un DNS lookup
kubectl run tmp --image=busybox --rm -it -- nslookup db-headless
# Retourne les IPs de tous les Pods, pas une seule IP de service
```

### 2.7 Endpoints et EndpointSlices

Les Services utilisent des **Endpoints** pour suivre les IPs des Pods.

```bash
# Voir les endpoints d'un service
kubectl get endpoints backend-svc

# Voir les dÃ©tails
kubectl describe endpoints backend-svc

# Depuis Kubernetes 1.21, utiliser EndpointSlices (plus scalable)
kubectl get endpointslices
```

**CrÃ©er un Service avec Endpoints manuels (pour services externes) :**

```yaml
# Service sans selector
apiVersion: v1
kind: Service
metadata:
  name: external-api
spec:
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
---
# Endpoints manuels
apiVersion: v1
kind: Endpoints
metadata:
  name: external-api
subsets:
- addresses:
  - ip: 192.168.1.100
  - ip: 192.168.1.101
  ports:
  - port: 80
```

### 2.8 Session Affinity

Diriger toujours le mÃªme client vers le mÃªme Pod.

```yaml
apiVersion: v1
kind: Service
metadata:
  name: sticky-service
spec:
  selector:
    app: web
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 3600
  ports:
  - port: 80
```

---

## Partie 3 : DNS et Service Discovery

### 3.1 DNS dans Kubernetes

Kubernetes inclut un serveur DNS (CoreDNS par dÃ©faut) qui crÃ©e automatiquement des enregistrements pour les Services et Pods.

**Architecture DNS :**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Pod Application                 â”‚
â”‚                                              â”‚
â”‚  RequÃªte DNS: backend-svc.default.svc.cluster.local
â”‚                     â”‚                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚    CoreDNS      â”‚
           â”‚ (kube-system)   â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼             â–¼             â–¼
   Service IP    Endpoints     Pod IPs
```

### 3.2 Format DNS des Services

**Format complet (FQDN) :**

```
<service-name>.<namespace>.svc.<cluster-domain>
```

**Exemples :**

```bash
# Service "backend-svc" dans namespace "default"
backend-svc.default.svc.cluster.local

# Service "api" dans namespace "production"
api.production.svc.cluster.local
```

**Formes courtes (depuis un Pod) :**

```bash
# MÃªme namespace
backend-svc

# Autre namespace (il faut le namespace)
api.production

# FQDN complet fonctionne partout
api.production.svc.cluster.local
```

### 3.3 DNS pour les Pods

Les Pods obtiennent aussi des enregistrements DNS.

**Format :**

```
<pod-ip-address>.<namespace>.pod.<cluster-domain>
```

**Exemple :**

```bash
# Pod avec IP 10.244.1.5 dans namespace default
10-244-1-5.default.pod.cluster.local
```

**Pour les Pods d'un Headless Service :**

```
<pod-name>.<headless-service-name>.<namespace>.svc.<cluster-domain>
```

### 3.4 Exercices pratiques DNS

**Exercice 3.1 : RÃ©solution DNS entre namespaces**

```bash
# CrÃ©er deux namespaces
kubectl create namespace frontend
kubectl create namespace backend

# CrÃ©er un service dans backend
kubectl create deployment api -n backend --image=nginx:alpine
kubectl expose deployment api -n backend --port=80

# CrÃ©er un pod dans frontend
kubectl run test -n frontend --image=busybox --rm -it -- sh

# Dans le pod, tester les diffÃ©rentes formes DNS
wget -qO- http://api.backend
wget -qO- http://api.backend.svc
wget -qO- http://api.backend.svc.cluster.local

# Tenter d'accÃ©der avec juste le nom (devrait Ã©chouer - namespace diffÃ©rent)
wget -qO- http://api  # ERREUR
```

**Exercice 3.2 : Debug DNS**

```bash
# Tester la rÃ©solution DNS
kubectl run dnsutils --image=tutum/dnsutils --rm -it -- sh

# Dans le pod
nslookup kubernetes.default
nslookup backend-svc.default.svc.cluster.local
host backend-svc.default.svc.cluster.local

# Voir la configuration DNS du pod
cat /etc/resolv.conf
```

**Sortie attendue (/etc/resolv.conf) :**

```
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local
options ndots:5
```

### 3.5 Configuration DNS des Pods

Personnaliser la configuration DNS d'un Pod :

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: custom-dns
spec:
  containers:
  - name: app
    image: nginx:alpine
  dnsPolicy: "None"
  dnsConfig:
    nameservers:
      - 8.8.8.8
    searches:
      - custom.local
    options:
      - name: ndots
        value: "2"
```

**Politiques DNS disponibles :**

- `ClusterFirst` (dÃ©faut) : Utilise CoreDNS du cluster
- `Default` : HÃ©rite du nÅ“ud
- `ClusterFirstWithHostNet` : Pour pods avec hostNetwork
- `None` : Configuration manuelle

---

## Partie 4 : NetworkPolicies et sÃ©curitÃ© rÃ©seau

### 4.1 Principe des NetworkPolicies

Par dÃ©faut, **tous les Pods peuvent communiquer avec tous les Pods**. Les NetworkPolicies permettent de restreindre ce trafic.

**âš ï¸ Important :** Les NetworkPolicies nÃ©cessitent un plugin CNI qui les supporte (Calico, Cilium, Weave, etc.). **Flannel ne supporte PAS les NetworkPolicies**.

**VÃ©rifier le support :**

```bash
# Voir le plugin CNI
kubectl get pods -n kube-system | grep -E 'calico|cilium|weave'
```

### 4.2 Comportement par dÃ©faut

```yaml
# NetworkPolicy qui deny tout le trafic ingress
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-ingress
  namespace: production
spec:
  podSelector: {}  # S'applique Ã  tous les pods du namespace
  policyTypes:
  - Ingress
```

```yaml
# NetworkPolicy qui deny tout le trafic egress
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-egress
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Egress
```

### 4.3 Allow depuis des Pods spÃ©cifiques

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-frontend
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    ports:
    - protocol: TCP
      port: 8080
```

**SchÃ©ma :**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pod frontend â”‚
â”‚ (app=frontend)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ âœ… AUTORISÃ‰ sur port 8080
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pod backend  â”‚
â”‚ (app=backend)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–²
       â”‚ âŒ REFUSÃ‰
â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Autre Pod   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.4 Allow depuis un Namespace spÃ©cifique

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-admin-ns
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: api
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: admin
    ports:
    - protocol: TCP
      port: 80
```

**âš ï¸ N'oubliez pas de labelliser le namespace :**

```bash
kubectl label namespace admin name=admin
```

### 4.5 RÃ¨gles Egress

ContrÃ´ler le trafic sortant des Pods :

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns-and-api
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: web
  policyTypes:
  - Egress
  egress:
  # Autoriser DNS
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
  # Autoriser l'API backend
  - to:
    - podSelector:
        matchLabels:
          app: api
    ports:
    - protocol: TCP
      port: 8080
```

### 4.6 Utilisation d'ipBlock

Autoriser/bloquer des plages d'IPs :

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-cidr
spec:
  podSelector:
    matchLabels:
      app: public-api
  policyTypes:
  - Ingress
  ingress:
  - from:
    - ipBlock:
        cidr: 192.168.1.0/24
        except:
        - 192.168.1.5/32
    ports:
    - protocol: TCP
      port: 443
```

### 4.7 Exemple complet : Architecture 3-tiers

```yaml
---
# Frontend peut accÃ©der au Backend
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: frontend-to-backend
  namespace: app
spec:
  podSelector:
    matchLabels:
      tier: backend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tier: frontend
    ports:
    - protocol: TCP
      port: 8080
---
# Backend peut accÃ©der Ã  la Database
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-to-database
  namespace: app
spec:
  podSelector:
    matchLabels:
      tier: database
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tier: backend
    ports:
    - protocol: TCP
      port: 5432
---
# Database : allow egress pour DNS uniquement
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: database-egress
  namespace: app
spec:
  podSelector:
    matchLabels:
      tier: database
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
```

**SchÃ©ma de l'architecture :**

```
Internet
   â”‚
   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend    â”‚  (tier=frontend)
â”‚  Pods        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ port 8080 âœ…
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Backend     â”‚  (tier=backend)
â”‚  Pods        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ port 5432 âœ…
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Database    â”‚  (tier=database)
â”‚  Pods        â”‚  (egress limitÃ© au DNS)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Partie 5 : DÃ©bogage rÃ©seau

### 5.1 Outils de dÃ©bogage

**CrÃ©er un pod de debug avec tous les outils :**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: netshoot
spec:
  containers:
  - name: netshoot
    image: nicolaka/netshoot
    command: ['sh', '-c', 'sleep 3600']
```

**Outils disponibles dans netshoot :**
- `curl`, `wget` : Tests HTTP
- `ping`, `traceroute` : Tests ICMP
- `nslookup`, `dig`, `host` : DNS
- `netstat`, `ss` : Connexions rÃ©seau
- `tcpdump` : Capture de paquets
- `nmap` : Scan de ports

### 5.2 Tests de connectivitÃ©

**Test HTTP :**

```bash
kubectl exec netshoot -- curl -v http://backend-svc
kubectl exec netshoot -- wget -O- --timeout=5 http://backend-svc
```

**Test DNS :**

```bash
kubectl exec netshoot -- nslookup backend-svc
kubectl exec netshoot -- dig backend-svc.default.svc.cluster.local
kubectl exec netshoot -- host backend-svc
```

**Test de port :**

```bash
# Tester si un port est ouvert
kubectl exec netshoot -- nc -zv backend-svc 80

# Scanner les ports
kubectl exec netshoot -- nmap -p 1-1000 <pod-ip>
```

**Test ICMP :**

```bash
kubectl exec netshoot -- ping -c 3 <pod-ip>
```

### 5.3 Diagnostic des Services

```bash
# VÃ©rifier que le service existe
kubectl get svc backend-svc

# VÃ©rifier les endpoints
kubectl get endpoints backend-svc
kubectl describe endpoints backend-svc

# VÃ©rifier les labels
kubectl get pods --show-labels
kubectl get pods -l app=backend

# Voir les dÃ©tails du service
kubectl describe svc backend-svc
```

**ProblÃ¨mes courants :**

| ProblÃ¨me | Cause probable | Solution |
|----------|----------------|----------|
| Endpoints vide | Selector ne matche aucun Pod | VÃ©rifier labels et selector |
| Service timeout | NetworkPolicy bloque | VÃ©rifier NetworkPolicies |
| DNS ne rÃ©sout pas | CoreDNS en erreur | VÃ©rifier pods kube-system |
| Connection refused | Port incorrect | VÃ©rifier targetPort vs containerPort |

### 5.4 Debug NetworkPolicies

```bash
# Lister toutes les NetworkPolicies
kubectl get networkpolicies --all-namespaces

# Voir les dÃ©tails
kubectl describe networkpolicy deny-all-ingress

# Tester la connectivitÃ©
kubectl run test --image=busybox --rm -it -- wget --timeout=2 http://<pod-ip>
# Si timeout = NetworkPolicy bloque probablement
```

**MÃ©thodologie de debug :**

1. **VÃ©rifier que le plugin CNI supporte NetworkPolicies**
2. **Tester sans NetworkPolicy** (supprimer temporairement)
3. **VÃ©rifier les labels** des Pods et Namespaces
4. **Tester Ã©tape par Ã©tape** (ingress puis egress)
5. **Utiliser les logs** des pods CNI

### 5.5 Capture de paquets avec tcpdump

```bash
# Dans un pod netshoot
kubectl exec -it netshoot -- tcpdump -i any port 80

# Capturer et sauvegarder
kubectl exec netshoot -- tcpdump -i any -w /tmp/capture.pcap

# Copier le fichier localement
kubectl cp netshoot:/tmp/capture.pcap ./capture.pcap

# Analyser avec Wireshark
wireshark capture.pcap
```

### 5.6 VÃ©rifier CoreDNS

```bash
# Status des pods CoreDNS
kubectl get pods -n kube-system -l k8s-app=kube-dns

# Logs CoreDNS
kubectl logs -n kube-system -l k8s-app=kube-dns

# ConfigMap CoreDNS
kubectl get configmap coredns -n kube-system -o yaml
```

---

## Partie 6 : Architectures rÃ©seau avancÃ©es

### 6.1 Architecture microservices sÃ©curisÃ©e

```yaml
---
# Namespace avec NetworkPolicies par dÃ©faut
apiVersion: v1
kind: Namespace
metadata:
  name: secure-app
---
# Deny all ingress par dÃ©faut
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
  namespace: secure-app
spec:
  podSelector: {}
  policyTypes:
  - Ingress
---
# Frontend Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: secure-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: frontend
      tier: frontend
  template:
    metadata:
      labels:
        app: frontend
        tier: frontend
    spec:
      containers:
      - name: frontend
        image: nginx:alpine
        ports:
        - containerPort: 80
---
# Frontend Service
apiVersion: v1
kind: Service
metadata:
  name: frontend
  namespace: secure-app
spec:
  type: NodePort
  selector:
    app: frontend
  ports:
  - port: 80
    nodePort: 30080
---
# Allow ingress to frontend from anywhere
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend-ingress
  namespace: secure-app
spec:
  podSelector:
    matchLabels:
      tier: frontend
  policyTypes:
  - Ingress
  ingress:
  - ports:
    - protocol: TCP
      port: 80
---
# Backend Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
  namespace: secure-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
      tier: backend
  template:
    metadata:
      labels:
        app: backend
        tier: backend
    spec:
      containers:
      - name: backend
        image: nginx:alpine
        ports:
        - containerPort: 8080
---
# Backend Service (ClusterIP)
apiVersion: v1
kind: Service
metadata:
  name: backend
  namespace: secure-app
spec:
  type: ClusterIP
  selector:
    app: backend
  ports:
  - port: 8080
    targetPort: 8080
---
# Allow backend ingress from frontend only
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-backend-from-frontend
  namespace: secure-app
spec:
  podSelector:
    matchLabels:
      tier: backend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tier: frontend
    ports:
    - protocol: TCP
      port: 8080
---
# Database StatefulSet
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: database
  namespace: secure-app
spec:
  serviceName: database-headless
  replicas: 1
  selector:
    matchLabels:
      app: database
      tier: database
  template:
    metadata:
      labels:
        app: database
        tier: database
    spec:
      containers:
      - name: postgres
        image: postgres:15-alpine
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_PASSWORD
          value: "secret"
---
# Database Headless Service
apiVersion: v1
kind: Service
metadata:
  name: database-headless
  namespace: secure-app
spec:
  clusterIP: None
  selector:
    app: database
  ports:
  - port: 5432
---
# Allow database access from backend only
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-database-from-backend
  namespace: secure-app
spec:
  podSelector:
    matchLabels:
      tier: database
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tier: backend
    ports:
    - protocol: TCP
      port: 5432
  egress:
  # Allow DNS
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
```

### 6.2 Multi-tenancy avec isolation rÃ©seau

```yaml
---
# Namespace Tenant A
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-a
  labels:
    tenant: a
---
# Namespace Tenant B
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-b
  labels:
    tenant: b
---
# Deny cross-tenant traffic pour Tenant A
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-other-tenants
  namespace: tenant-a
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          tenant: a
---
# Deny cross-tenant traffic pour Tenant B
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-other-tenants
  namespace: tenant-b
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          tenant: b
```

### 6.3 Monitoring avec Service Mesh (aperÃ§u)

Pour des architectures encore plus avancÃ©es, considÃ©rer un **Service Mesh** comme :

- **Istio** : FonctionnalitÃ©s complÃ¨tes (mTLS, tracing, policies)
- **Linkerd** : LÃ©ger et simple
- **Consul Connect** : Integration HashiCorp

**Avantages :**
- mTLS automatique entre services
- ObservabilitÃ© avancÃ©e (tracing distribuÃ©)
- Traffic management (canary, circuit breaker)
- Retry et timeout automatiques

---

## Exercices pratiques

### Exercice 1 : DÃ©ploiement multi-tiers

**Objectif :** CrÃ©er une architecture 3-tiers avec Services appropriÃ©s

1. CrÃ©er un namespace `my-app`
2. DÃ©ployer :
   - **Frontend** : nginx (3 replicas) â†’ NodePort
   - **Backend** : nginx (2 replicas) â†’ ClusterIP
   - **Database** : postgres (1 replica) â†’ Headless Service
3. Configurer les Services
4. Tester la communication entre les tiers

**VÃ©rifications :**
- Frontend accessible depuis l'extÃ©rieur
- Backend accessible depuis Frontend
- Database accessible depuis Backend uniquement

### Exercice 2 : NetworkPolicies progressives

**Objectif :** SÃ©curiser l'exercice 1 avec NetworkPolicies

1. Appliquer une politique "deny all" par dÃ©faut
2. Autoriser l'ingress vers Frontend depuis l'extÃ©rieur
3. Autoriser Frontend â†’ Backend
4. Autoriser Backend â†’ Database
5. Autoriser DNS pour tous les Pods
6. Tester que les restrictions fonctionnent

### Exercice 3 : Service Discovery

**Objectif :** MaÃ®triser le DNS Kubernetes

1. CrÃ©er 2 namespaces : `ns-a` et `ns-b`
2. DÃ©ployer un service dans chaque namespace
3. Tester les diffÃ©rentes formes DNS
4. Configurer un ExternalName pour un service externe
5. VÃ©rifier la rÃ©solution DNS avec dig/nslookup

### Exercice 4 : Debug rÃ©seau

**Objectif :** Diagnostiquer et rÃ©soudre des problÃ¨mes rÃ©seau

**ScÃ©narios Ã  rÃ©soudre :**

1. Service sans Endpoints
2. DNS qui ne rÃ©sout pas
3. NetworkPolicy qui bloque le trafic
4. Mauvais targetPort configurÃ©

**Outils Ã  utiliser :**
- kubectl describe
- kubectl logs
- Pod netshoot
- tcpdump

### Exercice 5 : Load balancing et Session Affinity

**Objectif :** Comprendre le load balancing des Services

1. CrÃ©er un Deployment avec 5 replicas
2. CrÃ©er un Service standard (round-robin)
3. GÃ©nÃ©rer du trafic et observer la distribution
4. Activer sessionAffinity
5. Observer le changement de comportement

---

## RÃ©sumÃ© des concepts clÃ©s

### Types de Services

| Type | Cas d'usage | Accessible depuis |
|------|-------------|-------------------|
| ClusterIP | Communication interne | Cluster uniquement |
| NodePort | Dev/test, accÃ¨s externe simple | ExtÃ©rieur via NodeIP:NodePort |
| LoadBalancer | Production cloud | ExtÃ©rieur via IP publique |
| ExternalName | Alias DNS vers externe | Cluster (rÃ©solution DNS) |
| Headless | StatefulSet, accÃ¨s direct Pods | Cluster (retourne IPs des Pods) |

### NetworkPolicy : SÃ©lecteurs

```yaml
# SÃ©lectionner des Pods
podSelector:
  matchLabels:
    app: backend

# SÃ©lectionner des Namespaces
namespaceSelector:
  matchLabels:
    env: production

# SÃ©lectionner des IPs
ipBlock:
  cidr: 192.168.1.0/24
  except:
  - 192.168.1.5/32
```

### DNS Kubernetes

```
# Format complet
<service>.<namespace>.svc.<cluster-domain>

# Exemples
backend.default.svc.cluster.local
api.production.svc.cluster.local

# Forme courte (mÃªme namespace)
backend

# Avec namespace
backend.default
```

### Commandes essentielles

```bash
# Services
kubectl get svc
kubectl describe svc <name>
kubectl get endpoints <name>

# NetworkPolicies
kubectl get networkpolicies
kubectl describe networkpolicy <name>

# DNS Debug
kubectl run tmp --image=busybox --rm -it -- nslookup <service>

# ConnectivitÃ©
kubectl exec <pod> -- curl http://<service>
kubectl exec <pod> -- nc -zv <host> <port>
```

---

## Ressources complÃ©mentaires

### Documentation officielle

- [Services](https://kubernetes.io/docs/concepts/services-networking/service/)
- [DNS for Services and Pods](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/)
- [Network Policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
- [Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/)

### Outils et plugins

- [Calico](https://www.projectcalico.org/) - NetworkPolicy et networking
- [Cilium](https://cilium.io/) - eBPF-based networking
- [Weave Net](https://www.weave.works/oss/net/) - Container networking
- [CoreDNS](https://coredns.io/) - DNS server

### Guides avancÃ©s

- [Network Policy Recipes](https://github.com/ahmetb/kubernetes-network-policy-recipes)
- [Debugging DNS Resolution](https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/)
- [Service Mesh Comparison](https://servicemesh.es/)

---

## Prochaines Ã©tapes

AprÃ¨s avoir maÃ®trisÃ© ce TP, vous pouvez explorer :

1. **Ingress Controllers** (TP6) pour le routing HTTP/HTTPS avancÃ©
2. **Service Mesh** (Istio, Linkerd) pour mTLS et observabilitÃ©
3. **Multi-cluster networking** avec Submariner ou Cilium Cluster Mesh
4. **IPv6** et dual-stack networking
5. **eBPF** avec Cilium pour haute performance

---

**ğŸ‰ FÃ©licitations !** Vous maÃ®trisez maintenant le rÃ©seau Kubernetes !

N'hÃ©sitez pas Ã  expÃ©rimenter avec diffÃ©rentes architectures et Ã  pratiquer le dÃ©bogage rÃ©seau. Le rÃ©seau est un aspect fondamental de Kubernetes, et cette maÃ®trise vous sera prÃ©cieuse dans vos dÃ©ploiements en production.
