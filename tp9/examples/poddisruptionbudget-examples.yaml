# Exemples de PodDisruptionBudgets (PDB) pour le TP9

---
# Exemple 1 : minAvailable - Nombre minimum de pods disponibles
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: web-pdb-min
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: web
      tier: frontend

---
# Exemple 2 : maxUnavailable - Maximum de pods qui peuvent être indisponibles
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: backend-pdb-max
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: backend
      tier: api

---
# Exemple 3 : minAvailable en pourcentage
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: frontend-pdb-percentage
spec:
  minAvailable: 50%
  selector:
    matchLabels:
      tier: frontend

---
# Exemple 4 : maxUnavailable en pourcentage
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: cache-pdb-percentage
spec:
  maxUnavailable: 25%
  selector:
    matchLabels:
      app: redis
      role: cache

---
# Exemple 5 : PDB pour base de données critique (haute disponibilité)
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: database-pdb-strict
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: postgres
      tier: database
      critical: "true"

---
# Exemple 6 : Application complète avec Deployment + Service + PDB
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
spec:
  replicas: 5
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
        tier: backend
    spec:
      containers:
      - name: myapp
        image: myapp:v2
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  selector:
    app: myapp
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
  type: ClusterIP
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: myapp-pdb
spec:
  minAvailable: 3
  selector:
    matchLabels:
      app: myapp

---
# Exemple 7 : StatefulSet avec PDB pour quorum (etcd-like)
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: etcd-cluster
spec:
  serviceName: etcd
  replicas: 5
  selector:
    matchLabels:
      app: etcd
  template:
    metadata:
      labels:
        app: etcd
    spec:
      containers:
      - name: etcd
        image: quay.io/coreos/etcd:v3.5.10
        ports:
        - containerPort: 2379
          name: client
        - containerPort: 2380
          name: peer
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
---
# PDB pour maintenir le quorum (minimum 3 sur 5)
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: etcd-pdb
spec:
  minAvailable: 3
  selector:
    matchLabels:
      app: etcd

---
# Exemple 8 : PDB pour workers traitement batch
apiVersion: apps/v1
kind: Deployment
metadata:
  name: batch-workers
spec:
  replicas: 10
  selector:
    matchLabels:
      app: batch-worker
      role: processing
  template:
    metadata:
      labels:
        app: batch-worker
        role: processing
    spec:
      containers:
      - name: worker
        image: batch-worker:v1
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
---
# Autorise jusqu'à 30% d'indisponibilité
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: batch-workers-pdb
spec:
  maxUnavailable: 30%
  selector:
    matchLabels:
      app: batch-worker
      role: processing

---
# Exemple 9 : PDB strict pour API critique (zero downtime)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: critical-api
spec:
  replicas: 6
  selector:
    matchLabels:
      app: critical-api
      sla: high
  template:
    metadata:
      labels:
        app: critical-api
        sla: high
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - critical-api
            topologyKey: kubernetes.io/hostname
      containers:
      - name: api
        image: critical-api:v3
        ports:
        - containerPort: 443
        resources:
          requests:
            memory: "1Gi"
            cpu: "1"
          limits:
            memory: "2Gi"
            cpu: "2"
---
# PDB très strict - maximum 1 pod indisponible à la fois
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: critical-api-pdb
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: critical-api
      sla: high

---
# Exemple 10 : Multiple PDBs pour la même application (différents selectors)
# Déploiement avec plusieurs tiers
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-frontend
spec:
  replicas: 8
  selector:
    matchLabels:
      app: mystack
      component: frontend
  template:
    metadata:
      labels:
        app: mystack
        component: frontend
        version: v2
    spec:
      containers:
      - name: frontend
        image: frontend:v2
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-backend
spec:
  replicas: 6
  selector:
    matchLabels:
      app: mystack
      component: backend
  template:
    metadata:
      labels:
        app: mystack
        component: backend
        version: v2
    spec:
      containers:
      - name: backend
        image: backend:v2
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
---
# PDB pour le frontend
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: mystack-frontend-pdb
spec:
  minAvailable: 60%
  selector:
    matchLabels:
      app: mystack
      component: frontend
---
# PDB pour le backend (plus strict)
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: mystack-backend-pdb
spec:
  minAvailable: 80%
  selector:
    matchLabels:
      app: mystack
      component: backend
---
# PDB global pour toute l'application
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: mystack-global-pdb
spec:
  minAvailable: 10
  selector:
    matchLabels:
      app: mystack

---
# Exemple 11 : PDB avec minAvailable=0 (permet drainage total)
# Utile pour environnements de développement
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: dev-app-pdb
  namespace: development
spec:
  minAvailable: 0
  selector:
    matchLabels:
      env: development

---
# Exemple 12 : PDB pour DaemonSet (cas spécial)
# Note: Les DaemonSets ne sont généralement pas affectés par les PDBs
# car ils sont conçus pour avoir exactement 1 pod par nœud
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: monitoring-agent
spec:
  selector:
    matchLabels:
      app: monitoring
  template:
    metadata:
      labels:
        app: monitoring
        critical: "false"
    spec:
      containers:
      - name: agent
        image: monitoring-agent:latest
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
---
# Ce PDB ne bloquera pas les drains car DaemonSet
# mais documente l'intention
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: monitoring-agent-pdb
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: monitoring
