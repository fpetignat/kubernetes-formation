# Tests de concepts du TP9 avec Minikube

Ce document montre quels concepts du TP9 peuvent Ãªtre testÃ©s avec Minikube (cluster mono-nÅ“ud).

## âš ï¸ Limitations

Minikube est un cluster **mono-nÅ“ud**, donc certains concepts multi-nÅ“uds ne peuvent pas Ãªtre testÃ©s :
- âŒ Ajout de workers
- âŒ Haute disponibilitÃ© du control plane
- âŒ Anti-affinitÃ© stricte entre nÅ“uds
- âŒ Distribution gÃ©ographique

## âœ… Concepts testables avec Minikube

### 1. Labels et NodeSelectors

```bash
# DÃ©marrer minikube
minikube start

# Voir les labels du nÅ“ud
kubectl get nodes --show-labels

# Ajouter des labels
kubectl label nodes minikube disktype=ssd
kubectl label nodes minikube zone=zone-a

# CrÃ©er un pod avec nodeSelector
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: nginx-ssd
spec:
  nodeSelector:
    disktype: ssd
  containers:
  - name: nginx
    image: nginx:alpine
EOF

# VÃ©rifier
kubectl get pod nginx-ssd -o wide
```

**âœ… RÃ©sultat attendu :** Le pod est planifiÃ© sur le nÅ“ud minikube avec le label disktype=ssd

---

### 2. Taints et Tolerations

```bash
# Ajouter un taint au nÅ“ud
kubectl taint nodes minikube dedicated=database:NoSchedule

# Essayer de crÃ©er un pod sans toleration
kubectl run test-no-toleration --image=nginx:alpine

# VÃ©rifier qu'il reste en Pending
kubectl get pods test-no-toleration
kubectl describe pod test-no-toleration

# CrÃ©er un pod avec toleration
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: nginx-with-toleration
spec:
  tolerations:
  - key: "dedicated"
    operator: "Equal"
    value: "database"
    effect: "NoSchedule"
  containers:
  - name: nginx
    image: nginx:alpine
EOF

# VÃ©rifier qu'il est Running
kubectl get pods nginx-with-toleration

# Nettoyer
kubectl delete pod test-no-toleration nginx-with-toleration
kubectl taint nodes minikube dedicated-
```

**âœ… RÃ©sultat attendu :**
- Pod sans toleration : Pending
- Pod avec toleration : Running

---

### 3. Node Affinity

```bash
# Tester l'affinitÃ© requise
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: nginx-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
  containers:
  - name: nginx
    image: nginx:alpine
EOF

# VÃ©rifier
kubectl get pod nginx-affinity -o wide
kubectl describe pod nginx-affinity | grep -A 5 "Node-Selectors"
```

**âœ… RÃ©sultat attendu :** Le pod utilise l'affinitÃ© pour Ãªtre planifiÃ©

---

### 4. PodDisruptionBudgets

```bash
# CrÃ©er un deployment
kubectl create deployment web --image=nginx:alpine --replicas=3

# CrÃ©er un PDB
cat <<EOF | kubectl apply -f -
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: web-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: web
EOF

# VÃ©rifier le PDB
kubectl get pdb
kubectl describe pdb web-pdb

# Tester l'Ã©vacuation du nÅ“ud (simulation)
# Note : Sur minikube, drain ne fonctionne pas comme sur multi-nÅ“uds
# car il n'y a qu'un seul nÅ“ud, mais on peut voir le PDB

# Voir les pods
kubectl get pods -l app=web

# Le PDB protÃ¨ge contre les Ã©vacuations accidentelles
kubectl get pdb web-pdb -o yaml | grep -A 5 status

# Nettoyer
kubectl delete deployment web
kubectl delete pdb web-pdb
```

**âœ… RÃ©sultat attendu :** Le PDB est crÃ©Ã© et protÃ¨ge les pods

---

### 5. Commandes de gestion des nÅ“uds

```bash
# Lister les nÅ“uds
kubectl get nodes
kubectl get nodes -o wide

# Voir les dÃ©tails d'un nÅ“ud
kubectl describe node minikube

# Voir les ressources
kubectl top node minikube

# Cordon (marquer comme non-planifiable)
kubectl cordon minikube
kubectl get nodes
# STATUS affichera "Ready,SchedulingDisabled"

# Essayer de crÃ©er un pod (restera en Pending)
kubectl run test-cordon --image=nginx:alpine
kubectl get pods test-cordon

# Uncordon (rÃ©activer)
kubectl uncordon minikube
kubectl get nodes

# Le pod devrait maintenant Ãªtre planifiÃ©
kubectl get pods test-cordon

# Nettoyer
kubectl delete pod test-cordon
```

**âœ… RÃ©sultat attendu :**
- Cordon : Nouveaux pods en Pending
- Uncordon : Pods planifiÃ©s normalement

---

### 6. Validation des manifests d'exemples

```bash
# Tester la syntaxe des exemples (dry-run)
kubectl apply --dry-run=client -f examples/node-affinity-examples.yaml
kubectl apply --dry-run=client -f examples/pod-affinity-examples.yaml
kubectl apply --dry-run=client -f examples/taints-tolerations-examples.yaml
kubectl apply --dry-run=client -f examples/poddisruptionbudget-examples.yaml

# Appliquer un exemple simple
kubectl apply -f examples/poddisruptionbudget-examples.yaml

# Voir les ressources crÃ©Ã©es
kubectl get all
kubectl get pdb

# Nettoyer
kubectl delete -f examples/poddisruptionbudget-examples.yaml
```

**âœ… RÃ©sultat attendu :** Tous les manifests sont valides

---

### 7. Exercice 2 : Script de maintenance (adaptÃ©)

```bash
# Le script exercice2-maintenance.sh peut Ãªtre testÃ© sur minikube
# mais avec des adaptations car il n'y a qu'un nÅ“ud

# CrÃ©er un deployment de test
kubectl create deployment test-app --image=nginx:alpine --replicas=3

# Voir les pods
kubectl get pods -o wide

# Cordon
kubectl cordon minikube

# Scale up (les nouveaux pods resteront en Pending)
kubectl scale deployment test-app --replicas=5
kubectl get pods

# Uncordon
kubectl uncordon minikube

# Les pods pending devraient maintenant Ãªtre Running
kubectl get pods

# Nettoyer
kubectl delete deployment test-app
```

**âœ… RÃ©sultat attendu :** Les concepts de maintenance sont dÃ©montrÃ©s

---

## ğŸ“Š RÃ©sumÃ© des tests possibles

| Concept | Testable avec Minikube | Remarques |
|---------|------------------------|-----------|
| Labels | âœ… Oui | Pleinement testable |
| NodeSelectors | âœ… Oui | Pleinement testable |
| Taints | âœ… Oui | Pleinement testable |
| Tolerations | âœ… Oui | Pleinement testable |
| Node Affinity | âœ… Oui | Testable (1 nÅ“ud) |
| Pod Affinity | âš ï¸ LimitÃ© | Pas d'effet visible (1 nÅ“ud) |
| Pod Anti-Affinity | âš ï¸ LimitÃ© | Pas d'effet visible (1 nÅ“ud) |
| PodDisruptionBudgets | âœ… Oui | Pleinement testable |
| Cordon/Uncordon | âœ… Oui | Testable |
| Drain | âš ï¸ LimitÃ© | Fonctionne mais pas reprÃ©sentatif |
| Ajout de nÅ“uds | âŒ Non | Multi-nÅ“uds requis |
| HA Control Plane | âŒ Non | Multi-nÅ“uds requis |
| Load Balancer | âŒ Non | Multi-nÅ“uds requis |

---

## ğŸ¯ Script de test automatisÃ© pour Minikube

```bash
#!/bin/bash
# test-tp9-minikube.sh - Tests des concepts applicables sur minikube

echo "=== Tests TP9 avec Minikube ==="
echo ""

# VÃ©rifier que minikube est dÃ©marrÃ©
if ! minikube status | grep -q "Running"; then
    echo "DÃ©marrage de minikube..."
    minikube start
fi

echo "1. Test des labels"
kubectl label nodes minikube disktype=ssd --overwrite
kubectl get nodes --show-labels | grep disktype && echo "âœ“ Labels OK"

echo ""
echo "2. Test des taints"
kubectl taint nodes minikube test=value:NoSchedule --overwrite
kubectl describe node minikube | grep -A 1 Taints | grep test && echo "âœ“ Taints OK"
kubectl taint nodes minikube test-

echo ""
echo "3. Test PodDisruptionBudget"
kubectl create deployment test --image=nginx:alpine --replicas=3
cat <<EOF | kubectl apply -f -
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: test-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: test
EOF
kubectl get pdb test-pdb && echo "âœ“ PDB crÃ©Ã©"
kubectl delete deployment test
kubectl delete pdb test-pdb

echo ""
echo "4. Test cordon/uncordon"
kubectl cordon minikube
kubectl get nodes | grep SchedulingDisabled && echo "âœ“ Cordon OK"
kubectl uncordon minikube
kubectl get nodes | grep -v SchedulingDisabled && echo "âœ“ Uncordon OK"

echo ""
echo "âœ“ Tous les tests sont passÃ©s !"
```

---

## ğŸ’¡ Conclusion

MÃªme avec Minikube (mono-nÅ“ud), il est possible de tester et comprendre :
- âœ… Les labels et sÃ©lecteurs
- âœ… Les taints et tolerations
- âœ… L'affinitÃ© de nÅ“uds
- âœ… Les PodDisruptionBudgets
- âœ… Les opÃ©rations de maintenance (cordon/uncordon)

Pour tester pleinement le TP9 (multi-nÅ“uds, HA, etc.), il faut :
- Un environnement multi-VMs (VirtualBox, VMware)
- Un cluster cloud (AWS, GCP, Azure)
- Plusieurs machines physiques
- Utiliser les scripts fournis (prepare-node.sh, add-worker-node.sh)

**Le TP9 reste pertinent car il explique comment crÃ©er et gÃ©rer ces environnements !**
