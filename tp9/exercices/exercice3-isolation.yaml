# Exercice 3 : Isolation des workloads par environnement
# Objectif : Utiliser taints et tolerations pour isoler les environnements

# Instructions :
# 1. Préparer les nœuds avec taints et labels :
#
#    kubectl label nodes worker1 environment=production
#    kubectl taint nodes worker1 environment=production:NoSchedule
#
#    kubectl label nodes worker2 environment=staging
#    kubectl taint nodes worker2 environment=staging:NoSchedule
#
#    kubectl label nodes worker3 environment=development
#    kubectl taint nodes worker3 environment=development:NoSchedule
#
# 2. Appliquer ce fichier : kubectl apply -f exercice3-isolation.yaml
#
# 3. Vérifier que chaque application est sur le bon environnement :
#    kubectl get pods -n production -o wide
#    kubectl get pods -n staging -o wide
#    kubectl get pods -n development -o wide
#
# 4. Essayer de déployer un pod sans toleration :
#    kubectl run test --image=nginx:alpine -n production
#    # Devrait rester en Pending
#
# 5. Nettoyer : kubectl delete -f exercice3-isolation.yaml

---
# Namespace Production
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    environment: production

---
# Application Production
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-prod
  namespace: production
  labels:
    app: myapp
    env: production
spec:
  replicas: 5
  selector:
    matchLabels:
      app: myapp
      env: production
  template:
    metadata:
      labels:
        app: myapp
        env: production
    spec:
      # Sélecteur de nœud pour garantir l'environnement
      nodeSelector:
        environment: production
      # Toleration pour le taint
      tolerations:
      - key: "environment"
        operator: "Equal"
        value: "production"
        effect: "NoSchedule"
      containers:
      - name: app
        image: nginx:alpine
        ports:
        - containerPort: 80
        env:
        - name: ENVIRONMENT
          value: "production"
        - name: LOG_LEVEL
          value: "warning"
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "512Mi"
            cpu: "500m"
      # Contraintes de sécurité strictes pour la production
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000

---
# Service Production
apiVersion: v1
kind: Service
metadata:
  name: app-prod
  namespace: production
spec:
  selector:
    app: myapp
    env: production
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP

---
# PDB pour production (strict)
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: app-prod-pdb
  namespace: production
spec:
  minAvailable: 4
  selector:
    matchLabels:
      app: myapp
      env: production

---
# Namespace Staging
apiVersion: v1
kind: Namespace
metadata:
  name: staging
  labels:
    environment: staging

---
# Application Staging
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-staging
  namespace: staging
  labels:
    app: myapp
    env: staging
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      env: staging
  template:
    metadata:
      labels:
        app: myapp
        env: staging
    spec:
      nodeSelector:
        environment: staging
      tolerations:
      - key: "environment"
        operator: "Equal"
        value: "staging"
        effect: "NoSchedule"
      containers:
      - name: app
        image: nginx:alpine
        ports:
        - containerPort: 80
        env:
        - name: ENVIRONMENT
          value: "staging"
        - name: LOG_LEVEL
          value: "info"
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"

---
# Service Staging
apiVersion: v1
kind: Service
metadata:
  name: app-staging
  namespace: staging
spec:
  selector:
    app: myapp
    env: staging
  ports:
  - port: 80
    targetPort: 80
  type: NodePort

---
# PDB pour staging (moins strict)
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: app-staging-pdb
  namespace: staging
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: myapp
      env: staging

---
# Namespace Development
apiVersion: v1
kind: Namespace
metadata:
  name: development
  labels:
    environment: development

---
# Application Development
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-dev
  namespace: development
  labels:
    app: myapp
    env: development
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
      env: development
  template:
    metadata:
      labels:
        app: myapp
        env: development
    spec:
      nodeSelector:
        environment: development
      tolerations:
      - key: "environment"
        operator: "Equal"
        value: "development"
        effect: "NoSchedule"
      containers:
      - name: app
        image: nginx:alpine
        ports:
        - containerPort: 80
        env:
        - name: ENVIRONMENT
          value: "development"
        - name: LOG_LEVEL
          value: "debug"
        - name: DEBUG
          value: "true"
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"

---
# Service Development
apiVersion: v1
kind: Service
metadata:
  name: app-dev
  namespace: development
spec:
  selector:
    app: myapp
    env: development
  ports:
  - port: 80
    targetPort: 80
  type: NodePort

---
# Pas de PDB pour development (drainage libre)
# Le développement peut tolérer des interruptions

---
# Pod admin qui peut accéder à tous les environnements
apiVersion: v1
kind: Pod
metadata:
  name: admin-debug
  namespace: default
  labels:
    role: admin
spec:
  # Tolère tous les taints d'environnement
  tolerations:
  - key: "environment"
    operator: "Exists"
    effect: "NoSchedule"
  containers:
  - name: debug
    image: nicolaka/netshoot
    command: ['sh', '-c', 'sleep infinity']
    resources:
      requests:
        memory: "64Mi"
        cpu: "50m"

# Commandes de vérification :
#
# 1. Vérifier la distribution des pods :
#    kubectl get pods --all-namespaces -o wide | grep -E 'production|staging|development'
#
# 2. Vérifier les nœuds et leurs taints :
#    kubectl describe nodes | grep -A 5 Taints
#
# 3. Test d'isolation - Essayer de créer un pod sans toleration :
#    kubectl run test-no-toleration --image=nginx:alpine -n production
#    kubectl describe pod test-no-toleration -n production
#    # Devrait afficher "0/3 nodes are available: 3 node(s) had taint..."
#    kubectl delete pod test-no-toleration -n production
#
# 4. Vérifier les PDBs :
#    kubectl get pdb --all-namespaces
#
# 5. Tester le pod admin :
#    kubectl get pod admin-debug -o wide
#    # Devrait pouvoir être planifié sur n'importe quel nœud
#
# 6. Nettoyer :
#    kubectl delete -f exercice3-isolation.yaml
#    kubectl taint nodes worker1 environment-
#    kubectl taint nodes worker2 environment-
#    kubectl taint nodes worker3 environment-
