# Gestion d'un Cluster Kubernetes en Environnement SÃ©curisÃ©

## Vue d'ensemble

Ce document couvre les meilleures pratiques pour gÃ©rer un cluster Kubernetes dans un environnement hautement sÃ©curisÃ©, notamment en DMZ (Zone DÃ©militarisÃ©e) ou dans des environnements isolÃ©s du rÃ©seau.

## ğŸ”’ Principes de SÃ©curitÃ© en Environnement HermÃ©tique

### CaractÃ©ristiques d'un Environnement SÃ©curisÃ©

Un environnement Kubernetes sÃ©curisÃ© prÃ©sente gÃ©nÃ©ralement les caractÃ©ristiques suivantes :

- **Isolation rÃ©seau stricte** : AccÃ¨s limitÃ© Ã  Internet et aux rÃ©seaux externes
- **DMZ** : Cluster dÃ©ployÃ© dans une zone dÃ©militarisÃ©e
- **Air-gapped** : Environnement totalement dÃ©connectÃ© d'Internet
- **ContrÃ´le des flux** : Tous les flux rÃ©seau doivent Ãªtre explicitement autorisÃ©s
- **TraÃ§abilitÃ©** : Logging et audit complets de toutes les opÃ©rations

### Architecture Typique en DMZ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Internet                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                      [Firewall]
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          â”‚                                   â”‚
â”‚                     [Proxy/WAF]                              â”‚
â”‚                          â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚                  DMZ (Zone 1)                   â”‚         â”‚
â”‚  â”‚                       â”‚                         â”‚         â”‚
â”‚  â”‚              [Ingress Controllers]              â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                          â”‚                                   â”‚
â”‚                    [Firewall Interne]                        â”‚
â”‚                          â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚             Cluster Kubernetes                  â”‚         â”‚
â”‚  â”‚                  (Zone SÃ©curisÃ©e)               â”‚         â”‚
â”‚  â”‚                                                  â”‚         â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚         â”‚
â”‚  â”‚  â”‚  Master  â”‚  â”‚  Master  â”‚  â”‚  Master  â”‚     â”‚         â”‚
â”‚  â”‚  â”‚   Node   â”‚  â”‚   Node   â”‚  â”‚   Node   â”‚     â”‚         â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚         â”‚
â”‚  â”‚                                                  â”‚         â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚         â”‚
â”‚  â”‚  â”‚  Worker  â”‚  â”‚  Worker  â”‚  â”‚  Worker  â”‚     â”‚         â”‚
â”‚  â”‚  â”‚   Node   â”‚  â”‚   Node   â”‚  â”‚   Node   â”‚     â”‚         â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚         â”‚
â”‚  â”‚                                                  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ StratÃ©gies de DÃ©ploiement en Environnement SÃ©curisÃ©

### 1. Registre d'Images PrivÃ©

**Obligatoire** : Un registre d'images privÃ© est essentiel en environnement sÃ©curisÃ©.

#### Solutions RecommandÃ©es

| Solution | Avantages | InconvÃ©nients | Cas d'usage |
|----------|-----------|---------------|-------------|
| **Harbor** | - Open source<br>- Scan de vulnÃ©rabilitÃ©s intÃ©grÃ©<br>- RÃ©plication<br>- Gestion RBAC | - Ressources importantes | Production enterprise |
| **Nexus Repository** | - Multi-format (Docker, Helm, Maven, etc.)<br>- Proxy cache<br>- MaturitÃ© | - Interface moins moderne | Environnements polyvalents |
| **JFrog Artifactory** | - Performant<br>- IntÃ©gration CI/CD<br>- Support commercial | - CoÃ»t Ã©levÃ© | Grandes entreprises |
| **Docker Registry** | - Simple<br>- LÃ©ger<br>- Facile Ã  dÃ©ployer | - FonctionnalitÃ©s limitÃ©es<br>- Pas de UI | Dev/test, petits clusters |
| **GitLab Container Registry** | - IntÃ©grÃ© Ã  GitLab<br>- CI/CD natif | - DÃ©pendance Ã  GitLab | Si dÃ©jÃ  utilisateur GitLab |

#### Exemple de DÃ©ploiement Harbor en DMZ

```yaml
# harbor-values.yaml
expose:
  type: loadBalancer
  tls:
    enabled: true
    certSource: secret
    secret:
      secretName: harbor-tls

externalURL: https://harbor.internal.company.com

persistence:
  enabled: true
  persistentVolumeClaim:
    registry:
      size: 500Gi
    database:
      size: 10Gi
    redis:
      size: 1Gi

trivy:
  enabled: true

clair:
  enabled: false

notary:
  enabled: true

chartmuseum:
  enabled: true
```

DÃ©ploiement :
```bash
helm repo add harbor https://helm.goharbor.io
helm install harbor harbor/harbor \
  --namespace harbor \
  --create-namespace \
  -f harbor-values.yaml
```

### 2. Gestion des Certificats TLS

En environnement sÃ©curisÃ©, les certificats TLS sont cruciaux.

#### Solutions de Gestion des Certificats

**Option 1 : PKI Interne avec cert-manager**

```yaml
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: internal-ca-issuer
spec:
  ca:
    secretName: internal-ca-key-pair
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: harbor-tls
  namespace: harbor
spec:
  secretName: harbor-tls
  issuerRef:
    name: internal-ca-issuer
    kind: ClusterIssuer
  dnsNames:
    - harbor.internal.company.com
  duration: 2160h  # 90 jours
  renewBefore: 360h  # 15 jours avant expiration
```

**Option 2 : Vault pour la Gestion des Secrets**

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: app-vault-auth
  namespace: production
---
apiVersion: secrets.hashicorp.com/v1beta1
kind: VaultAuth
metadata:
  name: vault-auth
  namespace: production
spec:
  method: kubernetes
  mount: kubernetes
  kubernetes:
    role: production-role
    serviceAccount: app-vault-auth
```

### 3. Network Policies Strictes

En DMZ, tous les flux doivent Ãªtre explicitement autorisÃ©s.

#### Policy par DÃ©faut : Deny All

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
```

#### Autoriser uniquement les flux nÃ©cessaires

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend-to-backend
  namespace: production
spec:
  podSelector:
    matchLabels:
      tier: backend
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tier: frontend
    ports:
    - protocol: TCP
      port: 8080
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-backend-to-database
  namespace: production
spec:
  podSelector:
    matchLabels:
      tier: backend
  policyTypes:
  - Egress
  egress:
  - to:
    - podSelector:
        matchLabels:
          tier: database
    ports:
    - protocol: TCP
      port: 5432
  # DNS resolution
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
```

### 4. ContrÃ´le d'AccÃ¨s RBAC Strict

#### Principe du Moindre PrivilÃ¨ge

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pod-reader
  namespace: production
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: production
subjects:
- kind: ServiceAccount
  name: monitoring-sa
  namespace: production
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```

#### Audit des Permissions

```bash
# VÃ©rifier les permissions d'un ServiceAccount
kubectl auth can-i --list \
  --as=system:serviceaccount:production:app-sa \
  -n production

# Trouver tous les ClusterRoleBindings avec des permissions admin
kubectl get clusterrolebindings -o json | \
  jq '.items[] | select(.roleRef.name=="cluster-admin") | .metadata.name'
```

## ğŸ” SÃ©curitÃ© au Niveau du Cluster

### 1. API Server SÃ©curisÃ©

Configuration recommandÃ©e pour le kube-apiserver :

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - name: kube-apiserver
    command:
    - kube-apiserver
    # Authentification
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key

    # Audit
    - --audit-log-path=/var/log/kubernetes/audit.log
    - --audit-log-maxage=30
    - --audit-log-maxbackup=10
    - --audit-log-maxsize=100
    - --audit-policy-file=/etc/kubernetes/audit-policy.yaml

    # SÃ©curitÃ©
    - --enable-admission-plugins=NodeRestriction,PodSecurityPolicy,ServiceAccount
    - --authorization-mode=Node,RBAC
    - --anonymous-auth=false

    # Encryption at rest
    - --encryption-provider-config=/etc/kubernetes/encryption-config.yaml
```

### 2. Encryption at Rest

```yaml
# /etc/kubernetes/encryption-config.yaml
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
      - secrets
      - configmaps
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: <base64-encoded-secret>
      - identity: {}
```

### 3. Pod Security Standards

**Utiliser Pod Security Admission (PSA)** au lieu de PodSecurityPolicy (dÃ©prÃ©ciÃ©)

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
```

Niveaux de sÃ©curitÃ© :
- **Privileged** : Non restreint
- **Baseline** : EmpÃªche les escalations de privilÃ¨ges connues
- **Restricted** : Fortement restreint (best practices)

## ğŸ“Š Monitoring et Logging en Environnement SÃ©curisÃ©

### Stack de Monitoring RecommandÃ©e

```yaml
# Prometheus dans un namespace dÃ©diÃ©
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
  labels:
    name: monitoring
---
# ServiceMonitor pour scraper les mÃ©triques
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kubernetes-apiservers
  namespace: monitoring
spec:
  endpoints:
  - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    interval: 30s
    port: https
    scheme: https
    tlsConfig:
      caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      serverName: kubernetes
  jobLabel: component
  namespaceSelector:
    matchNames:
    - default
  selector:
    matchLabels:
      component: apiserver
      provider: kubernetes
```

### Centralisation des Logs

**Option 1 : Stack EFK (Elasticsearch, Fluentd, Kibana)**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: kube-system
data:
  fluent.conf: |
    <source>
      @type tail
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      tag kubernetes.*
      read_from_head true
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>

    <filter kubernetes.**>
      @type kubernetes_metadata
    </filter>

    <match **>
      @type elasticsearch
      host elasticsearch.logging.svc.cluster.local
      port 9200
      logstash_format true
      logstash_prefix kubernetes
      <buffer>
        @type file
        path /var/log/fluentd-buffers/kubernetes.system.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_interval 5s
        retry_max_interval 30
        chunk_limit_size 2M
        total_limit_size 500M
        overflow_action block
      </buffer>
    </match>
```

**Option 2 : Loki (plus lÃ©ger)**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: promtail-config
  namespace: logging
data:
  promtail.yaml: |
    server:
      http_listen_port: 3101

    clients:
      - url: http://loki.logging.svc.cluster.local:3100/loki/api/v1/push

    positions:
      filename: /tmp/positions.yaml

    scrape_configs:
      - job_name: kubernetes-pods
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_node_name]
            target_label: node_name
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod
          - source_labels: [__meta_kubernetes_pod_container_name]
            target_label: container
```

## ğŸ”„ Mise Ã  Jour et Maintenance

### StratÃ©gie de Mise Ã  Jour en Production

**Approche Blue/Green pour les Applications Critiques**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend
  namespace: production
spec:
  selector:
    app: frontend
    version: blue  # Switcher entre blue/green
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend-blue
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend
      version: blue
  template:
    metadata:
      labels:
        app: frontend
        version: blue
    spec:
      containers:
      - name: frontend
        image: harbor.internal/app/frontend:v1.2.0
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend-green
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend
      version: green
  template:
    metadata:
      labels:
        app: frontend
        version: green
    spec:
      containers:
      - name: frontend
        image: harbor.internal/app/frontend:v1.3.0
```

Switch de version :
```bash
# Basculer vers green
kubectl patch service frontend -n production \
  -p '{"spec":{"selector":{"version":"green"}}}'

# Rollback vers blue si problÃ¨me
kubectl patch service frontend -n production \
  -p '{"spec":{"selector":{"version":"blue"}}}'
```

### Mise Ã  Jour du Cluster

**Ordre recommandÃ©** :
1. Backup etcd
2. Mise Ã  jour des masters (un par un)
3. Mise Ã  jour des workers (par groupes)
4. Validation post-upgrade

```bash
# 1. Backup etcd
ETCDCTL_API=3 etcdctl snapshot save /backup/etcd-snapshot-$(date +%Y%m%d-%H%M%S).db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key

# 2. Drain node
kubectl drain node1 --ignore-daemonsets --delete-emptydir-data

# 3. Upgrade kubeadm
apt-mark unhold kubeadm && \
apt-get update && apt-get install -y kubeadm=1.29.0-00 && \
apt-mark hold kubeadm

# 4. Upgrade node
kubeadm upgrade apply v1.29.0  # Sur master
kubeadm upgrade node           # Sur workers

# 5. Upgrade kubelet et kubectl
apt-mark unhold kubelet kubectl && \
apt-get update && apt-get install -y kubelet=1.29.0-00 kubectl=1.29.0-00 && \
apt-mark hold kubelet kubectl

# 6. Restart kubelet
systemctl daemon-reload
systemctl restart kubelet

# 7. Uncordon node
kubectl uncordon node1
```

## ğŸ“‹ Checklist de SÃ©curitÃ©

### Niveau Cluster

- [ ] API server accessible uniquement via VPN/bastion
- [ ] Certificats TLS pour tous les composants
- [ ] Encryption at rest activÃ©e
- [ ] Audit logging activÃ©
- [ ] RBAC configurÃ© avec principe du moindre privilÃ¨ge
- [ ] Pod Security Standards appliquÃ©s
- [ ] Network Policies par dÃ©faut (deny all)
- [ ] Secrets chiffrÃ©s avec KMS ou Vault

### Niveau Application

- [ ] Images provenant uniquement du registre privÃ©
- [ ] Scan de vulnÃ©rabilitÃ©s automatique
- [ ] Pas de privileged containers
- [ ] Resource limits dÃ©finis
- [ ] Health checks configurÃ©s
- [ ] Service mesh pour mTLS (optionnel)
- [ ] NetworkPolicies spÃ©cifiques Ã  l'application

### Niveau OpÃ©rationnel

- [ ] Monitoring complet (mÃ©triques + logs + traces)
- [ ] Alerting configurÃ©
- [ ] Backup automatique etcd
- [ ] Plan de reprise d'activitÃ© (DRP)
- [ ] ProcÃ©dure de mise Ã  jour documentÃ©e
- [ ] Tests de sÃ©curitÃ© rÃ©guliers (pentests)

## ğŸ“š RÃ©fÃ©rences

- [CIS Kubernetes Benchmark](https://www.cisecurity.org/benchmark/kubernetes)
- [NSA Kubernetes Hardening Guide](https://www.nsa.gov/Press-Room/News-Highlights/Article/Article/2716980/nsa-cisa-release-kubernetes-hardening-guidance/)
- [OWASP Kubernetes Security Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/Kubernetes_Security_Cheat_Sheet.html)
- [Kubernetes Network Policy Recipes](https://github.com/ahmetb/kubernetes-network-policy-recipes)

## Articles ComplÃ©mentaires

- [Gestion des Registres d'Images en DMZ](IMAGE_REGISTRY_DMZ.md)
- [Cycle de Vie des Applications](APPLICATION_LIFECYCLE.md)
- [DÃ©ploiement Air-Gapped](AIRGAP_DEPLOYMENT.md)
