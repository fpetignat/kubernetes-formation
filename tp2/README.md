# TP2 - Ma√Ætriser les Manifests Kubernetes

## Objectifs du TP

√Ä la fin de ce TP, vous serez capable de :
- Comprendre la structure des fichiers YAML Kubernetes
- √âcrire vos propres manifests pour diff√©rentes ressources
- Valider et tester vos configurations YAML
- Utiliser les labels et selectors efficacement
- G√©rer la configuration avec ConfigMaps et Secrets
- Appliquer les bonnes pratiques de r√©daction de manifests

## Pr√©requis

- Avoir compl√©t√© le TP1
- Un cluster Kubernetes fonctionnel (**minikube** ou **kubeadm**)
- Un √©diteur de texte (vim, nano, VS Code, etc.)

**Note :** Les manifests YAML sont identiques que vous utilisiez minikube ou kubeadm. Les diff√©rences se situent uniquement au niveau de l'acc√®s aux services (voir TP1, Partie 5.2).

## Partie 1 : Anatomie d'un manifest Kubernetes

### 1.1 Structure de base

Tous les manifests Kubernetes suivent la m√™me structure de base :

```yaml
apiVersion: <groupe>/<version>  # Version de l'API Kubernetes
kind: <Type>                    # Type de ressource
metadata:                       # M√©tadonn√©es
  name: <nom>
  labels:
    key: value
spec:                          # Sp√©cification de la ressource
  # Configuration sp√©cifique au type
```

### 1.2 Les champs essentiels

**apiVersion** : D√©termine quelle version de l'API utiliser
- `v1` : pour Pod, Service, ConfigMap, Secret
- `apps/v1` : pour Deployment, StatefulSet, DaemonSet
- `batch/v1` : pour Job, CronJob

**kind** : Type de ressource √† cr√©er
- Pod, Service, Deployment, ConfigMap, Secret, etc.

**metadata** : Informations sur la ressource
- `name` : Nom unique dans le namespace
- `labels` : Paires cl√©-valeur pour identifier et s√©lectionner les ressources
- `namespace` : Namespace o√π cr√©er la ressource (d√©faut: default)
- `annotations` : M√©tadonn√©es non-identifiantes

**spec** : D√©finit l'√©tat d√©sir√© de la ressource

### 1.3 Validation d'un manifest

```bash
# V√©rifier la syntaxe sans cr√©er la ressource
kubectl apply -f mon-fichier.yaml --dry-run=client

# Valider c√¥t√© serveur
kubectl apply -f mon-fichier.yaml --dry-run=server

# Afficher le YAML d'une ressource existante
kubectl get deployment nginx-demo -o yaml

# Expliquer la structure d'une ressource
kubectl explain pod
kubectl explain pod.spec
kubectl explain pod.spec.containers
```

## Partie 2 : Les Pods - La plus petite unit√©

### 2.1 Pod simple

Cr√©er un fichier `01-simple-pod.yaml` :

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    app: nginx
    env: dev
spec:
  containers:
  - name: nginx
    image: nginx:1.24
    ports:
    - containerPort: 80
```

**Exercice 1 : Votre premier Pod**

1. Cr√©ez le fichier ci-dessus
2. Validez-le avec `--dry-run=client`
3. Appliquez-le avec `kubectl apply`
4. V√©rifiez son statut avec `kubectl get pods`
5. Consultez ses d√©tails avec `kubectl describe pod nginx-pod`

```bash
# Commandes √† ex√©cuter
kubectl apply -f 01-simple-pod.yaml --dry-run=client
kubectl apply -f 01-simple-pod.yaml
kubectl get pods
kubectl describe pod nginx-pod
```

### 2.2 Pod avec ressources limit√©es

Cr√©er `02-pod-with-resources.yaml` :

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: webapp-pod
  labels:
    app: webapp
spec:
  containers:
  - name: webapp
    image: httpd:2.4
    ports:
    - containerPort: 80
    resources:
      requests:      # Ressources minimales garanties
        memory: "64Mi"
        cpu: "250m"
      limits:        # Ressources maximales
        memory: "128Mi"
        cpu: "500m"
```

**Exercice 2 : Pod avec contraintes de ressources**

1. Cr√©ez ce fichier
2. Appliquez-le
3. V√©rifiez les ressources allou√©es : `kubectl describe pod webapp-pod`
4. Observez la section "Requests" et "Limits"

### 2.3 Pod multi-conteneurs

Cr√©er `03-multi-container-pod.yaml` :

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: multi-container-pod
spec:
  containers:
  - name: nginx
    image: nginx:alpine
    ports:
    - containerPort: 80
    volumeMounts:
    - name: shared-data
      mountPath: /usr/share/nginx/html

  - name: content-generator
    image: busybox
    command: ["/bin/sh"]
    args:
      - -c
      - >
        while true; do
          echo "Hello from Kubernetes - $(date)" > /data/index.html;
          sleep 10;
        done
    volumeMounts:
    - name: shared-data
      mountPath: /data

  volumes:
  - name: shared-data
    emptyDir: {}
```

**Exercice 3 : Pod avec sidecar**

1. Cr√©ez et appliquez ce manifest
2. V√©rifiez que les 2 conteneurs tournent : `kubectl get pod multi-container-pod`
3. Consultez les logs de chaque conteneur :
   ```bash
   kubectl logs multi-container-pod -c nginx
   kubectl logs multi-container-pod -c content-generator
   ```
4. Testez l'application avec un port-forward :
   ```bash
   kubectl port-forward pod/multi-container-pod 8080:80
   curl localhost:8080
   ```

## Partie 3 : Deployments - Gestion des r√©plicas

### 3.1 Deployment de base

Cr√©er `04-deployment-basic.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-deployment
  labels:
    app: web
spec:
  replicas: 3
  selector:
    matchLabels:      # DOIT correspondre aux labels du template
      app: web
      env: prod
  template:           # Template du Pod
    metadata:
      labels:
        app: web
        env: prod
    spec:
      containers:
      - name: nginx
        image: nginx:1.24
        ports:
        - containerPort: 80
```

**Exercice 4 : D√©ploiement avec r√©plicas**

1. Cr√©ez ce deployment
2. V√©rifiez les pods cr√©√©s : `kubectl get pods -l app=web`
3. Supprimez un pod manuellement et observez la recr√©ation automatique
4. Modifiez le nombre de replicas dans le fichier √† 5
5. R√©appliquez : `kubectl apply -f 04-deployment-basic.yaml`

### 3.2 Deployment avec strat√©gie de mise √† jour

Cr√©er `05-deployment-strategy.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rolling-deployment
spec:
  replicas: 4
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1        # Nombre de pods suppl√©mentaires pendant la mise √† jour
      maxUnavailable: 1  # Nombre de pods indisponibles pendant la mise √† jour
  selector:
    matchLabels:
      app: rolling-app
  template:
    metadata:
      labels:
        app: rolling-app
        version: v1
    spec:
      containers:
      - name: app
        image: nginx:1.24
        ports:
        - containerPort: 80
        livenessProbe:    # V√©rification que le conteneur est vivant
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 10
        readinessProbe:   # V√©rification que le conteneur est pr√™t
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
```

**Exercice 5 : Rolling Update**

1. Cr√©ez ce deployment
2. Surveillez les pods : `kubectl get pods -l app=rolling-app -w`
3. Dans un autre terminal, mettez √† jour l'image :
   ```bash
   kubectl set image deployment/rolling-deployment app=nginx:1.25
   ```
4. Observez le rolling update en cours
5. Consultez l'historique : `kubectl rollout history deployment/rolling-deployment`
6. Effectuez un rollback : `kubectl rollout undo deployment/rolling-deployment`

## Partie 4 : Services - Exposition des applications

### 4.1 Service ClusterIP

Cr√©er `06-service-clusterip.yaml` :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: internal-service
spec:
  type: ClusterIP    # Accessible uniquement dans le cluster
  selector:
    app: web
  ports:
  - protocol: TCP
    port: 80         # Port du service
    targetPort: 80   # Port du conteneur
```

### 4.2 Service NodePort

Cr√©er `07-service-nodeport.yaml` :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: external-service
spec:
  type: NodePort
  selector:
    app: web
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
    nodePort: 30080  # Port accessible sur chaque n≈ìud (30000-32767)
```

### 4.3 Service avec annotations

Cr√©er `08-service-complete.yaml` :

```yaml
apiVersion: v1
kind: Service
metadata:
  name: app-service
  annotations:
    description: "Service principal de l'application"
  labels:
    env: production
spec:
  type: NodePort
  selector:
    app: web
    env: prod
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 80
  sessionAffinity: ClientIP  # Maintenir la session sur le m√™me pod
```

**Exercice 6 : Cr√©ation de services**

1. Cr√©ez les trois types de services ci-dessus
2. V√©rifiez avec : `kubectl get services`
3. Testez l'acc√®s au service NodePort :

   **Avec minikube :**
   ```bash
   curl http://$(minikube ip):30080
   ```

   **Avec kubeadm :**
   ```bash
   # R√©cup√©rer l'IP d'un n≈ìud
   NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')
   curl http://$NODE_IP:30080
   ```

4. Affichez les endpoints : `kubectl get endpoints`
5. D√©crivez le service : `kubectl describe service app-service`

## Partie 5 : ConfigMaps et Secrets

### 5.1 ConfigMap simple

Cr√©er `09-configmap.yaml` :

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  # Donn√©es de configuration sous forme cl√©-valeur
  database_url: "postgres://db:5432/myapp"
  log_level: "info"
  max_connections: "100"

  # Configuration multi-lignes
  app.conf: |
    server {
      listen 80;
      server_name localhost;

      location / {
        root /usr/share/nginx/html;
        index index.html;
      }
    }
```

### 5.2 Secret

Cr√©er `10-secret.yaml` :

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: app-secret
type: Opaque
stringData:  # Les donn√©es seront automatiquement encod√©es en base64
  username: admin
  password: supersecret123
  api-key: abcd1234efgh5678
```

### 5.3 Pod utilisant ConfigMap et Secret

Cr√©er `11-pod-with-config.yaml` :

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: configured-app
spec:
  containers:
  - name: app
    image: nginx:alpine

    # Variables d'environnement depuis ConfigMap
    env:
    - name: DATABASE_URL
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: database_url

    - name: LOG_LEVEL
      valueFrom:
        configMapKeyRef:
          name: app-config
          key: log_level

    # Variables d'environnement depuis Secret
    - name: USERNAME
      valueFrom:
        secretKeyRef:
          name: app-secret
          key: username

    - name: PASSWORD
      valueFrom:
        secretKeyRef:
          name: app-secret
          key: password

    # Monter le ConfigMap comme volume
    volumeMounts:
    - name: config-volume
      mountPath: /etc/config

    # Monter le Secret comme volume
    - name: secret-volume
      mountPath: /etc/secret
      readOnly: true

  volumes:
  - name: config-volume
    configMap:
      name: app-config

  - name: secret-volume
    secret:
      secretName: app-secret
```

**Exercice 7 : Configuration externalis√©e**

1. Cr√©ez le ConfigMap et le Secret
2. Cr√©ez le Pod qui les utilise
3. V√©rifiez les variables d'environnement :
   ```bash
   kubectl exec configured-app -- env | grep -E "(DATABASE_URL|USERNAME)"
   ```
4. V√©rifiez les fichiers mont√©s :
   ```bash
   kubectl exec configured-app -- ls /etc/config
   kubectl exec configured-app -- cat /etc/config/app.conf
   kubectl exec configured-app -- ls /etc/secret
   ```

### 5.4 ‚ö†Ô∏è LIMITES DE S√âCURIT√â DES SECRETS KUBERNETES

**IMPORTANT :** Les Secrets Kubernetes ne sont PAS une solution de s√©curit√© robuste par d√©faut. Voici les limites critiques √† conna√Ætre :

#### 5.4.1 Encodage vs Chiffrement

```bash
# Les Secrets sont encod√©s en base64, PAS chiffr√©s
echo "supersecret123" | base64
# R√©sultat : c3VwZXJzZWNyZXQxMjM=

# Ils peuvent √™tre d√©cod√©s facilement
echo "c3VwZXJzZWNyZXQxMjM=" | base64 -d
# R√©sultat : supersecret123
```

**‚ö†Ô∏è Risque :** N'importe qui ayant acc√®s au manifest YAML peut d√©coder les secrets encod√©s en base64.

#### 5.4.2 Stockage en clair dans etcd

**Par d√©faut**, les Secrets sont stock√©s **en clair** dans etcd (la base de donn√©es de Kubernetes).

```bash
# V√©rifier si l'encryption at rest est activ√©e
kubectl get secret app-secret -o yaml

# Le champ 'data' contient les valeurs en base64 seulement
```

**‚ö†Ô∏è Risque :**
- Toute personne ayant acc√®s √† etcd peut lire tous les secrets
- Les backups etcd contiennent les secrets en clair
- Un compromis du serveur etcd expose tous les secrets

**Solution :** Activer l'[Encryption at Rest](https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/) dans la configuration du cluster.

#### 5.4.3 Acc√®s via l'API Kubernetes

```bash
# Toute personne avec les permissions RBAC appropri√©es peut lire les secrets
kubectl get secret app-secret -o yaml
kubectl get secret app-secret -o jsonpath='{.data.password}' | base64 -d
```

**‚ö†Ô∏è Risque :**
- Un compte de service compromis avec les bonnes permissions peut lire tous les secrets
- Les permissions par d√©faut peuvent √™tre trop permissives

**Bonnes pratiques :**
```yaml
# Limiter l'acc√®s aux secrets avec RBAC
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: secret-reader
rules:
- apiGroups: [""]
  resources: ["secrets"]
  resourceNames: ["app-secret"]  # Limiter √† des secrets sp√©cifiques
  verbs: ["get"]
```

#### 5.4.4 Secrets mont√©s comme volumes

Quand un Secret est mont√© comme volume dans un Pod :

```bash
# Le secret est √©crit en clair sur le disque du n≈ìud
kubectl exec configured-app -- cat /etc/secret/password
# Affiche : supersecret123
```

**‚ö†Ô∏è Risques :**
- Les fichiers sont visibles sur le syst√®me de fichiers du n≈ìud (dans `/var/lib/kubelet/pods/...`)
- Un acc√®s SSH au n≈ìud permet de lire les secrets
- Les secrets restent sur le disque m√™me apr√®s la suppression du Pod

#### 5.4.5 Secrets dans les variables d'environnement

```yaml
env:
- name: PASSWORD
  valueFrom:
    secretKeyRef:
      name: app-secret
      key: password
```

**‚ö†Ô∏è Risques CRITIQUES :**
- Les variables d'environnement sont visibles dans `kubectl describe pod`
- Elles apparaissent dans les logs syst√®me et d'audit
- Les processus enfants h√©ritent des variables d'environnement
- Elles peuvent √™tre logu√©es involontairement par l'application

```bash
# Les variables d'environnement sont visibles !
kubectl exec configured-app -- env | grep PASSWORD
# Affiche : PASSWORD=supersecret123

# Elles apparaissent aussi dans describe
kubectl describe pod configured-app
# On peut voir les r√©f√©rences aux secrets (mais pas les valeurs directement)
```

**Recommandation :** Pr√©f√©rer les volumes aux variables d'environnement pour les secrets sensibles.

#### 5.4.6 Secrets dans Git

**‚ùå JAMAIS faire cela :**
```yaml
# Ne JAMAIS commiter ce fichier dans Git !
apiVersion: v1
kind: Secret
metadata:
  name: bad-secret
stringData:
  password: "supersecret123"  # Visible dans l'historique Git !
```

**‚ö†Ô∏è Risques :**
- Une fois dans Git, le secret reste dans l'historique m√™me si supprim√©
- Les forks et clones du d√©p√¥t contiennent le secret
- Les outils d'analyse de code peuvent d√©tecter et signaler les secrets

**Bonnes pratiques :**
```bash
# Ajouter les fichiers de secrets au .gitignore
echo "*-secret.yaml" >> .gitignore
echo "secrets/" >> .gitignore
```

#### 5.4.7 Solutions alternatives plus s√©curis√©es

Pour une s√©curit√© renforc√©e, consid√©rez ces solutions :

**1. Sealed Secrets (Bitnami)**
```bash
# Les secrets sont chiffr√©s et peuvent √™tre stock√©s dans Git
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: mysecret
spec:
  encryptedData:
    password: AgBpDH7X9k2... # Chiffr√©, safe pour Git
```

**2. External Secrets Operator**
```yaml
# Synchronise les secrets depuis un gestionnaire externe
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: app-secret
spec:
  secretStoreRef:
    name: vault-backend
  target:
    name: app-secret
  data:
  - secretKey: password
    remoteRef:
      key: secret/data/myapp
      property: password
```

**3. HashiCorp Vault**
- Gestionnaire de secrets d√©di√©
- Chiffrement, rotation automatique, audit
- Int√©gration avec Kubernetes via CSI driver

**4. Cloud Provider Secret Managers**
- AWS Secrets Manager
- Azure Key Vault
- Google Secret Manager

#### 5.4.8 Checklist de s√©curit√© pour les Secrets

Avant d'utiliser un Secret en production :

- [ ] ‚ùå Ne pas commiter les Secrets dans Git
- [ ] ‚úÖ Activer l'encryption at rest dans etcd
- [ ] ‚úÖ Utiliser RBAC pour limiter l'acc√®s aux Secrets
- [ ] ‚úÖ Pr√©f√©rer les volumes aux variables d'environnement
- [ ] ‚úÖ Auditer r√©guli√®rement les acc√®s aux Secrets
- [ ] ‚úÖ Utiliser des namespaces pour l'isolation
- [ ] ‚úÖ Consid√©rer des solutions externes (Vault, Sealed Secrets)
- [ ] ‚úÖ Activer les logs d'audit Kubernetes
- [ ] ‚úÖ Rotation r√©guli√®re des secrets
- [ ] ‚úÖ Scanner les d√©p√¥ts Git pour d√©tecter les secrets expos√©s

#### 5.4.9 Exemple de v√©rification de s√©curit√©

```bash
# V√©rifier les permissions sur les secrets
kubectl auth can-i get secrets --as=system:serviceaccount:default:default

# Lister tous les secrets dans un namespace
kubectl get secrets -n default

# Auditer qui a acc√®s aux secrets
kubectl get rolebindings,clusterrolebindings -A -o json | \
  jq '.items[] | select(.roleRef.kind=="Role" or .roleRef.kind=="ClusterRole") |
  select(.subjects[]?.kind=="ServiceAccount")'

# V√©rifier si l'encryption at rest est configur√©e
# (n√©cessite l'acc√®s au serveur API)
kubectl get configmap -n kube-system kube-apiserver-config -o yaml | grep -i encrypt
```

#### 5.4.10 R√©sum√© des risques

| Risque | Niveau | Mitigation |
|--------|--------|------------|
| Secrets en base64 seulement | üî¥ Critique | Utiliser des solutions de chiffrement |
| Stockage en clair dans etcd | üî¥ Critique | Activer encryption at rest |
| Acc√®s via API K8s | üü° Moyen | RBAC strict + audit |
| Secrets dans variables env | üü° Moyen | Pr√©f√©rer les volumes |
| Secrets dans Git | üî¥ Critique | .gitignore + Git scanning |
| Secrets sur disque n≈ìud | üü° Moyen | S√©curiser l'acc√®s SSH aux n≈ìuds |

**Conclusion :** Les Secrets Kubernetes sont un m√©canisme de base pour g√©rer les donn√©es sensibles, mais ils n√©cessitent des mesures de s√©curit√© suppl√©mentaires pour une utilisation en production. Pour des environnements critiques, privil√©giez des solutions d√©di√©es comme Vault ou les gestionnaires de secrets cloud.

## Partie 6 : Labels et Selectors - Ma√Ætrise compl√®te

### 6.1 Introduction aux labels

Les **labels** sont des paires cl√©-valeur attach√©es aux objets Kubernetes (Pods, Services, Deployments, etc.). Ils sont fondamentaux pour :
- **Organiser** : Grouper et cat√©goriser les ressources
- **S√©lectionner** : Identifier des ensembles de ressources
- **Connecter** : Lier les Services aux Pods, les Deployments aux Pods, etc.
- **Filtrer** : Interroger et manipuler des groupes de ressources

**Syntaxe et contraintes :**
```yaml
labels:
  key: value              # Format de base
  app: nginx              # Nom d'application
  env: production         # Environnement
  version: v1.2.3         # Version
  tier: frontend          # Tier architectural
```

**R√®gles de nommage :**
- Cl√©s : max 63 caract√®res (pr√©fixe optionnel jusqu'√† 253 caract√®res + `/`)
- Valeurs : max 63 caract√®res
- Caract√®res autoris√©s : alphanum√©riques, `-`, `_`, `.`
- Doit commencer et finir par un caract√®re alphanum√©rique

### 6.2 matchLabels : S√©lection simple

`matchLabels` effectue une correspondance **exacte** sur toutes les paires cl√©-valeur sp√©cifi√©es (AND logique).

**Exemple 1 : Deployment avec matchLabels simple**

Cr√©er `12a-deployment-matchlabels.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-deployment
  labels:
    app: webapp
    tier: frontend
spec:
  replicas: 3
  selector:
    matchLabels:      # S√©lection EXACTE
      app: webapp     # Le pod DOIT avoir app=webapp
      tier: frontend  # ET tier=frontend
  template:
    metadata:
      labels:
        app: webapp
        tier: frontend
        version: v1.0.0      # Labels suppl√©mentaires OK
        environment: prod    # mais matchLabels doit correspondre
    spec:
      containers:
      - name: webapp
        image: nginx:1.24
        ports:
        - containerPort: 80
        # Contexte de s√©curit√©
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        volumeMounts:
        - name: cache
          mountPath: /var/cache/nginx
        - name: run
          mountPath: /var/run
      volumes:
      - name: cache
        emptyDir: {}
      - name: run
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  selector:
    app: webapp       # S√©lectionne TOUS les pods avec app=webapp
    tier: frontend    # ET tier=frontend
  ports:
  - port: 80
    targetPort: 80
```

**‚ö†Ô∏è R√®gle CRITIQUE :** Les labels du `selector.matchLabels` **DOIVENT** √™tre un sous-ensemble des labels du `template.metadata.labels`. Sinon, le Deployment ne pourra pas g√©rer ses Pods.

**Exemple d'erreur courante :**
```yaml
spec:
  selector:
    matchLabels:
      app: webapp      # ‚ùå ERREUR !
  template:
    metadata:
      labels:
        app: different-name  # Ne correspond pas !
```

### 6.3 matchExpressions : S√©lection avanc√©e

`matchExpressions` permet des s√©lections plus complexes avec des op√©rateurs avanc√©s.

**Syntaxe :**
```yaml
selector:
  matchExpressions:
  - key: <label-key>
    operator: <In|NotIn|Exists|DoesNotExist>
    values: [<val1>, <val2>, ...]  # Requis pour In et NotIn, interdit pour Exists et DoesNotExist
```

### 6.4 Les 4 op√©rateurs de matchExpressions

#### Op√©rateur 1 : `In`
S√©lectionne les ressources o√π la cl√© existe ET la valeur est dans la liste.

**Exemple : D√©ploiement multi-environnements**

Cr√©er `12b-matchexpressions-in.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multi-env-app
spec:
  replicas: 3
  selector:
    matchExpressions:
    - key: environment
      operator: In
      values: ["staging", "production"]  # Pods avec env=staging OU env=production
    - key: app
      operator: In
      values: ["myapp"]                  # ET app=myapp
  template:
    metadata:
      labels:
        app: myapp
        environment: production  # Correspond car "production" est dans la liste
        tier: backend
    spec:
      containers:
      - name: app
        image: nginx:1.24-alpine
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "200m"
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        volumeMounts:
        - name: cache
          mountPath: /var/cache/nginx
        - name: run
          mountPath: /var/run
      volumes:
      - name: cache
        emptyDir: {}
      - name: run
        emptyDir: {}
```

**Cas d'usage :** G√©rer plusieurs environnements avec un seul Service.

```yaml
---
apiVersion: v1
kind: Service
metadata:
  name: multi-env-service
spec:
  selector:
    app: myapp
    # Ce service cible les pods staging ET production
  ports:
  - port: 80
```

#### Op√©rateur 2 : `NotIn`
S√©lectionne les ressources o√π la cl√© existe ET la valeur n'est PAS dans la liste.

**Exemple : Exclure les environnements de test**

Cr√©er `12c-matchexpressions-notin.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prod-only-app
spec:
  replicas: 2
  selector:
    matchExpressions:
    - key: environment
      operator: NotIn
      values: ["dev", "test"]  # Exclut dev et test
    - key: app
      operator: In
      values: ["critical-app"]
  template:
    metadata:
      labels:
        app: critical-app
        environment: production  # OK car pas dans [dev, test]
        criticality: high
    spec:
      containers:
      - name: app
        image: nginx:1.24-alpine
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "128Mi"
            cpu: "250m"
          limits:
            memory: "256Mi"
            cpu: "500m"
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        volumeMounts:
        - name: cache
          mountPath: /var/cache/nginx
        - name: run
          mountPath: /var/run
      volumes:
      - name: cache
        emptyDir: {}
      - name: run
        emptyDir: {}
```

**Cas d'usage :** NetworkPolicies pour bloquer le trafic vers les environnements non-production.

#### Op√©rateur 3 : `Exists`
S√©lectionne les ressources o√π la cl√© existe, **peu importe sa valeur**.

**Exemple : Tous les pods avec un label de monitoring**

Cr√©er `12d-matchexpressions-exists.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: monitored-app
spec:
  replicas: 2
  selector:
    matchExpressions:
    - key: monitoring
      operator: Exists  # Peu importe la valeur : monitoring=true, monitoring=enabled, etc.
    - key: app
      operator: In
      values: ["monitored-app"]
  template:
    metadata:
      labels:
        app: monitored-app
        monitoring: enabled  # N'importe quelle valeur fonctionne
        team: platform
    spec:
      containers:
      - name: app
        image: nginx:1.24-alpine
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "200m"
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        volumeMounts:
        - name: cache
          mountPath: /var/cache/nginx
        - name: run
          mountPath: /var/run
      volumes:
      - name: cache
        emptyDir: {}
      - name: run
        emptyDir: {}
```

**Cas d'usage :**
- S√©lectionner tous les pods qui doivent √™tre monitor√©s (peu importe la solution de monitoring)
- Identifier les ressources qui doivent √™tre sauvegard√©es

#### Op√©rateur 4 : `DoesNotExist`
S√©lectionne les ressources o√π la cl√© **n'existe PAS**.

**Exemple : Pods sans environnement sp√©cifi√© (fallback)**

Cr√©er `12e-matchexpressions-doesnotexist.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: default-env-app
spec:
  replicas: 1
  selector:
    matchExpressions:
    - key: environment
      operator: DoesNotExist  # Pods sans label "environment"
    - key: app
      operator: In
      values: ["legacy-app"]
  template:
    metadata:
      labels:
        app: legacy-app
        # Pas de label "environment"
        legacy: "true"
    spec:
      containers:
      - name: app
        image: nginx:1.24-alpine
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "64Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "200m"
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        volumeMounts:
        - name: cache
          mountPath: /var/cache/nginx
        - name: run
          mountPath: /var/run
      volumes:
      - name: cache
        emptyDir: {}
      - name: run
        emptyDir: {}
```

**Cas d'usage :**
- Identifier les ressources non √©tiquet√©es (pour audit)
- Appliquer des politiques par d√©faut aux ressources sans configuration sp√©cifique

### 6.5 Combinaison : matchLabels + matchExpressions

Vous pouvez combiner `matchLabels` et `matchExpressions` - toutes les conditions doivent √™tre satisfaites (AND logique).

**Exemple : S√©lection hybride complexe**

Cr√©er `12f-hybrid-selector.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hybrid-selector-app
spec:
  replicas: 3
  selector:
    matchLabels:          # Correspondance exacte
      app: myapp
      tier: backend
    matchExpressions:     # Conditions avanc√©es
    - key: environment
      operator: In
      values: ["staging", "production"]
    - key: version
      operator: Exists    # Doit avoir un label version
    - key: deprecated
      operator: DoesNotExist  # Ne doit PAS √™tre d√©pr√©ci√©
  template:
    metadata:
      labels:
        app: myapp                # ‚úì matchLabels
        tier: backend             # ‚úì matchLabels
        environment: production   # ‚úì In [staging, production]
        version: v2.1.0           # ‚úì Exists
        # deprecated: "true"      # ‚úì DoesNotExist (comment√© = n'existe pas)
    spec:
      containers:
      - name: backend
        image: nginx:1.24-alpine
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "128Mi"
            cpu: "250m"
          limits:
            memory: "256Mi"
            cpu: "500m"
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        volumeMounts:
        - name: cache
          mountPath: /var/cache/nginx
        - name: run
          mountPath: /var/run
      volumes:
      - name: cache
        emptyDir: {}
      - name: run
        emptyDir: {}
```

### 6.6 Labels recommand√©s par Kubernetes

Kubernetes recommande un ensemble de labels standardis√©s pour une meilleure interop√©rabilit√©.

**Labels recommand√©s officiels :**

Cr√©er `12g-recommended-labels.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: best-practice-app
  labels:
    # Labels recommand√©s par Kubernetes
    app.kubernetes.io/name: nginx           # Nom de l'application
    app.kubernetes.io/instance: nginx-prod  # Instance unique
    app.kubernetes.io/version: "1.24.0"     # Version actuelle
    app.kubernetes.io/component: webserver  # Composant dans l'archi
    app.kubernetes.io/part-of: ecommerce    # Application parente
    app.kubernetes.io/managed-by: kubectl   # Outil de gestion
spec:
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/name: nginx
      app.kubernetes.io/instance: nginx-prod
  template:
    metadata:
      labels:
        # M√™me structure de labels
        app.kubernetes.io/name: nginx
        app.kubernetes.io/instance: nginx-prod
        app.kubernetes.io/version: "1.24.0"
        app.kubernetes.io/component: webserver
        app.kubernetes.io/part-of: ecommerce
        app.kubernetes.io/managed-by: kubectl
    spec:
      containers:
      - name: nginx
        image: nginx:1.24
        ports:
        - name: http
          containerPort: 80
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "500m"
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        volumeMounts:
        - name: cache
          mountPath: /var/cache/nginx
        - name: run
          mountPath: /var/run
      volumes:
      - name: cache
        emptyDir: {}
      - name: run
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  labels:
    app.kubernetes.io/name: nginx
    app.kubernetes.io/instance: nginx-prod
    app.kubernetes.io/component: webserver
spec:
  selector:
    app.kubernetes.io/name: nginx
    app.kubernetes.io/instance: nginx-prod
  ports:
  - port: 80
    targetPort: http
```

**Tableau des labels recommand√©s :**

| Label | Description | Exemple |
|-------|-------------|---------|
| `app.kubernetes.io/name` | Nom de l'application | `mysql`, `wordpress` |
| `app.kubernetes.io/instance` | Instance unique | `mysql-prod`, `wordpress-dev` |
| `app.kubernetes.io/version` | Version actuelle | `5.7.21`, `1.0.0` |
| `app.kubernetes.io/component` | R√¥le dans l'architecture | `database`, `cache`, `frontend` |
| `app.kubernetes.io/part-of` | Application parente | `ecommerce`, `blog-platform` |
| `app.kubernetes.io/managed-by` | Outil de gestion | `helm`, `kubectl`, `argocd` |

### 6.7 Exercices pratiques

**Exercice 8a : Manipulation avec les selectors**

```bash
# 1. Cr√©er des ressources avec diff√©rents labels
kubectl apply -f 12a-deployment-matchlabels.yaml
kubectl apply -f 12b-matchexpressions-in.yaml
kubectl apply -f 12c-matchexpressions-notin.yaml

# 2. Lister tous les pods (toutes les applications)
kubectl get pods --show-labels

# 3. S√©lectionner les pods avec app=webapp
kubectl get pods -l app=webapp

# 4. S√©lectionner les pods avec environment in (production, staging)
kubectl get pods -l 'environment in (production,staging)'

# 5. S√©lectionner les pods qui ont le label monitoring (peu importe la valeur)
kubectl get pods -l monitoring

# 6. S√©lectionner les pods qui N'ONT PAS le label deprecated
kubectl get pods -l '!deprecated'

# 7. Combinaison : app=myapp ET environment=production
kubectl get pods -l 'app=myapp,environment=production'

# 8. Combinaison avec exclusion : app=myapp ET environment NOT IN (dev, test)
kubectl get pods -l 'app=myapp,environment notin (dev,test)'
```

**Exercice 8b : Modifier les labels dynamiquement**

```bash
# Ajouter un label √† un pod
kubectl label pod <pod-name> tested=true

# Modifier un label existant (--overwrite requis)
kubectl label pod <pod-name> environment=staging --overwrite

# Supprimer un label (suffixe -)
kubectl label pod <pod-name> tested-

# Ajouter un label √† tous les pods d'un deployment
kubectl label pods -l app=webapp team=platform

# V√©rifier les changements
kubectl get pods --show-labels
```

**Exercice 8c : Services et selectors**

Cr√©er `12h-service-selector-test.yaml` :

```yaml
# D√©ploiement frontend
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
      tier: frontend
  template:
    metadata:
      labels:
        app: myapp
        tier: frontend
        version: v1.0.0
    spec:
      containers:
      - name: nginx
        image: nginx:1.24-alpine
        ports:
        - containerPort: 80
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        volumeMounts:
        - name: cache
          mountPath: /var/cache/nginx
        - name: run
          mountPath: /var/run
      volumes:
      - name: cache
        emptyDir: {}
      - name: run
        emptyDir: {}
---
# D√©ploiement backend
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      tier: backend
  template:
    metadata:
      labels:
        app: myapp
        tier: backend
        version: v1.0.0
    spec:
      containers:
      - name: api
        image: httpd:2.4-alpine
        ports:
        - containerPort: 80
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
---
# Service pour frontend uniquement
apiVersion: v1
kind: Service
metadata:
  name: frontend-service
spec:
  selector:
    app: myapp
    tier: frontend  # S√©lectionne UNIQUEMENT les pods frontend
  ports:
  - port: 80
    targetPort: 80
---
# Service pour backend uniquement
apiVersion: v1
kind: Service
metadata:
  name: backend-service
spec:
  selector:
    app: myapp
    tier: backend  # S√©lectionne UNIQUEMENT les pods backend
  ports:
  - port: 80
    targetPort: 80
---
# Service pour TOUTE l'application (frontend + backend)
apiVersion: v1
kind: Service
metadata:
  name: app-service
spec:
  selector:
    app: myapp  # S√©lectionne TOUS les pods avec app=myapp (frontend ET backend)
  ports:
  - port: 80
    targetPort: 80
```

**Tester les selectors de Service :**

```bash
# Appliquer les ressources
kubectl apply -f 12h-service-selector-test.yaml

# V√©rifier les endpoints de chaque service
kubectl get endpoints

# Frontend service doit avoir 2 endpoints (2 replicas frontend)
kubectl get endpoints frontend-service

# Backend service doit avoir 3 endpoints (3 replicas backend)
kubectl get endpoints backend-service

# App service doit avoir 5 endpoints (2 frontend + 3 backend)
kubectl get endpoints app-service

# Afficher les d√©tails
kubectl describe service frontend-service
kubectl describe service backend-service
kubectl describe service app-service
```

**Exercice 8d : Debugging des selectors**

Fichier avec erreur `12i-buggy-selector.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: buggy-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp      # ‚ùå ERREUR
      tier: frontend  # ‚ùå ERREUR
  template:
    metadata:
      labels:
        app: different-app  # Ne correspond PAS au selector !
        tier: backend       # Ne correspond PAS au selector !
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
```

**Mission :**
1. Essayez d'appliquer ce manifest : `kubectl apply -f 12i-buggy-selector.yaml`
2. Lisez le message d'erreur
3. Identifiez le probl√®me
4. Corrigez le manifest

### 6.8 Cas d'usage avanc√©s

**Cas 1 : Canary Deployment avec labels**

```yaml
# Version stable (90% du trafic)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-stable
spec:
  replicas: 9
  selector:
    matchLabels:
      app: myapp
      track: stable
  template:
    metadata:
      labels:
        app: myapp
        track: stable
        version: v1.0.0
    spec:
      containers:
      - name: app
        image: myapp:1.0.0
---
# Version canary (10% du trafic)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-canary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
      track: canary
  template:
    metadata:
      labels:
        app: myapp
        track: canary
        version: v2.0.0
    spec:
      containers:
      - name: app
        image: myapp:2.0.0
---
# Service qui distribue le trafic sur les deux versions
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  selector:
    app: myapp  # S√©lectionne stable ET canary
  ports:
  - port: 80
```

**Cas 2 : Affinity rules avec labels**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cache-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: cache
  template:
    metadata:
      labels:
        app: cache
        tier: cache
    spec:
      # Anti-affinit√© : Ne pas placer 2 pods cache sur le m√™me n≈ìud
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - cache
            topologyKey: kubernetes.io/hostname
      containers:
      - name: redis
        image: redis:7-alpine
```

### 6.9 Bonnes pratiques

‚úÖ **√Ä FAIRE :**

1. **Utiliser des labels coh√©rents** dans toute l'organisation
   ```yaml
   labels:
     app: myapp
     environment: production
     team: platform
     cost-center: engineering
   ```

2. **Pr√©f√©rer les labels recommand√©s** par Kubernetes
   ```yaml
   labels:
     app.kubernetes.io/name: nginx
     app.kubernetes.io/instance: nginx-prod
   ```

3. **S'assurer de la correspondance selector ‚Üî labels**
   ```yaml
   selector:
     matchLabels:
       app: myapp  # ‚úì Doit correspondre
   template:
     metadata:
       labels:
         app: myapp  # ‚úì aux labels du template
   ```

4. **Utiliser des labels pour la facturation** (cloud)
   ```yaml
   labels:
     billing/team: platform
     billing/project: ecommerce
   ```

5. **Documenter la strat√©gie de labeling** de votre organisation

‚ùå **√Ä √âVITER :**

1. **Labels trop longs ou complexes**
   ```yaml
   labels:
     this-is-a-very-long-label-name-that-is-hard-to-type: value  # ‚ùå
   ```

2. **Valeurs changeantes** (timestamps, IDs al√©atoires)
   ```yaml
   labels:
     created-at: "2024-01-15T10:30:00Z"  # ‚ùå Change √† chaque d√©ploiement
   ```

3. **Informations sensibles dans les labels**
   ```yaml
   labels:
     api-key: secret123  # ‚ùå Les labels sont visibles !
   ```

4. **Selectors trop permissifs**
   ```yaml
   selector:
     matchLabels:
       app: myapp  # ‚ö†Ô∏è Peut s√©lectionner trop de pods
   ```

5. **Oublier de mettre √† jour les selectors lors des modifications**

### 6.10 R√©sum√© des selectors

| Type | Syntaxe | Use Case |
|------|---------|----------|
| **matchLabels** | `key: value` | S√©lection simple et exacte |
| **In** | `operator: In, values: [v1, v2]` | S√©lectionner parmi plusieurs valeurs |
| **NotIn** | `operator: NotIn, values: [v1, v2]` | Exclure certaines valeurs |
| **Exists** | `operator: Exists` | V√©rifier la pr√©sence d'un label |
| **DoesNotExist** | `operator: DoesNotExist` | V√©rifier l'absence d'un label |

**Commandes essentielles :**

```bash
# Afficher les labels
kubectl get pods --show-labels

# Filtrer par label √©galit√©
kubectl get pods -l app=myapp

# Filtrer par label in√©galit√©
kubectl get pods -l app!=myapp

# Filtrer In
kubectl get pods -l 'environment in (prod,staging)'

# Filtrer NotIn
kubectl get pods -l 'environment notin (dev,test)'

# Filtrer Exists
kubectl get pods -l environment

# Filtrer DoesNotExist
kubectl get pods -l '!environment'

# Combinaisons
kubectl get pods -l 'app=myapp,environment=prod'

# Ajouter/Modifier/Supprimer labels
kubectl label pod <name> key=value
kubectl label pod <name> key=value --overwrite
kubectl label pod <name> key-
```

## Partie 7 : Namespaces et organisation

### 7.1 Cr√©ation de namespaces

Cr√©er `13-namespaces.yaml` :

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: development
  labels:
    environment: dev

---
apiVersion: v1
kind: Namespace
metadata:
  name: staging
  labels:
    environment: staging

---
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    environment: prod
```

### 7.2 Ressources dans un namespace sp√©cifique

Cr√©er `14-app-in-namespace.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  namespace: development  # Sp√©cifier le namespace
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: app
        image: nginx:alpine

---
apiVersion: v1
kind: Service
metadata:
  name: myapp-service
  namespace: development
spec:
  selector:
    app: myapp
  ports:
  - port: 80
```

**Exercice 9 : Travailler avec les namespaces**

1. Cr√©ez les namespaces
2. Listez-les : `kubectl get namespaces`
3. D√©ployez l'application dans development
4. Listez les pods dans ce namespace :
   ```bash
   kubectl get pods -n development
   ```
5. D√©finissez development comme namespace par d√©faut :
   ```bash
   kubectl config set-context --current --namespace=development
   ```
6. Maintenant les commandes utilisent ce namespace automatiquement :
   ```bash
   kubectl get pods  # Affiche les pods de development
   ```

## Partie 8 : Exercices pratiques complets

### Exercice 10 : Application compl√®te WordPress

Cr√©ez une application WordPress avec MySQL en √©crivant les manifests pour :

1. **Namespace** : `wordpress-app`

2. **Secret** pour MySQL :
   - Nom : `mysql-secret`
   - Cl√©s : `password` avec valeur `wordpress123`

3. **PersistentVolumeClaim** pour MySQL :
   ```yaml
   apiVersion: v1
   kind: PersistentVolumeClaim
   metadata:
     name: mysql-pvc
     namespace: wordpress-app
   spec:
     accessModes:
       - ReadWriteOnce
     resources:
       requests:
         storage: 1Gi
   ```

4. **Deployment MySQL** :
   - Image : `mysql:8.0`
   - 1 replica
   - Variables d'environnement : `MYSQL_ROOT_PASSWORD`, `MYSQL_DATABASE=wordpress`
   - Volume mont√© sur `/var/lib/mysql`
   - Port : 3306

5. **Service MySQL** :
   - Type : ClusterIP
   - Port : 3306

6. **PersistentVolumeClaim** pour WordPress

7. **Deployment WordPress** :
   - Image : `wordpress:6.4-apache`
   - 2 replicas
   - Variables d'environnement : `WORDPRESS_DB_HOST=mysql-service`, `WORDPRESS_DB_USER=root`, `WORDPRESS_DB_PASSWORD` (depuis le secret), `WORDPRESS_DB_NAME=wordpress`
   - Port : 80

8. **Service WordPress** :
   - Type : NodePort
   - Port : 80

**Validation :**
```bash
kubectl apply -f wordpress-namespace.yaml
kubectl apply -f wordpress-secret.yaml
kubectl apply -f wordpress-mysql.yaml
kubectl apply -f wordpress-app.yaml

# Attendre que tout soit pr√™t
kubectl wait --for=condition=ready pod -l app=mysql -n wordpress-app --timeout=120s
kubectl wait --for=condition=ready pod -l app=wordpress -n wordpress-app --timeout=120s
```

**Acc√®s √† WordPress :**

Avec minikube :
```bash
minikube service wordpress-service -n wordpress-app
```

Avec kubeadm :
```bash
# R√©cup√©rer le NodePort et l'IP d'un n≈ìud
NODE_PORT=$(kubectl get svc wordpress-service -n wordpress-app -o jsonpath='{.spec.ports[0].nodePort}')
NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')
echo "WordPress accessible √† : http://$NODE_IP:$NODE_PORT"
```

### Exercice 11 : Application avec microservices

Cr√©ez une stack applicative compl√®te :

**Architecture :**
- Frontend (Nginx) ‚Üí Backend API (Node.js) ‚Üí Database (PostgreSQL) ‚Üí Cache (Redis)

**Contraintes :**
- Utiliser des labels coh√©rents pour l'ensemble
- Le frontend doit √™tre accessible via NodePort
- Backend, Database et Redis doivent √™tre en ClusterIP
- Utiliser des ConfigMaps pour la configuration
- Utiliser des Secrets pour les mots de passe
- D√©finir des ressources requests/limits
- Ajouter des probes (liveness et readiness)
- Organiser dans un namespace d√©di√©

**Structure sugg√©r√©e :**
```
microservices/
‚îú‚îÄ‚îÄ 00-namespace.yaml
‚îú‚îÄ‚îÄ 01-configmaps.yaml
‚îú‚îÄ‚îÄ 02-secrets.yaml
‚îú‚îÄ‚îÄ 03-redis.yaml
‚îú‚îÄ‚îÄ 04-database.yaml
‚îú‚îÄ‚îÄ 05-backend.yaml
‚îî‚îÄ‚îÄ 06-frontend.yaml
```

### Exercice 12 : Job et CronJob

Cr√©er `15-job.yaml` :

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: database-backup
spec:
  template:
    spec:
      containers:
      - name: backup
        image: busybox
        command: ["/bin/sh"]
        args:
          - -c
          - >
            echo "Starting backup..." &&
            echo "Backing up database..." &&
            sleep 10 &&
            echo "Backup completed successfully!"
      restartPolicy: OnFailure
  backoffLimit: 3
```

Cr√©er `16-cronjob.yaml` :

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cleanup-job
spec:
  schedule: "*/5 * * * *"  # Toutes les 5 minutes
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: cleanup
            image: busybox
            command: ["/bin/sh"]
            args:
              - -c
              - >
                echo "Running cleanup at $(date)" &&
                echo "Cleaning temporary files..." &&
                sleep 5 &&
                echo "Cleanup done!"
          restartPolicy: OnFailure
```

**Exercice :**
1. Cr√©ez le Job et surveillez son ex√©cution : `kubectl get jobs -w`
2. Consultez les logs : `kubectl logs job/database-backup`
3. Cr√©ez le CronJob
4. Listez les CronJobs : `kubectl get cronjobs`
5. Attendez quelques minutes et listez les jobs cr√©√©s : `kubectl get jobs`
6. Suspendez le CronJob : `kubectl patch cronjob cleanup-job -p '{"spec":{"suspend":true}}'`

## Partie 9 : Validation et bonnes pratiques

### 9.1 Outils de validation

**Validation avec kubectl :**
```bash
# Dry-run c√¥t√© client
kubectl apply -f manifest.yaml --dry-run=client -o yaml

# Dry-run c√¥t√© serveur (validation plus stricte)
kubectl apply -f manifest.yaml --dry-run=server

# Validation de la syntaxe YAML
kubectl apply -f manifest.yaml --validate=true

# Diff avant application
kubectl diff -f manifest.yaml
```

**Validation avec kubeval :**
```bash
# Installation
wget https://github.com/instrumenta/kubeval/releases/latest/download/kubeval-linux-amd64.tar.gz
tar xf kubeval-linux-amd64.tar.gz
sudo mv kubeval /usr/local/bin

# Utilisation
kubeval manifest.yaml
kubeval *.yaml
```

**Validation avec kube-score :**
```bash
# Installation
wget https://github.com/zegl/kube-score/releases/download/v1.17.0/kube-score_1.17.0_linux_amd64
chmod +x kube-score_1.17.0_linux_amd64
sudo mv kube-score_1.17.0_linux_amd64 /usr/local/bin/kube-score

# Utilisation
kube-score score manifest.yaml
```

### 9.2 Bonnes pratiques

**1. Toujours sp√©cifier les versions d'images**
```yaml
# Mauvais
image: nginx

# Bon
image: nginx:1.24.0
```

**2. D√©finir les ressources requests et limits**
```yaml
resources:
  requests:
    memory: "64Mi"
    cpu: "250m"
  limits:
    memory: "128Mi"
    cpu: "500m"
```

**3. Utiliser des labels coh√©rents**
```yaml
labels:
  app.kubernetes.io/name: myapp
  app.kubernetes.io/instance: myapp-prod
  app.kubernetes.io/version: "1.0.0"
  app.kubernetes.io/component: frontend
  app.kubernetes.io/part-of: ecommerce
  app.kubernetes.io/managed-by: kubectl
```

**4. Ajouter des health checks**
```yaml
livenessProbe:
  httpGet:
    path: /healthz
    port: 8080
  initialDelaySeconds: 30
  periodSeconds: 10

readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5
```

**5. Utiliser des namespaces pour l'isolation**

**6. Ne jamais commiter les secrets en clair**

**7. Documenter avec des annotations**
```yaml
metadata:
  annotations:
    description: "Service principal pour l'API backend"
    contact: "team-backend@example.com"
    documentation: "https://docs.example.com/api"
```

### 9.3 Template de manifest complet

Cr√©er `template-complete.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: complete-app
  namespace: production
  labels:
    app.kubernetes.io/name: complete-app
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/component: api
  annotations:
    description: "Template complet d'une application Kubernetes"
spec:
  replicas: 3

  # Strat√©gie de d√©ploiement
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0

  selector:
    matchLabels:
      app.kubernetes.io/name: complete-app

  template:
    metadata:
      labels:
        app.kubernetes.io/name: complete-app
        app.kubernetes.io/version: "1.0.0"

    spec:
      # Contraintes de placement
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - complete-app
              topologyKey: kubernetes.io/hostname

      containers:
      - name: app
        image: nginx:1.24.0

        ports:
        - name: http
          containerPort: 80
          protocol: TCP

        # Variables d'environnement
        env:
        - name: LOG_LEVEL
          value: "info"
        - name: DB_HOST
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: database_url
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: app-secret
              key: password

        # Ressources
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "500m"

        # Health checks
        livenessProbe:
          httpGet:
            path: /healthz
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3

        readinessProbe:
          httpGet:
            path: /ready
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3

        # Security context
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL

        # Volumes
        volumeMounts:
        - name: config
          mountPath: /etc/config
          readOnly: true
        - name: cache
          mountPath: /var/cache/nginx
        - name: run
          mountPath: /var/run

      volumes:
      - name: config
        configMap:
          name: app-config
      - name: cache
        emptyDir: {}
      - name: run
        emptyDir: {}
```

## Partie 10 : Tests et debugging

### 10.1 Commandes de test

```bash
# Appliquer et surveiller
kubectl apply -f manifest.yaml && kubectl get pods -w

# Tester la connectivit√©
kubectl run test-pod --image=busybox -it --rm -- wget -qO- http://service-name

# Port-forward pour tester localement
kubectl port-forward deployment/myapp 8080:80

# Ex√©cuter des commandes dans un pod
kubectl exec -it pod-name -- /bin/sh

# Copier des fichiers depuis/vers un pod
kubectl cp pod-name:/path/to/file ./local-file
kubectl cp ./local-file pod-name:/path/to/file

# Afficher les √©v√©nements
kubectl get events --sort-by='.lastTimestamp'

# Debug d'un pod qui ne d√©marre pas
kubectl describe pod pod-name
kubectl logs pod-name
kubectl logs pod-name --previous  # Logs du conteneur pr√©c√©dent
```

### 10.2 Exercice de debugging

**Fichier avec erreurs** `17-buggy-manifest.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: buggy-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: buggy
  template:
    metadata:
      labels:
        app: wrong-label  # Bug 1: Label ne correspond pas au selector
    spec:
      containers:
      - name: app
        image: ngin:latest  # Bug 2: Image incorrecte
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "1Gi"
            cpu: "2000m"  # Bug 3: Ressources trop √©lev√©es (unrealistic)
          limits:
            memory: "512Mi"  # Bug 4: Limit < Request
            cpu: "1000m"
```

**Mission :**
1. Essayez d'appliquer ce manifest
2. Identifiez toutes les erreurs
3. Corrigez-les une par une
4. Validez que l'application fonctionne

### 10.3 Checklist de validation

Avant d'appliquer un manifest, v√©rifiez :

- [ ] La syntaxe YAML est correcte (indentation, guillemets)
- [ ] Les labels du selector correspondent aux labels des pods
- [ ] Les versions d'images sont sp√©cifi√©es
- [ ] Les ressources requests sont d√©finies
- [ ] Les limits sont >= aux requests
- [ ] Les ports sont corrects
- [ ] Les noms de ConfigMaps/Secrets existent
- [ ] Les volumes mont√©s correspondent aux volumes d√©clar√©s
- [ ] Les probes sont configur√©es si n√©cessaire
- [ ] Le namespace existe (si sp√©cifi√©)
- [ ] Validation avec `--dry-run=server` r√©ussit

## Solutions des exercices

<details>
<summary>Solution Exercice 10 : WordPress complet</summary>

**wordpress-namespace.yaml**
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: wordpress-app
```

**wordpress-secret.yaml**
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysql-secret
  namespace: wordpress-app
type: Opaque
stringData:
  password: wordpress123
```

**wordpress-mysql.yaml**
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc
  namespace: wordpress-app
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
  namespace: wordpress-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:8.0
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: password
        - name: MYSQL_DATABASE
          value: wordpress
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: mysql-storage
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-storage
        persistentVolumeClaim:
          claimName: mysql-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: mysql-service
  namespace: wordpress-app
spec:
  type: ClusterIP
  selector:
    app: mysql
  ports:
  - port: 3306
```

**wordpress-app.yaml**
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wordpress-pvc
  namespace: wordpress-app
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
  namespace: wordpress-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      containers:
      - name: wordpress
        image: wordpress:6.4-apache
        env:
        - name: WORDPRESS_DB_HOST
          value: mysql-service
        - name: WORDPRESS_DB_USER
          value: root
        - name: WORDPRESS_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-secret
              key: password
        - name: WORDPRESS_DB_NAME
          value: wordpress
        ports:
        - containerPort: 80
        volumeMounts:
        - name: wordpress-storage
          mountPath: /var/www/html
      volumes:
      - name: wordpress-storage
        persistentVolumeClaim:
          claimName: wordpress-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: wordpress-service
  namespace: wordpress-app
spec:
  type: NodePort
  selector:
    app: wordpress
  ports:
  - port: 80
    targetPort: 80
    nodePort: 30080
```

**D√©ploiement :**
```bash
kubectl apply -f wordpress-namespace.yaml
kubectl apply -f wordpress-secret.yaml
kubectl apply -f wordpress-mysql.yaml
kubectl apply -f wordpress-app.yaml

# Attendre que tout soit pr√™t
kubectl wait --for=condition=ready pod -l app=mysql -n wordpress-app --timeout=120s
kubectl wait --for=condition=ready pod -l app=wordpress -n wordpress-app --timeout=120s

# Acc√©der √† WordPress
# Avec minikube :
minikube service wordpress-service -n wordpress-app

# Avec kubeadm :
NODE_PORT=$(kubectl get svc wordpress-service -n wordpress-app -o jsonpath='{.spec.ports[0].nodePort}')
NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')
echo "http://$NODE_IP:$NODE_PORT"
```
</details>

<details>
<summary>Solution Exercice de debugging</summary>

**Version corrig√©e** `17-buggy-manifest-fixed.yaml` :

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: buggy-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: buggy  # Correction 1: Doit correspondre au label du pod
  template:
    metadata:
      labels:
        app: buggy  # Correction 1: Label corrig√©
    spec:
      containers:
      - name: app
        image: nginx:1.24  # Correction 2: Nom d'image corrig√© avec version sp√©cifique
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "128Mi"  # Correction 3: Ressources r√©alistes
            cpu: "100m"
          limits:
            memory: "256Mi"  # Correction 4: Limits > Requests
            cpu: "500m"
```
</details>

## Ressources compl√©mentaires

### Documentation
- API Reference Kubernetes : https://kubernetes.io/docs/reference/kubernetes-api/
- YAML Specification : https://yaml.org/spec/
- Best Practices : https://kubernetes.io/docs/concepts/configuration/overview/

### Outils utiles
- **kubectl explain** : Documentation int√©gr√©e
- **kubeval** : Validation de manifests
- **kube-score** : Analyse de qualit√©
- **yamllint** : Linter YAML
- **VS Code** : Extension Kubernetes pour l'auto-compl√©tion

### Exemples de manifests
```bash
# Obtenir le YAML d'une ressource existante
kubectl get deployment nginx -o yaml > example-deployment.yaml

# G√©n√©rer un template
kubectl create deployment test --image=nginx --dry-run=client -o yaml
kubectl create service clusterip test --tcp=80:80 --dry-run=client -o yaml
```

## Points cl√©s √† retenir

1. **Structure** : Tous les manifests suivent apiVersion, kind, metadata, spec
2. **Labels** : Essentiels pour lier les ressources (Services ‚Üí Pods)
3. **Validation** : Toujours utiliser `--dry-run` avant d'appliquer
4. **Ressources** : D√©finir requests et limits pour une meilleure gestion
5. **Health checks** : Liveness et readiness probes pour la fiabilit√©
6. **Configuration** : Externaliser avec ConfigMaps et Secrets
7. **Namespaces** : Organiser et isoler les ressources
8. **Documentation** : Utiliser labels et annotations pour la tra√ßabilit√©
9. **Versions** : Toujours sp√©cifier les versions d'images
10. **Tests** : Valider avec plusieurs outils avant de d√©ployer en production

## Prochaines √©tapes

Apr√®s avoir ma√Ætris√© les manifests, vous pouvez explorer :
- **Helm** : Gestionnaire de packages pour Kubernetes
- **Kustomize** : Personnalisation de manifests
- **GitOps** : D√©ploiement automatis√© avec ArgoCD ou Flux
- **StatefulSets** : Pour les applications avec √©tat
- **DaemonSets** : D√©ploiement sur tous les n≈ìuds
- **Ingress** : Gestion avanc√©e du trafic HTTP/HTTPS
- **Network Policies** : S√©curit√© r√©seau
- **RBAC** : Contr√¥le d'acc√®s
